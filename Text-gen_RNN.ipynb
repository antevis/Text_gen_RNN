{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Text-gen RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load text into a one huge string (millions of chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every unhappy family is unhappy in its own\n",
      "way.\n",
      "\n",
      "Everythin\n",
      "1966145\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create vocabulary : a set of all chars of which 'text' consists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E', 'n', 'O', '5', 'g', \"'\", '-', 'B', 'q', 'K', 'U', 'Q', 'X', ')', '\\n', 'k', 'l', '!', '`', 'e', 'J', 'H', 'a', 'h', 'b', 'p', 'F', 'A', 'N', 'R', 'y', '1', 's', 'L', 'W', 'I', '7', '3', 'M', 'i', 'C', 'P', '6', 'S', '2', 'w', '9', ' ', 'o', ',', '\"', 'x', 'm', 'Y', '?', 'v', 'r', '_', '(', ':', 'G', 'D', 'f', '8', 'T', 't', 'j', '0', 'u', '.', 'c', 'Z', 'V', 'd', 'z', ';', '4'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create vocab_to_int and int_to_vocab. These are dictionaries. You won't need it. Hopefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E': 0, 'b': 24, 'n': 1, '5': 3, 'g': 4, 'U': 10, 'B': 7, 'K': 9, 'q': 8, 'O': 2, 'k': 15, 'l': 16, '!': 17, '`': 18, '4': 76, 'e': 19, 's': 32, 'J': 20, 'a': 22, 'h': 23, 'p': 25, 'R': 29, \"'\": 5, 'N': 28, 'y': 30, '1': 31, 'W': 34, 'L': 33, 'X': 12, '7': 36, 'I': 35, '9': 46, '.': 69, '3': 37, 'H': 21, 'M': 38, 'i': 39, 'C': 40, 'c': 70, 'P': 41, '6': 42, 'S': 43, 'G': 60, 'w': 45, 'A': 27, '-': 6, ' ': 47, '0': 67, 'o': 48, '\"': 50, '2': 44, 'x': 51, 'm': 52, 'Y': 53, 'F': 26, 'r': 56, '(': 58, ':': 59, 'D': 61, '\\n': 14, 'f': 62, '?': 54, 'T': 64, 't': 65, '8': 63, 'Z': 71, ',': 49, ';': 75, 'v': 55, 'u': 68, 'j': 66, 'Q': 11, 'V': 72, 'd': 73, 'z': 74, ')': 13, '_': 57}\n",
      "\n",
      "{0: 'E', 1: 'n', 2: 'O', 3: '5', 4: 'g', 5: \"'\", 6: '-', 7: 'B', 8: 'q', 9: 'K', 10: 'U', 11: 'Q', 12: 'X', 13: ')', 14: '\\n', 15: 'k', 16: 'l', 17: '!', 18: '`', 19: 'e', 20: 'J', 21: 'H', 22: 'a', 23: 'h', 24: 'b', 25: 'p', 26: 'F', 27: 'A', 28: 'N', 29: 'R', 30: 'y', 31: '1', 32: 's', 33: 'L', 34: 'W', 35: 'I', 36: '7', 37: '3', 38: 'M', 39: 'i', 40: 'C', 41: 'P', 42: '6', 43: 'S', 44: '2', 45: 'w', 46: '9', 47: ' ', 48: 'o', 49: ',', 50: '\"', 51: 'x', 52: 'm', 53: 'Y', 54: '?', 55: 'v', 56: 'r', 57: '_', 58: '(', 59: ':', 60: 'G', 61: 'D', 62: 'f', 63: '8', 64: 'T', 65: 't', 66: 'j', 67: '0', 68: 'u', 69: '.', 70: 'c', 71: 'Z', 72: 'V', 73: 'd', 74: 'z', 75: ';', 76: '4'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_to_int)\n",
    "print()\n",
    "print(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create an iteger representation of 'text' (millions of chars as ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "unichr(x) - char from unicode int\n",
    "ord(x) - byte or in from chr() or unichar() respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40 23 22 25 65 19 56 47 31 14 14 14 21 22 25 25 30 47 62 22 52 39 16 39 19\n",
      " 32 47 22 56 19 47 22 16 16 47 22 16 39 15 19 75 47 19 55 19 56 30 47 68  1\n",
      " 23 22 25 25 30 47 62 22 52 39 16 30 47 39 32 47 68  1 23 22 25 25 30 47 39\n",
      "  1 47 39 65 32 47 48 45  1 14 45 22 30 69 14 14  0 55 19 56 30 65 23 39  1]\n"
     ]
    }
   ],
   "source": [
    "#chars = np.array([ord(c) for c in text], dtype=np.int32)\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "print(chars[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars_vector,\n",
    "               samples_per_batch,\n",
    "               sample_length,\n",
    "               split_frac=0.9):\n",
    "\n",
    "    x = chars_vector[:-1]\n",
    "    y = chars_vector[1:]\n",
    "\n",
    "    sample_count = len(x)-sample_length+1\n",
    "    \n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    start_range = range(0, sample_count, sample_length)\n",
    "\n",
    "    x_samples = np.array([x[start:start+sample_length] for start in start_range])\n",
    "    y_samples = np.array([y[start:start+sample_length] for start in start_range])\n",
    "\n",
    "    if sample_count > samples_per_batch:\n",
    "\n",
    "        batch_count = len(x_samples) // samples_per_batch\n",
    "        new_length = batch_count * samples_per_batch\n",
    "        end_crop_count = len(x_samples)-new_length\n",
    "\n",
    "        if end_crop_count != 0:\n",
    "            x_samples = x_samples[:-end_crop_count]\n",
    "            y_samples = y_samples[:-end_crop_count]\n",
    "\n",
    "        x_batches = np.array(np.split(x_samples, batch_count))\n",
    "        y_batches = np.array(np.split(y_samples, batch_count))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        x_batches = x_samples\n",
    "        y_batches = y_samples\n",
    "        \n",
    "    \n",
    "    split_idx = int(len(x_batches)*split_frac)\n",
    "\n",
    "    train_x, train_y = x_batches[:split_idx], y_batches[:split_idx]\n",
    "    val_x, val_y = x_batches[split_idx:], y_batches[split_idx:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tx:\n",
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]\n",
      "  [ 9 10 11]\n",
      "  [12 13 14]]\n",
      "\n",
      " [[15 16 17]\n",
      "  [18 19 20]\n",
      "  [21 22 23]\n",
      "  [24 25 26]\n",
      "  [27 28 29]]]\n",
      "\n",
      "ty:\n",
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]\n",
      "  [ 7  8  9]\n",
      "  [10 11 12]\n",
      "  [13 14 15]]\n",
      "\n",
      " [[16 17 18]\n",
      "  [19 20 21]\n",
      "  [22 23 24]\n",
      "  [25 26 27]\n",
      "  [28 29 30]]]\n",
      "(2, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "vector = np.arange(42)\n",
    "samples_in_batch = 5\n",
    "sample_length = 3\n",
    "split_frac = 1.\n",
    "\n",
    "tx, ty, _, _ = split_data(vector, samples_in_batch, sample_length, split_frac)\n",
    "\n",
    "print('\\ntx:')\n",
    "print(tx)\n",
    "print('\\nty:')\n",
    "print(ty)\n",
    "\n",
    "print(tx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(tx, ty):\n",
    "    for x, y in zip(tx, ty):\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Creating training and validation sets using function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars_vector=chars, \n",
    "                                            samples_per_batch=100, \n",
    "                                            sample_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape: (176, 100, 100)\n",
      "train_y.shape: (176, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "print('train_x.shape: {}'.format(train_x.shape))\n",
    "print('train_y.shape: {}'.format(train_y.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 23, 22, 25, 65, 19, 56, 47, 31, 14, 14, 14, 21, 22, 25, 25, 30,\n",
       "        47, 62, 22, 52, 39, 16, 39, 19, 32, 47, 22, 56, 19, 47, 22, 16, 16,\n",
       "        47, 22, 16, 39, 15, 19, 75, 47, 19, 55, 19, 56, 30, 47, 68,  1, 23,\n",
       "        22, 25, 25, 30, 47, 62, 22, 52, 39, 16, 30, 47, 39, 32, 47, 68,  1,\n",
       "        23, 22, 25, 25, 30, 47, 39,  1, 47, 39, 65, 32, 47, 48, 45,  1, 14,\n",
       "        45, 22, 30, 69, 14, 14,  0, 55, 19, 56, 30, 65, 23, 39,  1],\n",
       "       [ 4, 47, 45, 22, 32, 47, 39,  1, 47, 70, 48,  1, 62, 68, 32, 39, 48,\n",
       "         1, 47, 39,  1, 47, 65, 23, 19, 47,  2, 24, 16, 48,  1, 32, 15, 30,\n",
       "        32,  5, 47, 23, 48, 68, 32, 19, 69, 47, 64, 23, 19, 47, 45, 39, 62,\n",
       "        19, 47, 23, 22, 73, 14, 73, 39, 32, 70, 48, 55, 19, 56, 19, 73, 47,\n",
       "        65, 23, 22, 65, 47, 65, 23, 19, 47, 23, 68, 32, 24, 22,  1, 73, 47,\n",
       "        45, 22, 32, 47, 70, 22, 56, 56, 30, 39,  1,  4, 47, 48,  1]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0,:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23, 22, 25, 65, 19, 56, 47, 31, 14, 14, 14, 21, 22, 25, 25, 30, 47,\n",
       "        62, 22, 52, 39, 16, 39, 19, 32, 47, 22, 56, 19, 47, 22, 16, 16, 47,\n",
       "        22, 16, 39, 15, 19, 75, 47, 19, 55, 19, 56, 30, 47, 68,  1, 23, 22,\n",
       "        25, 25, 30, 47, 62, 22, 52, 39, 16, 30, 47, 39, 32, 47, 68,  1, 23,\n",
       "        22, 25, 25, 30, 47, 39,  1, 47, 39, 65, 32, 47, 48, 45,  1, 14, 45,\n",
       "        22, 30, 69, 14, 14,  0, 55, 19, 56, 30, 65, 23, 39,  1,  4],\n",
       "       [47, 45, 22, 32, 47, 39,  1, 47, 70, 48,  1, 62, 68, 32, 39, 48,  1,\n",
       "        47, 39,  1, 47, 65, 23, 19, 47,  2, 24, 16, 48,  1, 32, 15, 30, 32,\n",
       "         5, 47, 23, 48, 68, 32, 19, 69, 47, 64, 23, 19, 47, 45, 39, 62, 19,\n",
       "        47, 23, 22, 73, 14, 73, 39, 32, 70, 48, 55, 19, 56, 19, 73, 47, 65,\n",
       "        23, 22, 65, 47, 65, 23, 19, 47, 23, 68, 32, 24, 22,  1, 73, 47, 45,\n",
       "        22, 32, 47, 70, 22, 56, 56, 30, 39,  1,  4, 47, 48,  1, 47]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0, :2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes,\n",
    "              samples_per_batch=50,\n",
    "              sample_length=50,\n",
    "              lstm_size=128, \n",
    "              num_layers=2,\n",
    "              learning_rate=0.001, \n",
    "              grad_clip=5, \n",
    "              sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        samples_per_batch, sample_length = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [samples_per_batch, sample_length], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [samples_per_batch, sample_length], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(samples_per_batch, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, sample_length, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "samples_per_batch = 100\n",
    "sample_length = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.002\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epoch_count = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "print('Generating training and validation data...')\n",
    "train_x, train_y, val_x, val_y = split_data(chars, samples_per_batch, sample_length)\n",
    "print('train_x len: {}'.format(len(train_x)))\n",
    "print('train_y len: {}'.format(len(train_y)))\n",
    "print('val_x len: {}'.format(len(val_x)))\n",
    "print('val_y len: {}'.format(len(val_y)))\n",
    "print('Data generation complete.')\n",
    "\n",
    "print('Building model...')\n",
    "model = build_rnn(len(vocab_int), \n",
    "                  samples_per_batch=samples_per_batch,\n",
    "                  sample_length=sample_length,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "print('Model built')\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    n_batches = len(train_x)\n",
    "    print('total batches: {}'.format(n_batches))\n",
    "    \n",
    "    iteration_count = n_batches * epoch_count\n",
    "    print('total iteration count: {}'.format(iteration_count))\n",
    "    \n",
    "    for epoch in range(epoch_count):\n",
    "        \n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        \n",
    "        for i, (x, y) in enumerate(get_batch(train_x, train_y)):\n",
    "#             print(train_x[i])\n",
    "#             print(x.shape)\n",
    "            iteration = epoch*n_batches + i + 1\n",
    "            start = time.time()\n",
    "            \n",
    "            feed = {model.inputs: x, \n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            \n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(epoch, epoch_count-1),\n",
    "                  'Iteration {}/{}'.format(iteration, iteration_count),\n",
    "                  'Training loss: {:.4f}'.format(loss/(i+1)),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iteration_count):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch(val_x, val_y):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3560_l512_v1.330.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512_v2.251.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512_v1.872.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512_v1.697.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512_v1.579.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512_v1.509.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512_v1.472.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512_v1.430.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512_v1.400.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512_v1.390.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512_v1.373.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512_v1.368.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512_v1.350.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512_v1.346.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512_v1.346.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512_v1.338.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512_v1.336.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512_v1.330.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3560_l512_v1.330.ckpt\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            #x[0,0] = ord(c)# vocab_to_int[c]\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab), 1)\n",
    "        #samples.append(chr(c))#(int_to_vocab[c])\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            #samples.append(chr(c))# (int_to_vocab[c])\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [77] rhs shape= [83]\n\t [[Node: save/Assign_19 = Assign[T=DT_FLOAT, _class=[\"loc:@softmax/Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax/Variable_1/Adam_1, save/RestoreV2_19)]]\n\nCaused by op 'save/Assign_19', defined at:\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-e74df10e554b>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Fra\")\n  File \"<ipython-input-20-2da7ff682674>\", line 4, in sample\n    saver = tf.train.Saver()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [77] rhs shape= [83]\n\t [[Node: save/Assign_19 = Assign[T=DT_FLOAT, _class=[\"loc:@softmax/Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax/Variable_1/Adam_1, save/RestoreV2_19)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [77] rhs shape= [83]\n\t [[Node: save/Assign_19 = Assign[T=DT_FLOAT, _class=[\"loc:@softmax/Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax/Variable_1/Adam_1, save/RestoreV2_19)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e74df10e554b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"checkpoints/i3560_l512_v1.330.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Fra\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-2da7ff682674>\u001b[0m in \u001b[0;36msample\u001b[0;34m(checkpoint, n_samples, lstm_size, vocab_size, prime)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1437\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1439\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [77] rhs shape= [83]\n\t [[Node: save/Assign_19 = Assign[T=DT_FLOAT, _class=[\"loc:@softmax/Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax/Variable_1/Adam_1, save/RestoreV2_19)]]\n\nCaused by op 'save/Assign_19', defined at:\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-e74df10e554b>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Fra\")\n  File \"<ipython-input-20-2da7ff682674>\", line 4, in sample\n    saver = tf.train.Saver()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [77] rhs shape= [83]\n\t [[Node: save/Assign_19 = Assign[T=DT_FLOAT, _class=[\"loc:@softmax/Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax/Variable_1/Adam_1, save/RestoreV2_19)]]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i3560_l512_v1.330.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Fra\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
