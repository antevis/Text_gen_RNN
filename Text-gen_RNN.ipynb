{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Text-gen RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load text into a one huge string (millions of chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every unhappy family is unhappy in its own\n",
      "way.\n",
      "\n",
      "Everythin\n",
      "1985223\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create vocabulary : a set of all chars of which 'text' consists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N', 'Q', 'S', 'v', 'w', 'D', '6', 'c', 's', 'K', 'J', 'i', '!', ' ', ';', '&', ':', '1', 'M', '*', 'g', 'F', '_', 'x', 'P', 'B', 'z', \"'\", 'n', 'V', 't', '5', '0', 'X', 'u', 'h', ')', 'k', '4', 'r', '/', 'Z', '-', '3', ',', 'C', 'q', 'G', '8', 'O', '`', 'b', 'p', 'I', '9', 'T', 'f', '(', 'o', 'R', 'E', '2', 'y', 'L', 'U', '@', 'j', 'm', '\"', 'W', 'A', '%', '?', 'l', '\\n', 'a', 'e', '$', '.', 'd', 'Y', '7', 'H'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create vocab_to_int and int_to_vocab. These are dictionaries. You won't need it. Hopefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 0, 'Q': 1, 'S': 2, 'v': 3, 'w': 4, '6': 6, 'R': 59, 'c': 7, 's': 8, 'K': 9, 'i': 11, '!': 12, ' ': 13, 'n': 28, ';': 14, '&': 15, ':': 16, '1': 17, 'M': 18, 'U': 64, 'D': 5, 'L': 63, 'g': 20, '0': 32, 'B': 25, 'x': 23, 'P': 24, \"'\": 27, 'V': 29, '5': 31, 'X': 33, 'e': 76, 'G': 47, 'z': 26, 'u': 34, '4': 38, ')': 36, 'k': 37, '/': 40, 'r': 39, 'Z': 41, '-': 42, '3': 43, ',': 44, '.': 78, 'C': 45, 'j': 66, 'q': 46, 'O': 49, 'b': 51, 't': 30, 'I': 53, 'h': 35, '*': 19, 'T': 55, 'f': 56, '(': 57, 'o': 58, 'E': 60, 'Y': 80, 'y': 62, 'A': 70, '@': 65, 'm': 67, 'W': 69, '\"': 68, '%': 71, '9': 54, '8': 48, 'F': 21, 'l': 73, '\\n': 74, 'p': 52, '_': 22, 'a': 75, 'J': 10, '2': 61, 'd': 79, '$': 77, '`': 50, '?': 72, '7': 81, 'H': 82}\n",
      "\n",
      "{0: 'N', 1: 'Q', 2: 'S', 3: 'v', 4: 'w', 5: 'D', 6: '6', 7: 'c', 8: 's', 9: 'K', 10: 'J', 11: 'i', 12: '!', 13: ' ', 14: ';', 15: '&', 16: ':', 17: '1', 18: 'M', 19: '*', 20: 'g', 21: 'F', 22: '_', 23: 'x', 24: 'P', 25: 'B', 26: 'z', 27: \"'\", 28: 'n', 29: 'V', 30: 't', 31: '5', 32: '0', 33: 'X', 34: 'u', 35: 'h', 36: ')', 37: 'k', 38: '4', 39: 'r', 40: '/', 41: 'Z', 42: '-', 43: '3', 44: ',', 45: 'C', 46: 'q', 47: 'G', 48: '8', 49: 'O', 50: '`', 51: 'b', 52: 'p', 53: 'I', 54: '9', 55: 'T', 56: 'f', 57: '(', 58: 'o', 59: 'R', 60: 'E', 61: '2', 62: 'y', 63: 'L', 64: 'U', 65: '@', 66: 'j', 67: 'm', 68: '\"', 69: 'W', 70: 'A', 71: '%', 72: '?', 73: 'l', 74: '\\n', 75: 'a', 76: 'e', 77: '$', 78: '.', 79: 'd', 80: 'Y', 81: '7', 82: 'H'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_to_int)\n",
    "print()\n",
    "print(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create an iteger representation of 'text' (millions of chars as ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "unichr(x) - char from unicode int\n",
    "ord(x) - byte or in from chr() or unichar() respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 78  81  83 118 119  68  54  99 115  75  74 105  33  32  59  38  58  49\n",
      "  77  42 103  70  95 120  80  66 122  39 110  86 116  53  48  88 117 104\n",
      "  41 107  52 114  47  90  45  51  44  67 113  71  56  79  96  98 112  73\n",
      "  57  84 102  40 111  82  69  50 121  76  85  64 106 109  34  87  65  37\n",
      "  63 108  10  97 101  36  46 100  89  55  72]\n",
      "83\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "vocab_int = np.array([ord(c) for c in vocab], dtype=np.int32)\n",
    "print(vocab_int)\n",
    "\n",
    "print(len(vocab))\n",
    "print(len(vocab_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45 35 75 52 30 76 39 13 17 74 74 74 82 75 52 52 62 13 56 75 67 11 73 11 76\n",
      "  8 13 75 39 76 13 75 73 73 13 75 73 11 37 76 14 13 76  3 76 39 62 13 34 28\n",
      " 35 75 52 52 62 13 56 75 67 11 73 62 13 11  8 13 34 28 35 75 52 52 62 13 11\n",
      " 28 13 11 30  8 13 58  4 28 74  4 75 62 78 74 74 60  3 76 39 62 30 35 11 28]\n"
     ]
    }
   ],
   "source": [
    "#chars = np.array([ord(c) for c in text], dtype=np.int32)\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "print(chars[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars_vector,\n",
    "               samples_per_batch,\n",
    "               sample_length,\n",
    "               split_frac=0.9):\n",
    "\n",
    "    x = chars_vector[:-1]\n",
    "    y = chars_vector[1:]\n",
    "\n",
    "    sample_count = len(x)-sample_length+1\n",
    "    \n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    start_range = range(0, sample_count, sample_length)\n",
    "\n",
    "    x_samples = np.array([x[start:start+sample_length] for start in start_range])\n",
    "    y_samples = np.array([y[start:start+sample_length] for start in start_range])\n",
    "\n",
    "    if sample_count > samples_per_batch:\n",
    "\n",
    "        batch_count = len(x_samples) // samples_per_batch\n",
    "        new_length = batch_count * samples_per_batch\n",
    "        end_crop_count = len(x_samples)-new_length\n",
    "\n",
    "        if end_crop_count != 0:\n",
    "            x_samples = x_samples[:-end_crop_count]\n",
    "            y_samples = y_samples[:-end_crop_count]\n",
    "\n",
    "        x_batches = np.array(np.split(x_samples, batch_count))\n",
    "        y_batches = np.array(np.split(y_samples, batch_count))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        x_batches = x_samples\n",
    "        y_batches = y_samples\n",
    "        \n",
    "    \n",
    "    split_idx = int(len(x_batches)*split_frac)\n",
    "\n",
    "    train_x, train_y = x_batches[:split_idx], y_batches[:split_idx]\n",
    "    val_x, val_y = x_batches[split_idx:], y_batches[split_idx:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tx:\n",
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]\n",
      "  [ 9 10 11]\n",
      "  [12 13 14]]]\n",
      "\n",
      "ty:\n",
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]\n",
      "  [ 7  8  9]\n",
      "  [10 11 12]\n",
      "  [13 14 15]]]\n",
      "(1, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "vector = np.arange(17)\n",
    "samples_in_batch = 5\n",
    "sample_length = 3\n",
    "split_frac = 1.\n",
    "\n",
    "tx, ty, _, _ = split_data(vector, samples_in_batch, sample_length, split_frac)\n",
    "\n",
    "print('\\ntx:')\n",
    "print(tx)\n",
    "print('\\nty:')\n",
    "print(ty)\n",
    "\n",
    "print(tx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(tx, ty):\n",
    "    for x, y in zip(tx, ty):\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Creating training and validation sets using function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars_vector=chars, \n",
    "                                            samples_per_batch=100, \n",
    "                                            sample_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape: (178, 100, 100)\n",
      "train_y.shape: (178, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "print('train_x.shape: {}'.format(train_x.shape))\n",
    "print('train_y.shape: {}'.format(train_y.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45, 35, 75, 52, 30, 76, 39, 13, 17, 74, 74, 74, 82, 75, 52, 52, 62,\n",
       "        13, 56, 75, 67, 11, 73, 11, 76,  8, 13, 75, 39, 76, 13, 75, 73, 73,\n",
       "        13, 75, 73, 11, 37, 76, 14, 13, 76,  3, 76, 39, 62, 13, 34, 28, 35,\n",
       "        75, 52, 52, 62, 13, 56, 75, 67, 11, 73, 62, 13, 11,  8, 13, 34, 28,\n",
       "        35, 75, 52, 52, 62, 13, 11, 28, 13, 11, 30,  8, 13, 58,  4, 28, 74,\n",
       "         4, 75, 62, 78, 74, 74, 60,  3, 76, 39, 62, 30, 35, 11, 28],\n",
       "       [20, 13,  4, 75,  8, 13, 11, 28, 13,  7, 58, 28, 56, 34,  8, 11, 58,\n",
       "        28, 13, 11, 28, 13, 30, 35, 76, 13, 49, 51, 73, 58, 28,  8, 37, 62,\n",
       "         8, 27, 13, 35, 58, 34,  8, 76, 78, 13, 55, 35, 76, 13,  4, 11, 56,\n",
       "        76, 13, 35, 75, 79, 74, 79, 11,  8,  7, 58,  3, 76, 39, 76, 79, 13,\n",
       "        30, 35, 75, 30, 13, 30, 35, 76, 13, 35, 34,  8, 51, 75, 28, 79, 13,\n",
       "         4, 75,  8, 13,  7, 75, 39, 39, 62, 11, 28, 20, 13, 58, 28]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0,:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35, 75, 52, 30, 76, 39, 13, 17, 74, 74, 74, 82, 75, 52, 52, 62, 13,\n",
       "        56, 75, 67, 11, 73, 11, 76,  8, 13, 75, 39, 76, 13, 75, 73, 73, 13,\n",
       "        75, 73, 11, 37, 76, 14, 13, 76,  3, 76, 39, 62, 13, 34, 28, 35, 75,\n",
       "        52, 52, 62, 13, 56, 75, 67, 11, 73, 62, 13, 11,  8, 13, 34, 28, 35,\n",
       "        75, 52, 52, 62, 13, 11, 28, 13, 11, 30,  8, 13, 58,  4, 28, 74,  4,\n",
       "        75, 62, 78, 74, 74, 60,  3, 76, 39, 62, 30, 35, 11, 28, 20],\n",
       "       [13,  4, 75,  8, 13, 11, 28, 13,  7, 58, 28, 56, 34,  8, 11, 58, 28,\n",
       "        13, 11, 28, 13, 30, 35, 76, 13, 49, 51, 73, 58, 28,  8, 37, 62,  8,\n",
       "        27, 13, 35, 58, 34,  8, 76, 78, 13, 55, 35, 76, 13,  4, 11, 56, 76,\n",
       "        13, 35, 75, 79, 74, 79, 11,  8,  7, 58,  3, 76, 39, 76, 79, 13, 30,\n",
       "        35, 75, 30, 13, 30, 35, 76, 13, 35, 34,  8, 51, 75, 28, 79, 13,  4,\n",
       "        75,  8, 13,  7, 75, 39, 39, 62, 11, 28, 20, 13, 58, 28, 13]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0, :2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes,\n",
    "              samples_per_batch=50,\n",
    "              sample_length=50,\n",
    "              lstm_size=128, \n",
    "              num_layers=2,\n",
    "              learning_rate=0.001, \n",
    "              grad_clip=5, \n",
    "              sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        samples_per_batch, sample_length = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [samples_per_batch, sample_length], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [samples_per_batch, sample_length], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(samples_per_batch, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, sample_length, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "samples_per_batch = 100\n",
    "sample_length = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.002\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training and validation data...\n",
      "train_x len: 178\n",
      "train_y len: 178\n",
      "val_x len: 20\n",
      "val_y len: 20\n",
      "Data generation complete.\n",
      "Building model...\n",
      "Model built\n",
      "total batches: 178\n",
      "total iteration count: 178\n",
      "Epoch 0/0  Iteration 0/178 Training loss: 4.4236 6.4256 sec/batch\n",
      "Validation loss: 4.27024 Saving checkpoint!\n",
      "Epoch 0/0  Iteration 1/178 Training loss: 4.3506 5.3474 sec/batch\n",
      "Epoch 0/0  Iteration 2/178 Training loss: 4.6816 5.5415 sec/batch\n",
      "Epoch 0/0  Iteration 3/178 Training loss: 4.5882 5.6118 sec/batch\n",
      "Epoch 0/0  Iteration 4/178 Training loss: 4.4483 5.8159 sec/batch\n",
      "Epoch 0/0  Iteration 5/178 Training loss: 4.3196 5.6982 sec/batch\n",
      "Epoch 0/0  Iteration 6/178 Training loss: 4.2095 5.6530 sec/batch\n",
      "Epoch 0/0  Iteration 7/178 Training loss: 4.1144 5.7437 sec/batch\n",
      "Epoch 0/0  Iteration 8/178 Training loss: 4.0389 5.6312 sec/batch\n",
      "Epoch 0/0  Iteration 9/178 Training loss: 3.9698 5.6350 sec/batch\n",
      "Epoch 0/0  Iteration 10/178 Training loss: 3.9030 5.6098 sec/batch\n",
      "Epoch 0/0  Iteration 11/178 Training loss: 3.8495 5.6654 sec/batch\n",
      "Epoch 0/0  Iteration 12/178 Training loss: 3.8059 5.6593 sec/batch\n",
      "Epoch 0/0  Iteration 13/178 Training loss: 3.7672 5.6647 sec/batch\n",
      "Epoch 0/0  Iteration 14/178 Training loss: 3.7339 5.6386 sec/batch\n",
      "Epoch 0/0  Iteration 15/178 Training loss: 3.7040 5.6293 sec/batch\n",
      "Epoch 0/0  Iteration 16/178 Training loss: 3.6777 5.6509 sec/batch\n",
      "Epoch 0/0  Iteration 17/178 Training loss: 3.6529 5.7336 sec/batch\n",
      "Epoch 0/0  Iteration 18/178 Training loss: 3.6279 5.6665 sec/batch\n",
      "Epoch 0/0  Iteration 19/178 Training loss: 3.6063 5.5911 sec/batch\n",
      "Epoch 0/0  Iteration 20/178 Training loss: 3.5869 5.6122 sec/batch\n",
      "Epoch 0/0  Iteration 21/178 Training loss: 3.5714 5.9261 sec/batch\n",
      "Epoch 0/0  Iteration 22/178 Training loss: 3.5536 6.1311 sec/batch\n",
      "Epoch 0/0  Iteration 23/178 Training loss: 3.5383 5.7937 sec/batch\n",
      "Epoch 0/0  Iteration 24/178 Training loss: 3.5215 5.9922 sec/batch\n",
      "Epoch 0/0  Iteration 25/178 Training loss: 3.5078 5.9634 sec/batch\n",
      "Epoch 0/0  Iteration 26/178 Training loss: 3.4943 6.4108 sec/batch\n",
      "Epoch 0/0  Iteration 27/178 Training loss: 3.4835 6.8237 sec/batch\n",
      "Epoch 0/0  Iteration 28/178 Training loss: 3.4724 8.4485 sec/batch\n",
      "Epoch 0/0  Iteration 29/178 Training loss: 3.4626 6.1517 sec/batch\n",
      "Epoch 0/0  Iteration 30/178 Training loss: 3.4525 5.8402 sec/batch\n",
      "Epoch 0/0  Iteration 31/178 Training loss: 3.4427 5.5512 sec/batch\n",
      "Epoch 0/0  Iteration 32/178 Training loss: 3.4359 5.5639 sec/batch\n",
      "Epoch 0/0  Iteration 33/178 Training loss: 3.4273 7.1629 sec/batch\n",
      "Epoch 0/0  Iteration 34/178 Training loss: 3.4188 5.5763 sec/batch\n",
      "Epoch 0/0  Iteration 35/178 Training loss: 3.4107 5.5099 sec/batch\n",
      "Epoch 0/0  Iteration 36/178 Training loss: 3.4026 5.5353 sec/batch\n",
      "Epoch 0/0  Iteration 37/178 Training loss: 3.3954 6.2947 sec/batch\n",
      "Epoch 0/0  Iteration 38/178 Training loss: 3.3895 5.9024 sec/batch\n",
      "Epoch 0/0  Iteration 39/178 Training loss: 3.3839 6.3761 sec/batch\n",
      "Epoch 0/0  Iteration 40/178 Training loss: 3.3792 6.1584 sec/batch\n",
      "Epoch 0/0  Iteration 41/178 Training loss: 3.3731 5.8287 sec/batch\n",
      "Epoch 0/0  Iteration 42/178 Training loss: 3.3686 6.9955 sec/batch\n",
      "Epoch 0/0  Iteration 43/178 Training loss: 3.3634 5.8761 sec/batch\n",
      "Epoch 0/0  Iteration 44/178 Training loss: 3.3570 5.4514 sec/batch\n",
      "Epoch 0/0  Iteration 45/178 Training loss: 3.3520 5.7250 sec/batch\n",
      "Epoch 0/0  Iteration 46/178 Training loss: 3.3469 6.8589 sec/batch\n",
      "Epoch 0/0  Iteration 47/178 Training loss: 3.3413 6.5203 sec/batch\n",
      "Epoch 0/0  Iteration 48/178 Training loss: 3.3360 6.7272 sec/batch\n",
      "Epoch 0/0  Iteration 49/178 Training loss: 3.3318 6.5273 sec/batch\n",
      "Epoch 0/0  Iteration 50/178 Training loss: 3.3279 5.4048 sec/batch\n",
      "Validation loss: 3.08266 Saving checkpoint!\n",
      "Epoch 0/0  Iteration 51/178 Training loss: 3.3234 5.2799 sec/batch\n",
      "Epoch 0/0  Iteration 52/178 Training loss: 3.3196 5.3467 sec/batch\n",
      "Epoch 0/0  Iteration 53/178 Training loss: 3.3160 5.3677 sec/batch\n",
      "Epoch 0/0  Iteration 54/178 Training loss: 3.3114 5.3655 sec/batch\n",
      "Epoch 0/0  Iteration 55/178 Training loss: 3.3080 5.3723 sec/batch\n",
      "Epoch 0/0  Iteration 56/178 Training loss: 3.3058 5.3875 sec/batch\n",
      "Epoch 0/0  Iteration 57/178 Training loss: 3.3085 5.3499 sec/batch\n",
      "Epoch 0/0  Iteration 58/178 Training loss: 3.3116 5.3343 sec/batch\n",
      "Epoch 0/0  Iteration 59/178 Training loss: 3.3127 5.3937 sec/batch\n",
      "Epoch 0/0  Iteration 60/178 Training loss: 3.3113 5.3477 sec/batch\n",
      "Epoch 0/0  Iteration 61/178 Training loss: 3.3078 5.4486 sec/batch\n",
      "Epoch 0/0  Iteration 62/178 Training loss: 3.3042 5.4258 sec/batch\n",
      "Epoch 0/0  Iteration 63/178 Training loss: 3.3000 5.4579 sec/batch\n",
      "Epoch 0/0  Iteration 64/178 Training loss: 3.2971 5.3730 sec/batch\n",
      "Epoch 0/0  Iteration 65/178 Training loss: 3.2939 5.4984 sec/batch\n",
      "Epoch 0/0  Iteration 66/178 Training loss: 3.2900 5.4436 sec/batch\n",
      "Epoch 0/0  Iteration 67/178 Training loss: 3.2863 5.4415 sec/batch\n",
      "Epoch 0/0  Iteration 68/178 Training loss: 3.2826 5.3994 sec/batch\n",
      "Epoch 0/0  Iteration 69/178 Training loss: 3.2790 5.4386 sec/batch\n",
      "Epoch 0/0  Iteration 70/178 Training loss: 3.2755 5.4742 sec/batch\n",
      "Epoch 0/0  Iteration 71/178 Training loss: 3.2723 5.4597 sec/batch\n",
      "Epoch 0/0  Iteration 72/178 Training loss: 3.2697 5.3787 sec/batch\n",
      "Epoch 0/0  Iteration 73/178 Training loss: 3.2665 5.4125 sec/batch\n",
      "Epoch 0/0  Iteration 74/178 Training loss: 3.2630 5.3696 sec/batch\n",
      "Epoch 0/0  Iteration 75/178 Training loss: 3.2608 5.3393 sec/batch\n",
      "Epoch 0/0  Iteration 76/178 Training loss: 3.2574 5.4282 sec/batch\n",
      "Epoch 0/0  Iteration 77/178 Training loss: 3.2537 5.4008 sec/batch\n",
      "Epoch 0/0  Iteration 78/178 Training loss: 3.2497 5.3950 sec/batch\n",
      "Epoch 0/0  Iteration 79/178 Training loss: 3.2462 5.4238 sec/batch\n",
      "Epoch 0/0  Iteration 80/178 Training loss: 3.2429 5.3566 sec/batch\n",
      "Epoch 0/0  Iteration 81/178 Training loss: 3.2399 5.3831 sec/batch\n",
      "Epoch 0/0  Iteration 82/178 Training loss: 3.2365 5.4038 sec/batch\n",
      "Epoch 0/0  Iteration 83/178 Training loss: 3.2320 5.3626 sec/batch\n",
      "Epoch 0/0  Iteration 84/178 Training loss: 3.2304 5.3905 sec/batch\n",
      "Epoch 0/0  Iteration 85/178 Training loss: 3.2285 5.3744 sec/batch\n",
      "Epoch 0/0  Iteration 86/178 Training loss: 3.2254 5.3923 sec/batch\n",
      "Epoch 0/0  Iteration 87/178 Training loss: 3.2227 5.3988 sec/batch\n",
      "Epoch 0/0  Iteration 88/178 Training loss: 3.2195 5.3939 sec/batch\n",
      "Epoch 0/0  Iteration 89/178 Training loss: 3.2169 5.4037 sec/batch\n",
      "Epoch 0/0  Iteration 90/178 Training loss: 3.2143 5.3704 sec/batch\n",
      "Epoch 0/0  Iteration 91/178 Training loss: 3.2116 5.4105 sec/batch\n",
      "Epoch 0/0  Iteration 92/178 Training loss: 3.2093 5.5120 sec/batch\n",
      "Epoch 0/0  Iteration 93/178 Training loss: 3.2061 8.0285 sec/batch\n",
      "Epoch 0/0  Iteration 94/178 Training loss: 3.2032 5.3453 sec/batch\n",
      "Epoch 0/0  Iteration 95/178 Training loss: 3.2004 5.3992 sec/batch\n",
      "Epoch 0/0  Iteration 96/178 Training loss: 3.1969 5.3771 sec/batch\n",
      "Epoch 0/0  Iteration 97/178 Training loss: 3.1935 7.2724 sec/batch\n",
      "Epoch 0/0  Iteration 98/178 Training loss: 3.1899 5.6986 sec/batch\n",
      "Epoch 0/0  Iteration 99/178 Training loss: 3.1860 5.4172 sec/batch\n",
      "Epoch 0/0  Iteration 100/178 Training loss: 3.1830 5.6489 sec/batch\n",
      "Validation loss: 2.78755 Saving checkpoint!\n",
      "Epoch 0/0  Iteration 101/178 Training loss: 3.1787 5.3146 sec/batch\n",
      "Epoch 0/0  Iteration 102/178 Training loss: 3.1750 5.3672 sec/batch\n",
      "Epoch 0/0  Iteration 103/178 Training loss: 3.1716 5.3638 sec/batch\n",
      "Epoch 0/0  Iteration 104/178 Training loss: 3.1680 5.3886 sec/batch\n",
      "Epoch 0/0  Iteration 105/178 Training loss: 3.1639 5.4589 sec/batch\n",
      "Epoch 0/0  Iteration 106/178 Training loss: 3.1601 5.3463 sec/batch\n",
      "Epoch 0/0  Iteration 107/178 Training loss: 3.1566 5.3607 sec/batch\n",
      "Epoch 0/0  Iteration 108/178 Training loss: 3.1524 5.3356 sec/batch\n",
      "Epoch 0/0  Iteration 109/178 Training loss: 3.1480 5.3272 sec/batch\n",
      "Epoch 0/0  Iteration 110/178 Training loss: 3.1441 5.3768 sec/batch\n",
      "Epoch 0/0  Iteration 111/178 Training loss: 3.1403 5.3503 sec/batch\n",
      "Epoch 0/0  Iteration 112/178 Training loss: 3.1362 5.3563 sec/batch\n",
      "Epoch 0/0  Iteration 113/178 Training loss: 3.1321 5.3944 sec/batch\n",
      "Epoch 0/0  Iteration 114/178 Training loss: 3.1280 5.3345 sec/batch\n",
      "Epoch 0/0  Iteration 115/178 Training loss: 3.1239 5.3423 sec/batch\n",
      "Epoch 0/0  Iteration 116/178 Training loss: 3.1192 5.3792 sec/batch\n",
      "Epoch 0/0  Iteration 117/178 Training loss: 3.1151 5.3228 sec/batch\n",
      "Epoch 0/0  Iteration 118/178 Training loss: 3.1110 5.3876 sec/batch\n",
      "Epoch 0/0  Iteration 119/178 Training loss: 3.1064 6.7768 sec/batch\n",
      "Epoch 0/0  Iteration 120/178 Training loss: 3.1020 5.3693 sec/batch\n",
      "Epoch 0/0  Iteration 121/178 Training loss: 3.0978 5.3576 sec/batch\n",
      "Epoch 0/0  Iteration 122/178 Training loss: 3.0933 5.3389 sec/batch\n",
      "Epoch 0/0  Iteration 123/178 Training loss: 3.0893 5.3237 sec/batch\n",
      "Epoch 0/0  Iteration 124/178 Training loss: 3.0858 5.3919 sec/batch\n",
      "Epoch 0/0  Iteration 125/178 Training loss: 3.0820 5.3258 sec/batch\n",
      "Epoch 0/0  Iteration 126/178 Training loss: 3.0784 5.3140 sec/batch\n",
      "Epoch 0/0  Iteration 127/178 Training loss: 3.0741 5.4541 sec/batch\n",
      "Epoch 0/0  Iteration 128/178 Training loss: 3.0701 7.0226 sec/batch\n",
      "Epoch 0/0  Iteration 129/178 Training loss: 3.0656 6.6807 sec/batch\n",
      "Epoch 0/0  Iteration 130/178 Training loss: 3.0609 5.3402 sec/batch\n",
      "Epoch 0/0  Iteration 131/178 Training loss: 3.0566 5.6198 sec/batch\n",
      "Epoch 0/0  Iteration 132/178 Training loss: 3.0526 5.3463 sec/batch\n",
      "Epoch 0/0  Iteration 133/178 Training loss: 3.0487 5.3582 sec/batch\n",
      "Epoch 0/0  Iteration 134/178 Training loss: 3.0452 5.3925 sec/batch\n",
      "Epoch 0/0  Iteration 135/178 Training loss: 3.0413 5.4176 sec/batch\n",
      "Epoch 0/0  Iteration 136/178 Training loss: 3.0374 5.4487 sec/batch\n",
      "Epoch 0/0  Iteration 137/178 Training loss: 3.0338 5.7329 sec/batch\n",
      "Epoch 0/0  Iteration 138/178 Training loss: 3.0300 5.3925 sec/batch\n",
      "Epoch 0/0  Iteration 139/178 Training loss: 3.0263 5.8438 sec/batch\n",
      "Epoch 0/0  Iteration 140/178 Training loss: 3.0226 5.8730 sec/batch\n",
      "Epoch 0/0  Iteration 141/178 Training loss: 3.0187 5.4534 sec/batch\n",
      "Epoch 0/0  Iteration 142/178 Training loss: 3.0153 6.5077 sec/batch\n",
      "Epoch 0/0  Iteration 143/178 Training loss: 3.0113 5.9244 sec/batch\n",
      "Epoch 0/0  Iteration 144/178 Training loss: 3.0077 5.7102 sec/batch\n",
      "Epoch 0/0  Iteration 145/178 Training loss: 3.0040 7.9365 sec/batch\n",
      "Epoch 0/0  Iteration 146/178 Training loss: 3.0003 6.0129 sec/batch\n",
      "Epoch 0/0  Iteration 147/178 Training loss: 2.9969 6.0282 sec/batch\n",
      "Epoch 0/0  Iteration 148/178 Training loss: 2.9935 5.7589 sec/batch\n",
      "Epoch 0/0  Iteration 149/178 Training loss: 2.9903 5.6867 sec/batch\n",
      "Epoch 0/0  Iteration 150/178 Training loss: 2.9871 5.6986 sec/batch\n",
      "Validation loss: 2.4228 Saving checkpoint!\n",
      "Epoch 0/0  Iteration 151/178 Training loss: 2.9836 5.5293 sec/batch\n",
      "Epoch 0/0  Iteration 152/178 Training loss: 2.9806 5.6844 sec/batch\n",
      "Epoch 0/0  Iteration 153/178 Training loss: 2.9774 5.5778 sec/batch\n",
      "Epoch 0/0  Iteration 154/178 Training loss: 2.9740 5.6028 sec/batch\n",
      "Epoch 0/0  Iteration 155/178 Training loss: 2.9704 5.8044 sec/batch\n",
      "Epoch 0/0  Iteration 156/178 Training loss: 2.9669 5.8268 sec/batch\n",
      "Epoch 0/0  Iteration 157/178 Training loss: 2.9634 5.8129 sec/batch\n",
      "Epoch 0/0  Iteration 158/178 Training loss: 2.9602 5.5197 sec/batch\n",
      "Epoch 0/0  Iteration 159/178 Training loss: 2.9570 5.4418 sec/batch\n",
      "Epoch 0/0  Iteration 160/178 Training loss: 2.9538 5.5740 sec/batch\n",
      "Epoch 0/0  Iteration 161/178 Training loss: 2.9501 5.3770 sec/batch\n",
      "Epoch 0/0  Iteration 162/178 Training loss: 2.9468 5.3939 sec/batch\n",
      "Epoch 0/0  Iteration 163/178 Training loss: 2.9436 5.3715 sec/batch\n",
      "Epoch 0/0  Iteration 164/178 Training loss: 2.9405 5.3630 sec/batch\n",
      "Epoch 0/0  Iteration 165/178 Training loss: 2.9372 5.3547 sec/batch\n",
      "Epoch 0/0  Iteration 166/178 Training loss: 2.9343 5.3058 sec/batch\n",
      "Epoch 0/0  Iteration 167/178 Training loss: 2.9311 5.3658 sec/batch\n",
      "Epoch 0/0  Iteration 168/178 Training loss: 2.9280 5.3210 sec/batch\n",
      "Epoch 0/0  Iteration 169/178 Training loss: 2.9246 5.3289 sec/batch\n",
      "Epoch 0/0  Iteration 170/178 Training loss: 2.9213 5.3552 sec/batch\n",
      "Epoch 0/0  Iteration 171/178 Training loss: 2.9180 5.3680 sec/batch\n",
      "Epoch 0/0  Iteration 172/178 Training loss: 2.9147 5.3495 sec/batch\n",
      "Epoch 0/0  Iteration 173/178 Training loss: 2.9119 6.4689 sec/batch\n",
      "Epoch 0/0  Iteration 174/178 Training loss: 2.9095 5.3452 sec/batch\n",
      "Epoch 0/0  Iteration 175/178 Training loss: 2.9068 6.0777 sec/batch\n",
      "Epoch 0/0  Iteration 176/178 Training loss: 2.9043 5.7920 sec/batch\n",
      "Epoch 0/0  Iteration 177/178 Training loss: 2.9019 6.9962 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epoch_count = 1\n",
    "# Save every N iterations\n",
    "save_every_n = 50\n",
    "\n",
    "print('Generating training and validation data...')\n",
    "train_x, train_y, val_x, val_y = split_data(chars, samples_per_batch, sample_length)\n",
    "print('train_x len: {}'.format(len(train_x)))\n",
    "print('train_y len: {}'.format(len(train_y)))\n",
    "print('val_x len: {}'.format(len(val_x)))\n",
    "print('val_y len: {}'.format(len(val_y)))\n",
    "print('Data generation complete.')\n",
    "\n",
    "print('Building model...')\n",
    "model = build_rnn(len(vocab_int), \n",
    "                  samples_per_batch=samples_per_batch,\n",
    "                  sample_length=sample_length,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "print('Model built')\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    n_batches = len(train_x)\n",
    "    print('total batches: {}'.format(n_batches))\n",
    "    \n",
    "    iteration_count = n_batches * epoch_count\n",
    "    print('total iteration count: {}'.format(iteration_count))\n",
    "    \n",
    "    for epoch in range(epoch_count):\n",
    "        \n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        \n",
    "        for i, (x, y) in enumerate(get_batch(train_x, train_y)):\n",
    "#             print(train_x[i])\n",
    "#             print(x.shape)\n",
    "            iteration = epoch*n_batches + i + 1\n",
    "            start = time.time()\n",
    "            \n",
    "            feed = {model.inputs: x, \n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            \n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(epoch, epoch_count-1),\n",
    "                  'Iteration {}/{}'.format(iteration, iteration_count),\n",
    "                  'Training loss: {:.4f}'.format(loss/(i+1)),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iteration_count):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch(val_x, val_y):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i150_l512_v2.423.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i0_l512_v4.270.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i50_l512_v3.083.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i100_l512_v2.788.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i150_l512_v2.423.ckpt\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            #x[0,0] = ord(c)# vocab_to_int[c]\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab), 1)\n",
    "        #samples.append(chr(c))#(int_to_vocab[c])\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            #samples.append(chr(c))# (int_to_vocab[c])\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sard solend the th ware hhe salede held sad and ont as tham tire wer orad whe hares atd ont to mer tot his the sat to lo the cer had anler ortant hes he tharse sos had whas, word\n",
      "thes when whor athen. Ad anly onter, an sotise at to tele tinle the sorente he whan to simas athe anse the wolt he sot ot as te al thereran her of ard wal the he ardensit orat ant otin the te thin tislese she thar simas athang wel so sothe tha sar at hire at hor wore she she couthe he the asser sind afthe whr sos han and sard af and\n",
      "athers, bath the ans ald warteses aler as olly. Anlderyinn hot wersid too tor the se the sore thes ase she the colles ortaring on anerand and and wald whes sh mer he shers wot ar ant an tha sar ond,\n",
      "be so sar at ald wit her at hom\n",
      "ther hit houg har ante wol sith the whr wan hire therensens he cererent an ald ald,.\n",
      "\"\"\"\n",
      " heut sote siman asese wild hor wan toun tar too lethen att and so tal tint an hersothin wasd, and and onle and as otel te sere sis atito hhe san sorere ad sor hasd asd the sosis tho tan att to te sher tart one atterer on the sorin ther. \n",
      "nothar ant old, bas ha de anded.\"\n",
      "He thaled and the teus an ar ter the\n",
      " hhe sis ans thim an tha sosind ant ter sathan te ther. \n",
      "ot to tot hin tho sate hored ward wot thar, whar tathe wherd and at ha sas onthite se ware the wasd we cisl oth sherige the wot th whers ther, whis sanland anl ole and\n",
      "wasle the sothe tho sading toude an sothe hhe and an walt hin as the sont the shisged. I hole bote th me whe se wall worind an to sh ad tor sas here bon he arlos an she sils an tor hersig tarid sothe heris on an te the\n",
      "lald ante sar te sot an this toul the woth serad ande and shed an the wat th alless wher wint, thing tot hha tand of sot an sinl, tor he woth an ho woude woth thas hir hor wate he cole tere an andes wer tared thes ses and time ses tis hee wer soridedes ald tin to he timed the tis ofosith the hasd.\n",
      "\n",
      "Toe lasely, and wint and so tore the sot hom sorerat he hhing\n",
      "ther ald wer she whan wint hored. \"e\n",
      "th on wor the\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i150_l512_v2.423.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"The\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
