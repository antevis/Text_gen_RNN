{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Text-gen RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load text into a one huge string (millions of chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every unhappy family is unhappy in its own\n",
      "way.\n",
      "\n",
      "Everythin\n",
      "1966145\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create vocabulary : a set of all chars of which 'text' consists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n', '.', 'I', '4', '6', 'J', 'K', 'F', ' ', 'H', 'e', 'L', 'S', '3', '0', ',', 'u', 'o', '2', 'V', '8', 'B', 'c', 'R', 'v', '!', '1', 'O', '_', 'a', 'w', ':', 'G', 'b', '7', 'P', 'h', 'k', 'q', 'A', 'm', 'y', 'z', 'X', '9', 'Y', 'C', 'r', '(', 'g', '\"', ';', 't', 'i', \"'\", 'N', 'Q', 'T', 'l', 'p', '-', 'M', 'j', 'x', 'Z', 'U', '`', 'W', ')', '5', 'f', 'E', 's', 'D', '?', '\\n', 'd'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create vocab_to_int and int_to_vocab. These are dictionaries. You won't need it. Hopefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 0, 'r': 47, '.': 1, '4': 3, '6': 4, 'J': 5, 'K': 6, 'F': 7, 'H': 9, '0': 14, 'e': 10, 'L': 11, 'S': 12, 'U': 65, ',': 15, 'u': 16, 'o': 17, '-': 60, '2': 18, 'V': 19, '8': 20, 'B': 21, 'c': 22, 'R': 23, 'v': 24, 'I': 2, '!': 25, ' ': 8, '`': 66, '1': 26, ')': 68, 'O': 27, 'N': 55, '_': 28, ':': 31, 'w': 30, 'G': 32, 'b': 33, 'M': 61, 'P': 35, 'h': 36, 'k': 37, 'q': 38, 'A': 39, 'm': 40, 'z': 42, '9': 44, 'Y': 45, 'C': 46, '(': 48, 'g': 49, '\"': 50, ';': 51, 't': 52, 'i': 53, \"'\": 54, 'X': 43, 'Q': 56, 'T': 57, 'l': 58, 'y': 41, 'Z': 64, 'j': 62, 'p': 59, '?': 74, '7': 34, 'W': 67, '3': 13, '5': 69, 'f': 70, 'E': 71, 's': 72, 'D': 73, 'a': 29, '\\n': 75, 'x': 63, 'd': 76}\n",
      "\n",
      "{0: 'n', 1: '.', 2: 'I', 3: '4', 4: '6', 5: 'J', 6: 'K', 7: 'F', 8: ' ', 9: 'H', 10: 'e', 11: 'L', 12: 'S', 13: '3', 14: '0', 15: ',', 16: 'u', 17: 'o', 18: '2', 19: 'V', 20: '8', 21: 'B', 22: 'c', 23: 'R', 24: 'v', 25: '!', 26: '1', 27: 'O', 28: '_', 29: 'a', 30: 'w', 31: ':', 32: 'G', 33: 'b', 34: '7', 35: 'P', 36: 'h', 37: 'k', 38: 'q', 39: 'A', 40: 'm', 41: 'y', 42: 'z', 43: 'X', 44: '9', 45: 'Y', 46: 'C', 47: 'r', 48: '(', 49: 'g', 50: '\"', 51: ';', 52: 't', 53: 'i', 54: \"'\", 55: 'N', 56: 'Q', 57: 'T', 58: 'l', 59: 'p', 60: '-', 61: 'M', 62: 'j', 63: 'x', 64: 'Z', 65: 'U', 66: '`', 67: 'W', 68: ')', 69: '5', 70: 'f', 71: 'E', 72: 's', 73: 'D', 74: '?', 75: '\\n', 76: 'd'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_to_int)\n",
    "print()\n",
    "print(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create an iteger representation of 'text' (millions of chars as ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46 36 29 59 52 10 47  8 26 75 75 75  9 29 59 59 41  8 70 29 40 53 58 53 10\n",
      " 72  8 29 47 10  8 29 58 58  8 29 58 53 37 10 51  8 10 24 10 47 41  8 16  0\n",
      " 36 29 59 59 41  8 70 29 40 53 58 41  8 53 72  8 16  0 36 29 59 59 41  8 53\n",
      "  0  8 53 52 72  8 17 30  0 75 30 29 41  1 75 75 71 24 10 47 41 52 36 53  0]\n"
     ]
    }
   ],
   "source": [
    "#chars = np.array([ord(c) for c in text], dtype=np.int32)\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "print(chars[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 176800)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)\n",
    "\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Creating training and validation sets using function defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "        outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.002\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, train_writer, test_writer):\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=100)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Use the line below to load a checkpoint and resume training\n",
    "        #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "\n",
    "        n_batches = int(train_x.shape[1]/num_steps)\n",
    "        iterations = n_batches * epochs\n",
    "        for e in range(epochs):\n",
    "\n",
    "            # Train network\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            loss = 0\n",
    "            for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "                iteration = e*n_batches + b\n",
    "                start = time.time()\n",
    "                feed = {model.inputs: x,\n",
    "                        model.targets: y,\n",
    "                        model.keep_prob: 0.5,\n",
    "                        model.initial_state: new_state}\n",
    "                summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                              model.final_state, model.optimizer], \n",
    "                                                              feed_dict=feed)\n",
    "                loss += batch_loss\n",
    "                end = time.time()\n",
    "                print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                      'Iteration {}/{}'.format(iteration, iterations),\n",
    "                      'Training loss: {:.4f}'.format(loss/b),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "                train_writer.add_summary(summary, iteration)\n",
    "                \n",
    "                if (iteration%200 == 0) or (iteration == iterations):\n",
    "                    # Check performance, notice dropout has been set to 1\n",
    "                    val_loss = []\n",
    "                    new_state = sess.run(model.initial_state)\n",
    "                    for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                        feed = {model.inputs: x,\n",
    "                                model.targets: y,\n",
    "                                model.keep_prob: 1.,\n",
    "                                model.initial_state: new_state}\n",
    "                        summary, batch_loss, new_state = sess.run([model.merged,\n",
    "                                                                   model.cost, \n",
    "                                                                   model.final_state], \n",
    "                                                                  feed_dict=feed)\n",
    "                        val_loss.append(batch_loss)\n",
    "\n",
    "                    test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                    print('Validation loss:', np.mean(val_loss),\n",
    "                          'Saving checkpoint!')\n",
    "                    # Below command is commented out in Mat's version\n",
    "                    saver.save(sess, \n",
    "                               \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, \n",
    "                                                                             lstm_size, \n",
    "                                                                             np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/3520 Training loss: 4.3420 0.6371 sec/batch\n",
      "Epoch 1/20  Iteration 2/3520 Training loss: 4.3169 0.1283 sec/batch\n",
      "Epoch 1/20  Iteration 3/3520 Training loss: 4.2543 0.1137 sec/batch\n",
      "Epoch 1/20  Iteration 4/3520 Training loss: 4.1053 0.1096 sec/batch\n",
      "Epoch 1/20  Iteration 5/3520 Training loss: 3.9753 0.1055 sec/batch\n",
      "Epoch 1/20  Iteration 6/3520 Training loss: 3.8783 0.0985 sec/batch\n",
      "Epoch 1/20  Iteration 7/3520 Training loss: 3.8034 0.0944 sec/batch\n",
      "Epoch 1/20  Iteration 8/3520 Training loss: 3.7427 0.0929 sec/batch\n",
      "Epoch 1/20  Iteration 9/3520 Training loss: 3.6939 0.0887 sec/batch\n",
      "Epoch 1/20  Iteration 10/3520 Training loss: 3.6537 0.0883 sec/batch\n",
      "Epoch 1/20  Iteration 11/3520 Training loss: 3.6174 0.0857 sec/batch\n",
      "Epoch 1/20  Iteration 12/3520 Training loss: 3.5862 0.0887 sec/batch\n",
      "Epoch 1/20  Iteration 13/3520 Training loss: 3.5597 0.0878 sec/batch\n",
      "Epoch 1/20  Iteration 14/3520 Training loss: 3.5345 0.0875 sec/batch\n",
      "Epoch 1/20  Iteration 15/3520 Training loss: 3.5117 0.0857 sec/batch\n",
      "Epoch 1/20  Iteration 16/3520 Training loss: 3.4918 0.0856 sec/batch\n",
      "Epoch 1/20  Iteration 17/3520 Training loss: 3.4741 0.0879 sec/batch\n",
      "Epoch 1/20  Iteration 18/3520 Training loss: 3.4584 0.0892 sec/batch\n",
      "Epoch 1/20  Iteration 19/3520 Training loss: 3.4425 0.0886 sec/batch\n",
      "Epoch 1/20  Iteration 20/3520 Training loss: 3.4292 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 21/3520 Training loss: 3.4166 0.0850 sec/batch\n",
      "Epoch 1/20  Iteration 22/3520 Training loss: 3.4049 0.0840 sec/batch\n",
      "Epoch 1/20  Iteration 23/3520 Training loss: 3.3940 0.0882 sec/batch\n",
      "Epoch 1/20  Iteration 24/3520 Training loss: 3.3841 0.0883 sec/batch\n",
      "Epoch 1/20  Iteration 25/3520 Training loss: 3.3747 0.0865 sec/batch\n",
      "Epoch 1/20  Iteration 26/3520 Training loss: 3.3653 0.0875 sec/batch\n",
      "Epoch 1/20  Iteration 27/3520 Training loss: 3.3567 0.0883 sec/batch\n",
      "Epoch 1/20  Iteration 28/3520 Training loss: 3.3480 0.0848 sec/batch\n",
      "Epoch 1/20  Iteration 29/3520 Training loss: 3.3405 0.0869 sec/batch\n",
      "Epoch 1/20  Iteration 30/3520 Training loss: 3.3332 0.0842 sec/batch\n",
      "Epoch 1/20  Iteration 31/3520 Training loss: 3.3257 0.0863 sec/batch\n",
      "Epoch 1/20  Iteration 32/3520 Training loss: 3.3187 0.0861 sec/batch\n",
      "Epoch 1/20  Iteration 33/3520 Training loss: 3.3123 0.0877 sec/batch\n",
      "Epoch 1/20  Iteration 34/3520 Training loss: 3.3068 0.0881 sec/batch\n",
      "Epoch 1/20  Iteration 35/3520 Training loss: 3.3006 0.0865 sec/batch\n",
      "Epoch 1/20  Iteration 36/3520 Training loss: 3.2952 0.0886 sec/batch\n",
      "Epoch 1/20  Iteration 37/3520 Training loss: 3.2900 0.0863 sec/batch\n",
      "Epoch 1/20  Iteration 38/3520 Training loss: 3.2849 0.0885 sec/batch\n",
      "Epoch 1/20  Iteration 39/3520 Training loss: 3.2802 0.0884 sec/batch\n",
      "Epoch 1/20  Iteration 40/3520 Training loss: 3.2755 0.0845 sec/batch\n",
      "Epoch 1/20  Iteration 41/3520 Training loss: 3.2711 0.0875 sec/batch\n",
      "Epoch 1/20  Iteration 42/3520 Training loss: 3.2665 0.0844 sec/batch\n",
      "Epoch 1/20  Iteration 43/3520 Training loss: 3.2622 0.0862 sec/batch\n",
      "Epoch 1/20  Iteration 44/3520 Training loss: 3.2575 0.0865 sec/batch\n",
      "Epoch 1/20  Iteration 45/3520 Training loss: 3.2532 0.0868 sec/batch\n",
      "Epoch 1/20  Iteration 46/3520 Training loss: 3.2490 0.0867 sec/batch\n",
      "Epoch 1/20  Iteration 47/3520 Training loss: 3.2450 0.0882 sec/batch\n",
      "Epoch 1/20  Iteration 48/3520 Training loss: 3.2412 0.0884 sec/batch\n",
      "Epoch 1/20  Iteration 49/3520 Training loss: 3.2376 0.0875 sec/batch\n",
      "Epoch 1/20  Iteration 50/3520 Training loss: 3.2341 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 51/3520 Training loss: 3.2308 0.0881 sec/batch\n",
      "Epoch 1/20  Iteration 52/3520 Training loss: 3.2276 0.0857 sec/batch\n",
      "Epoch 1/20  Iteration 53/3520 Training loss: 3.2241 0.0863 sec/batch\n",
      "Epoch 1/20  Iteration 54/3520 Training loss: 3.2211 0.0840 sec/batch\n",
      "Epoch 1/20  Iteration 55/3520 Training loss: 3.2177 0.0878 sec/batch\n",
      "Epoch 1/20  Iteration 56/3520 Training loss: 3.2145 0.0852 sec/batch\n",
      "Epoch 1/20  Iteration 57/3520 Training loss: 3.2110 0.0884 sec/batch\n",
      "Epoch 1/20  Iteration 58/3520 Training loss: 3.2075 0.0876 sec/batch\n",
      "Epoch 1/20  Iteration 59/3520 Training loss: 3.2042 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 60/3520 Training loss: 3.2008 0.0875 sec/batch\n",
      "Epoch 1/20  Iteration 61/3520 Training loss: 3.1974 0.0882 sec/batch\n",
      "Epoch 1/20  Iteration 62/3520 Training loss: 3.1942 0.0884 sec/batch\n",
      "Epoch 1/20  Iteration 63/3520 Training loss: 3.1909 0.0881 sec/batch\n",
      "Epoch 1/20  Iteration 64/3520 Training loss: 3.1871 0.0883 sec/batch\n",
      "Epoch 1/20  Iteration 65/3520 Training loss: 3.1838 0.0870 sec/batch\n",
      "Epoch 1/20  Iteration 66/3520 Training loss: 3.1801 0.0855 sec/batch\n",
      "Epoch 1/20  Iteration 67/3520 Training loss: 3.1768 0.0866 sec/batch\n",
      "Epoch 1/20  Iteration 68/3520 Training loss: 3.1733 0.0866 sec/batch\n",
      "Epoch 1/20  Iteration 69/3520 Training loss: 3.1696 0.0879 sec/batch\n",
      "Epoch 1/20  Iteration 70/3520 Training loss: 3.1662 0.0864 sec/batch\n",
      "Epoch 1/20  Iteration 71/3520 Training loss: 3.1626 0.0987 sec/batch\n",
      "Epoch 1/20  Iteration 72/3520 Training loss: 3.1592 0.0873 sec/batch\n",
      "Epoch 1/20  Iteration 73/3520 Training loss: 3.1557 0.0848 sec/batch\n",
      "Epoch 1/20  Iteration 74/3520 Training loss: 3.1518 0.0860 sec/batch\n",
      "Epoch 1/20  Iteration 75/3520 Training loss: 3.1478 0.0869 sec/batch\n",
      "Epoch 1/20  Iteration 76/3520 Training loss: 3.1439 0.0866 sec/batch\n",
      "Epoch 1/20  Iteration 77/3520 Training loss: 3.1400 0.0872 sec/batch\n",
      "Epoch 1/20  Iteration 78/3520 Training loss: 3.1358 0.0861 sec/batch\n",
      "Epoch 1/20  Iteration 79/3520 Training loss: 3.1318 0.0868 sec/batch\n",
      "Epoch 1/20  Iteration 80/3520 Training loss: 3.1277 0.0872 sec/batch\n",
      "Epoch 1/20  Iteration 81/3520 Training loss: 3.1236 0.0882 sec/batch\n",
      "Epoch 1/20  Iteration 82/3520 Training loss: 3.1193 0.0889 sec/batch\n",
      "Epoch 1/20  Iteration 83/3520 Training loss: 3.1151 0.0907 sec/batch\n",
      "Epoch 1/20  Iteration 84/3520 Training loss: 3.1112 0.0882 sec/batch\n",
      "Epoch 1/20  Iteration 85/3520 Training loss: 3.1069 0.0902 sec/batch\n",
      "Epoch 1/20  Iteration 86/3520 Training loss: 3.1026 0.0858 sec/batch\n",
      "Epoch 1/20  Iteration 87/3520 Training loss: 3.0983 0.0847 sec/batch\n",
      "Epoch 1/20  Iteration 88/3520 Training loss: 3.0939 0.0835 sec/batch\n",
      "Epoch 1/20  Iteration 89/3520 Training loss: 3.0896 0.0895 sec/batch\n",
      "Epoch 1/20  Iteration 90/3520 Training loss: 3.0853 0.0911 sec/batch\n",
      "Epoch 1/20  Iteration 91/3520 Training loss: 3.0809 0.0858 sec/batch\n",
      "Epoch 1/20  Iteration 92/3520 Training loss: 3.0768 0.0881 sec/batch\n",
      "Epoch 1/20  Iteration 93/3520 Training loss: 3.0732 0.0859 sec/batch\n",
      "Epoch 1/20  Iteration 94/3520 Training loss: 3.0690 0.0860 sec/batch\n",
      "Epoch 1/20  Iteration 95/3520 Training loss: 3.0648 0.0885 sec/batch\n",
      "Epoch 1/20  Iteration 96/3520 Training loss: 3.0605 0.0853 sec/batch\n",
      "Epoch 1/20  Iteration 97/3520 Training loss: 3.0568 0.0835 sec/batch\n",
      "Epoch 1/20  Iteration 98/3520 Training loss: 3.0527 0.0857 sec/batch\n",
      "Epoch 1/20  Iteration 99/3520 Training loss: 3.0486 0.0892 sec/batch\n",
      "Epoch 1/20  Iteration 100/3520 Training loss: 3.0441 0.0888 sec/batch\n",
      "Epoch 1/20  Iteration 101/3520 Training loss: 3.0400 0.0898 sec/batch\n",
      "Epoch 1/20  Iteration 102/3520 Training loss: 3.0360 0.0894 sec/batch\n",
      "Epoch 1/20  Iteration 103/3520 Training loss: 3.0318 0.0841 sec/batch\n",
      "Epoch 1/20  Iteration 104/3520 Training loss: 3.0274 0.0984 sec/batch\n",
      "Epoch 1/20  Iteration 105/3520 Training loss: 3.0232 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 106/3520 Training loss: 3.0191 0.0855 sec/batch\n",
      "Epoch 1/20  Iteration 107/3520 Training loss: 3.0146 0.0900 sec/batch\n",
      "Epoch 1/20  Iteration 108/3520 Training loss: 3.0101 0.0845 sec/batch\n",
      "Epoch 1/20  Iteration 109/3520 Training loss: 3.0060 0.0864 sec/batch\n",
      "Epoch 1/20  Iteration 110/3520 Training loss: 3.0016 0.0900 sec/batch\n",
      "Epoch 1/20  Iteration 111/3520 Training loss: 2.9971 0.0866 sec/batch\n",
      "Epoch 1/20  Iteration 112/3520 Training loss: 2.9928 0.0857 sec/batch\n",
      "Epoch 1/20  Iteration 113/3520 Training loss: 2.9886 0.0851 sec/batch\n",
      "Epoch 1/20  Iteration 114/3520 Training loss: 2.9843 0.0862 sec/batch\n",
      "Epoch 1/20  Iteration 115/3520 Training loss: 2.9800 0.0854 sec/batch\n",
      "Epoch 1/20  Iteration 116/3520 Training loss: 2.9759 0.0872 sec/batch\n",
      "Epoch 1/20  Iteration 117/3520 Training loss: 2.9718 0.0877 sec/batch\n",
      "Epoch 1/20  Iteration 118/3520 Training loss: 2.9676 0.0836 sec/batch\n",
      "Epoch 1/20  Iteration 119/3520 Training loss: 2.9637 0.0873 sec/batch\n",
      "Epoch 1/20  Iteration 120/3520 Training loss: 2.9595 0.0872 sec/batch\n",
      "Epoch 1/20  Iteration 121/3520 Training loss: 2.9556 0.0864 sec/batch\n",
      "Epoch 1/20  Iteration 122/3520 Training loss: 2.9518 0.0877 sec/batch\n",
      "Epoch 1/20  Iteration 123/3520 Training loss: 2.9477 0.0853 sec/batch\n",
      "Epoch 1/20  Iteration 124/3520 Training loss: 2.9439 0.0867 sec/batch\n",
      "Epoch 1/20  Iteration 125/3520 Training loss: 2.9403 0.0884 sec/batch\n",
      "Epoch 1/20  Iteration 126/3520 Training loss: 2.9365 0.0851 sec/batch\n",
      "Epoch 1/20  Iteration 127/3520 Training loss: 2.9326 0.0872 sec/batch\n",
      "Epoch 1/20  Iteration 128/3520 Training loss: 2.9288 0.0864 sec/batch\n",
      "Epoch 1/20  Iteration 129/3520 Training loss: 2.9249 0.0922 sec/batch\n",
      "Epoch 1/20  Iteration 130/3520 Training loss: 2.9211 0.0822 sec/batch\n",
      "Epoch 1/20  Iteration 131/3520 Training loss: 2.9175 0.0845 sec/batch\n",
      "Epoch 1/20  Iteration 132/3520 Training loss: 2.9138 0.0858 sec/batch\n",
      "Epoch 1/20  Iteration 133/3520 Training loss: 2.9104 0.0872 sec/batch\n",
      "Epoch 1/20  Iteration 134/3520 Training loss: 2.9067 0.0887 sec/batch\n",
      "Epoch 1/20  Iteration 135/3520 Training loss: 2.9033 0.0883 sec/batch\n",
      "Epoch 1/20  Iteration 136/3520 Training loss: 2.8999 0.0875 sec/batch\n",
      "Epoch 1/20  Iteration 137/3520 Training loss: 2.8966 0.0883 sec/batch\n",
      "Epoch 1/20  Iteration 138/3520 Training loss: 2.8931 0.0867 sec/batch\n",
      "Epoch 1/20  Iteration 139/3520 Training loss: 2.8897 0.0861 sec/batch\n",
      "Epoch 1/20  Iteration 140/3520 Training loss: 2.8864 0.0847 sec/batch\n",
      "Epoch 1/20  Iteration 141/3520 Training loss: 2.8831 0.0851 sec/batch\n",
      "Epoch 1/20  Iteration 142/3520 Training loss: 2.8798 0.0853 sec/batch\n",
      "Epoch 1/20  Iteration 143/3520 Training loss: 2.8765 0.0863 sec/batch\n",
      "Epoch 1/20  Iteration 144/3520 Training loss: 2.8732 0.0848 sec/batch\n",
      "Epoch 1/20  Iteration 145/3520 Training loss: 2.8700 0.0854 sec/batch\n",
      "Epoch 1/20  Iteration 146/3520 Training loss: 2.8667 0.0865 sec/batch\n",
      "Epoch 1/20  Iteration 147/3520 Training loss: 2.8635 0.0858 sec/batch\n",
      "Epoch 1/20  Iteration 148/3520 Training loss: 2.8606 0.0914 sec/batch\n",
      "Epoch 1/20  Iteration 149/3520 Training loss: 2.8576 0.0837 sec/batch\n",
      "Epoch 1/20  Iteration 150/3520 Training loss: 2.8546 0.0884 sec/batch\n",
      "Epoch 1/20  Iteration 151/3520 Training loss: 2.8515 0.0882 sec/batch\n",
      "Epoch 1/20  Iteration 152/3520 Training loss: 2.8484 0.0843 sec/batch\n",
      "Epoch 1/20  Iteration 153/3520 Training loss: 2.8454 0.0860 sec/batch\n",
      "Epoch 1/20  Iteration 154/3520 Training loss: 2.8424 0.0852 sec/batch\n",
      "Epoch 1/20  Iteration 155/3520 Training loss: 2.8395 0.0868 sec/batch\n",
      "Epoch 1/20  Iteration 156/3520 Training loss: 2.8363 0.0857 sec/batch\n",
      "Epoch 1/20  Iteration 157/3520 Training loss: 2.8333 0.0862 sec/batch\n",
      "Epoch 1/20  Iteration 158/3520 Training loss: 2.8304 0.0866 sec/batch\n",
      "Epoch 1/20  Iteration 159/3520 Training loss: 2.8273 0.0853 sec/batch\n",
      "Epoch 1/20  Iteration 160/3520 Training loss: 2.8244 0.0853 sec/batch\n",
      "Epoch 1/20  Iteration 161/3520 Training loss: 2.8216 0.0857 sec/batch\n",
      "Epoch 1/20  Iteration 162/3520 Training loss: 2.8186 0.0876 sec/batch\n",
      "Epoch 1/20  Iteration 163/3520 Training loss: 2.8157 0.0876 sec/batch\n",
      "Epoch 1/20  Iteration 164/3520 Training loss: 2.8126 0.0851 sec/batch\n",
      "Epoch 1/20  Iteration 165/3520 Training loss: 2.8097 0.0875 sec/batch\n",
      "Epoch 1/20  Iteration 166/3520 Training loss: 2.8069 0.0887 sec/batch\n",
      "Epoch 1/20  Iteration 167/3520 Training loss: 2.8040 0.0892 sec/batch\n",
      "Epoch 1/20  Iteration 168/3520 Training loss: 2.8013 0.0855 sec/batch\n",
      "Epoch 1/20  Iteration 169/3520 Training loss: 2.7986 0.0848 sec/batch\n",
      "Epoch 1/20  Iteration 170/3520 Training loss: 2.7957 0.0848 sec/batch\n",
      "Epoch 1/20  Iteration 171/3520 Training loss: 2.7930 0.0863 sec/batch\n",
      "Epoch 1/20  Iteration 172/3520 Training loss: 2.7903 0.0880 sec/batch\n",
      "Epoch 1/20  Iteration 173/3520 Training loss: 2.7876 0.0793 sec/batch\n",
      "Epoch 1/20  Iteration 174/3520 Training loss: 2.7850 0.0855 sec/batch\n",
      "Epoch 1/20  Iteration 175/3520 Training loss: 2.7822 0.0849 sec/batch\n",
      "Epoch 1/20  Iteration 176/3520 Training loss: 2.7796 0.0838 sec/batch\n",
      "Epoch 2/20  Iteration 177/3520 Training loss: 2.3665 0.0871 sec/batch\n",
      "Epoch 2/20  Iteration 178/3520 Training loss: 2.3207 0.0846 sec/batch\n",
      "Epoch 2/20  Iteration 179/3520 Training loss: 2.3187 0.0878 sec/batch\n",
      "Epoch 2/20  Iteration 180/3520 Training loss: 2.3166 0.0878 sec/batch\n",
      "Epoch 2/20  Iteration 181/3520 Training loss: 2.3154 0.0874 sec/batch\n",
      "Epoch 2/20  Iteration 182/3520 Training loss: 2.3162 0.0891 sec/batch\n",
      "Epoch 2/20  Iteration 183/3520 Training loss: 2.3151 0.0898 sec/batch\n",
      "Epoch 2/20  Iteration 184/3520 Training loss: 2.3174 0.0877 sec/batch\n",
      "Epoch 2/20  Iteration 185/3520 Training loss: 2.3160 0.0841 sec/batch\n",
      "Epoch 2/20  Iteration 186/3520 Training loss: 2.3172 0.0889 sec/batch\n",
      "Epoch 2/20  Iteration 187/3520 Training loss: 2.3152 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 188/3520 Training loss: 2.3149 0.0878 sec/batch\n",
      "Epoch 2/20  Iteration 189/3520 Training loss: 2.3132 0.0887 sec/batch\n",
      "Epoch 2/20  Iteration 190/3520 Training loss: 2.3121 0.0870 sec/batch\n",
      "Epoch 2/20  Iteration 191/3520 Training loss: 2.3089 0.0872 sec/batch\n",
      "Epoch 2/20  Iteration 192/3520 Training loss: 2.3075 0.0863 sec/batch\n",
      "Epoch 2/20  Iteration 193/3520 Training loss: 2.3055 0.0833 sec/batch\n",
      "Epoch 2/20  Iteration 194/3520 Training loss: 2.3055 0.0860 sec/batch\n",
      "Epoch 2/20  Iteration 195/3520 Training loss: 2.3030 0.0923 sec/batch\n",
      "Epoch 2/20  Iteration 196/3520 Training loss: 2.3026 0.0855 sec/batch\n",
      "Epoch 2/20  Iteration 197/3520 Training loss: 2.3019 0.0852 sec/batch\n",
      "Epoch 2/20  Iteration 198/3520 Training loss: 2.3013 0.0849 sec/batch\n",
      "Epoch 2/20  Iteration 199/3520 Training loss: 2.3023 0.0878 sec/batch\n",
      "Epoch 2/20  Iteration 200/3520 Training loss: 2.3017 0.0853 sec/batch\n",
      "Validation loss: 2.19415 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 201/3520 Training loss: 2.3013 0.0874 sec/batch\n",
      "Epoch 2/20  Iteration 202/3520 Training loss: 2.2995 0.0879 sec/batch\n",
      "Epoch 2/20  Iteration 203/3520 Training loss: 2.2972 0.0884 sec/batch\n",
      "Epoch 2/20  Iteration 204/3520 Training loss: 2.2957 0.0887 sec/batch\n",
      "Epoch 2/20  Iteration 205/3520 Training loss: 2.2940 0.0871 sec/batch\n",
      "Epoch 2/20  Iteration 206/3520 Training loss: 2.2930 0.0891 sec/batch\n",
      "Epoch 2/20  Iteration 207/3520 Training loss: 2.2919 0.0895 sec/batch\n",
      "Epoch 2/20  Iteration 208/3520 Training loss: 2.2906 0.0847 sec/batch\n",
      "Epoch 2/20  Iteration 209/3520 Training loss: 2.2892 0.0893 sec/batch\n",
      "Epoch 2/20  Iteration 210/3520 Training loss: 2.2896 0.0874 sec/batch\n",
      "Epoch 2/20  Iteration 211/3520 Training loss: 2.2883 0.0861 sec/batch\n",
      "Epoch 2/20  Iteration 212/3520 Training loss: 2.2871 0.0871 sec/batch\n",
      "Epoch 2/20  Iteration 213/3520 Training loss: 2.2861 0.0871 sec/batch\n",
      "Epoch 2/20  Iteration 214/3520 Training loss: 2.2857 0.0875 sec/batch\n",
      "Epoch 2/20  Iteration 215/3520 Training loss: 2.2855 0.0862 sec/batch\n",
      "Epoch 2/20  Iteration 216/3520 Training loss: 2.2847 0.0883 sec/batch\n",
      "Epoch 2/20  Iteration 217/3520 Training loss: 2.2836 0.0874 sec/batch\n",
      "Epoch 2/20  Iteration 218/3520 Training loss: 2.2817 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 219/3520 Training loss: 2.2811 0.0879 sec/batch\n",
      "Epoch 2/20  Iteration 220/3520 Training loss: 2.2795 0.0856 sec/batch\n",
      "Epoch 2/20  Iteration 221/3520 Training loss: 2.2789 0.0859 sec/batch\n",
      "Epoch 2/20  Iteration 222/3520 Training loss: 2.2776 0.0879 sec/batch\n",
      "Epoch 2/20  Iteration 223/3520 Training loss: 2.2762 0.0865 sec/batch\n",
      "Epoch 2/20  Iteration 224/3520 Training loss: 2.2751 0.0879 sec/batch\n",
      "Epoch 2/20  Iteration 225/3520 Training loss: 2.2745 0.0864 sec/batch\n",
      "Epoch 2/20  Iteration 226/3520 Training loss: 2.2739 0.0853 sec/batch\n",
      "Epoch 2/20  Iteration 227/3520 Training loss: 2.2732 0.0852 sec/batch\n",
      "Epoch 2/20  Iteration 228/3520 Training loss: 2.2726 0.0854 sec/batch\n",
      "Epoch 2/20  Iteration 229/3520 Training loss: 2.2721 0.0919 sec/batch\n",
      "Epoch 2/20  Iteration 230/3520 Training loss: 2.2715 0.0859 sec/batch\n",
      "Epoch 2/20  Iteration 231/3520 Training loss: 2.2700 0.0873 sec/batch\n",
      "Epoch 2/20  Iteration 232/3520 Training loss: 2.2689 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 233/3520 Training loss: 2.2677 0.0890 sec/batch\n",
      "Epoch 2/20  Iteration 234/3520 Training loss: 2.2668 0.0890 sec/batch\n",
      "Epoch 2/20  Iteration 235/3520 Training loss: 2.2658 0.0881 sec/batch\n",
      "Epoch 2/20  Iteration 236/3520 Training loss: 2.2651 0.0860 sec/batch\n",
      "Epoch 2/20  Iteration 237/3520 Training loss: 2.2643 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 238/3520 Training loss: 2.2637 0.0865 sec/batch\n",
      "Epoch 2/20  Iteration 239/3520 Training loss: 2.2627 0.0861 sec/batch\n",
      "Epoch 2/20  Iteration 240/3520 Training loss: 2.2617 0.0861 sec/batch\n",
      "Epoch 2/20  Iteration 241/3520 Training loss: 2.2607 0.0882 sec/batch\n",
      "Epoch 2/20  Iteration 242/3520 Training loss: 2.2598 0.0873 sec/batch\n",
      "Epoch 2/20  Iteration 243/3520 Training loss: 2.2588 0.0917 sec/batch\n",
      "Epoch 2/20  Iteration 244/3520 Training loss: 2.2577 0.0864 sec/batch\n",
      "Epoch 2/20  Iteration 245/3520 Training loss: 2.2567 0.0879 sec/batch\n",
      "Epoch 2/20  Iteration 246/3520 Training loss: 2.2561 0.0845 sec/batch\n",
      "Epoch 2/20  Iteration 247/3520 Training loss: 2.2552 0.0856 sec/batch\n",
      "Epoch 2/20  Iteration 248/3520 Training loss: 2.2545 0.0848 sec/batch\n",
      "Epoch 2/20  Iteration 249/3520 Training loss: 2.2536 0.0853 sec/batch\n",
      "Epoch 2/20  Iteration 250/3520 Training loss: 2.2524 0.0926 sec/batch\n",
      "Epoch 2/20  Iteration 251/3520 Training loss: 2.2513 0.0862 sec/batch\n",
      "Epoch 2/20  Iteration 252/3520 Training loss: 2.2506 0.0879 sec/batch\n",
      "Epoch 2/20  Iteration 253/3520 Training loss: 2.2496 0.0868 sec/batch\n",
      "Epoch 2/20  Iteration 254/3520 Training loss: 2.2484 0.0889 sec/batch\n",
      "Epoch 2/20  Iteration 255/3520 Training loss: 2.2474 0.0897 sec/batch\n",
      "Epoch 2/20  Iteration 256/3520 Training loss: 2.2465 0.0859 sec/batch\n",
      "Epoch 2/20  Iteration 257/3520 Training loss: 2.2452 0.0881 sec/batch\n",
      "Epoch 2/20  Iteration 258/3520 Training loss: 2.2445 0.0867 sec/batch\n",
      "Epoch 2/20  Iteration 259/3520 Training loss: 2.2433 0.0877 sec/batch\n",
      "Epoch 2/20  Iteration 260/3520 Training loss: 2.2426 0.0884 sec/batch\n",
      "Epoch 2/20  Iteration 261/3520 Training loss: 2.2418 0.0844 sec/batch\n",
      "Epoch 2/20  Iteration 262/3520 Training loss: 2.2407 0.0917 sec/batch\n",
      "Epoch 2/20  Iteration 263/3520 Training loss: 2.2401 0.0873 sec/batch\n",
      "Epoch 2/20  Iteration 264/3520 Training loss: 2.2392 0.0849 sec/batch\n",
      "Epoch 2/20  Iteration 265/3520 Training loss: 2.2384 0.0850 sec/batch\n",
      "Epoch 2/20  Iteration 266/3520 Training loss: 2.2377 0.0840 sec/batch\n",
      "Epoch 2/20  Iteration 267/3520 Training loss: 2.2370 0.0888 sec/batch\n",
      "Epoch 2/20  Iteration 268/3520 Training loss: 2.2365 0.0870 sec/batch\n",
      "Epoch 2/20  Iteration 269/3520 Training loss: 2.2359 0.0882 sec/batch\n",
      "Epoch 2/20  Iteration 270/3520 Training loss: 2.2351 0.0876 sec/batch\n",
      "Epoch 2/20  Iteration 271/3520 Training loss: 2.2343 0.0878 sec/batch\n",
      "Epoch 2/20  Iteration 272/3520 Training loss: 2.2333 0.0889 sec/batch\n",
      "Epoch 2/20  Iteration 273/3520 Training loss: 2.2329 0.0920 sec/batch\n",
      "Epoch 2/20  Iteration 274/3520 Training loss: 2.2323 0.0850 sec/batch\n",
      "Epoch 2/20  Iteration 275/3520 Training loss: 2.2316 0.0872 sec/batch\n",
      "Epoch 2/20  Iteration 276/3520 Training loss: 2.2307 0.0858 sec/batch\n",
      "Epoch 2/20  Iteration 277/3520 Training loss: 2.2300 0.0961 sec/batch\n",
      "Epoch 2/20  Iteration 278/3520 Training loss: 2.2295 0.0856 sec/batch\n",
      "Epoch 2/20  Iteration 279/3520 Training loss: 2.2285 0.0845 sec/batch\n",
      "Epoch 2/20  Iteration 280/3520 Training loss: 2.2274 0.0852 sec/batch\n",
      "Epoch 2/20  Iteration 281/3520 Training loss: 2.2266 0.0840 sec/batch\n",
      "Epoch 2/20  Iteration 282/3520 Training loss: 2.2264 0.0876 sec/batch\n",
      "Epoch 2/20  Iteration 283/3520 Training loss: 2.2263 0.0866 sec/batch\n",
      "Epoch 2/20  Iteration 284/3520 Training loss: 2.2261 0.0853 sec/batch\n",
      "Epoch 2/20  Iteration 285/3520 Training loss: 2.2261 0.0859 sec/batch\n",
      "Epoch 2/20  Iteration 286/3520 Training loss: 2.2260 0.0846 sec/batch\n",
      "Epoch 2/20  Iteration 287/3520 Training loss: 2.2255 0.0885 sec/batch\n",
      "Epoch 2/20  Iteration 288/3520 Training loss: 2.2254 0.0862 sec/batch\n",
      "Epoch 2/20  Iteration 289/3520 Training loss: 2.2251 0.0880 sec/batch\n",
      "Epoch 2/20  Iteration 290/3520 Training loss: 2.2245 0.0865 sec/batch\n",
      "Epoch 2/20  Iteration 291/3520 Training loss: 2.2239 0.0863 sec/batch\n",
      "Epoch 2/20  Iteration 292/3520 Training loss: 2.2233 0.0847 sec/batch\n",
      "Epoch 2/20  Iteration 293/3520 Training loss: 2.2227 0.0842 sec/batch\n",
      "Epoch 2/20  Iteration 294/3520 Training loss: 2.2221 0.0862 sec/batch\n",
      "Epoch 2/20  Iteration 295/3520 Training loss: 2.2216 0.0864 sec/batch\n",
      "Epoch 2/20  Iteration 296/3520 Training loss: 2.2208 0.0851 sec/batch\n",
      "Epoch 2/20  Iteration 297/3520 Training loss: 2.2202 0.0892 sec/batch\n",
      "Epoch 2/20  Iteration 298/3520 Training loss: 2.2195 0.0861 sec/batch\n",
      "Epoch 2/20  Iteration 299/3520 Training loss: 2.2188 0.0843 sec/batch\n",
      "Epoch 2/20  Iteration 300/3520 Training loss: 2.2181 0.0854 sec/batch\n",
      "Epoch 2/20  Iteration 301/3520 Training loss: 2.2176 0.0867 sec/batch\n",
      "Epoch 2/20  Iteration 302/3520 Training loss: 2.2169 0.0848 sec/batch\n",
      "Epoch 2/20  Iteration 303/3520 Training loss: 2.2160 0.0899 sec/batch\n",
      "Epoch 2/20  Iteration 304/3520 Training loss: 2.2153 0.0839 sec/batch\n",
      "Epoch 2/20  Iteration 305/3520 Training loss: 2.2145 0.0862 sec/batch\n",
      "Epoch 2/20  Iteration 306/3520 Training loss: 2.2139 0.0849 sec/batch\n",
      "Epoch 2/20  Iteration 307/3520 Training loss: 2.2132 0.0864 sec/batch\n",
      "Epoch 2/20  Iteration 308/3520 Training loss: 2.2126 0.0887 sec/batch\n",
      "Epoch 2/20  Iteration 309/3520 Training loss: 2.2122 0.0860 sec/batch\n",
      "Epoch 2/20  Iteration 310/3520 Training loss: 2.2116 0.0855 sec/batch\n",
      "Epoch 2/20  Iteration 311/3520 Training loss: 2.2110 0.0862 sec/batch\n",
      "Epoch 2/20  Iteration 312/3520 Training loss: 2.2104 0.0874 sec/batch\n",
      "Epoch 2/20  Iteration 313/3520 Training loss: 2.2099 0.0873 sec/batch\n",
      "Epoch 2/20  Iteration 314/3520 Training loss: 2.2091 0.0863 sec/batch\n",
      "Epoch 2/20  Iteration 315/3520 Training loss: 2.2085 0.0855 sec/batch\n",
      "Epoch 2/20  Iteration 316/3520 Training loss: 2.2079 0.0871 sec/batch\n",
      "Epoch 2/20  Iteration 317/3520 Training loss: 2.2073 0.0879 sec/batch\n",
      "Epoch 2/20  Iteration 318/3520 Training loss: 2.2067 0.0852 sec/batch\n",
      "Epoch 2/20  Iteration 319/3520 Training loss: 2.2060 0.0858 sec/batch\n",
      "Epoch 2/20  Iteration 320/3520 Training loss: 2.2053 0.0849 sec/batch\n",
      "Epoch 2/20  Iteration 321/3520 Training loss: 2.2047 0.0879 sec/batch\n",
      "Epoch 2/20  Iteration 322/3520 Training loss: 2.2040 0.0889 sec/batch\n",
      "Epoch 2/20  Iteration 323/3520 Training loss: 2.2033 0.0859 sec/batch\n",
      "Epoch 2/20  Iteration 324/3520 Training loss: 2.2029 0.0856 sec/batch\n",
      "Epoch 2/20  Iteration 325/3520 Training loss: 2.2022 0.0871 sec/batch\n",
      "Epoch 2/20  Iteration 326/3520 Training loss: 2.2019 0.0879 sec/batch\n",
      "Epoch 2/20  Iteration 327/3520 Training loss: 2.2012 0.0873 sec/batch\n",
      "Epoch 2/20  Iteration 328/3520 Training loss: 2.2005 0.0841 sec/batch\n",
      "Epoch 2/20  Iteration 329/3520 Training loss: 2.1998 0.0848 sec/batch\n",
      "Epoch 2/20  Iteration 330/3520 Training loss: 2.1991 0.0859 sec/batch\n",
      "Epoch 2/20  Iteration 331/3520 Training loss: 2.1984 0.0858 sec/batch\n",
      "Epoch 2/20  Iteration 332/3520 Training loss: 2.1976 0.0865 sec/batch\n",
      "Epoch 2/20  Iteration 333/3520 Training loss: 2.1968 0.0878 sec/batch\n",
      "Epoch 2/20  Iteration 334/3520 Training loss: 2.1961 0.0853 sec/batch\n",
      "Epoch 2/20  Iteration 335/3520 Training loss: 2.1955 0.0866 sec/batch\n",
      "Epoch 2/20  Iteration 336/3520 Training loss: 2.1949 0.0870 sec/batch\n",
      "Epoch 2/20  Iteration 337/3520 Training loss: 2.1942 0.0881 sec/batch\n",
      "Epoch 2/20  Iteration 338/3520 Training loss: 2.1935 0.0845 sec/batch\n",
      "Epoch 2/20  Iteration 339/3520 Training loss: 2.1929 0.0841 sec/batch\n",
      "Epoch 2/20  Iteration 340/3520 Training loss: 2.1921 0.0855 sec/batch\n",
      "Epoch 2/20  Iteration 341/3520 Training loss: 2.1913 0.0858 sec/batch\n",
      "Epoch 2/20  Iteration 342/3520 Training loss: 2.1906 0.0874 sec/batch\n",
      "Epoch 2/20  Iteration 343/3520 Training loss: 2.1898 0.0850 sec/batch\n",
      "Epoch 2/20  Iteration 344/3520 Training loss: 2.1892 0.0829 sec/batch\n",
      "Epoch 2/20  Iteration 345/3520 Training loss: 2.1885 0.0847 sec/batch\n",
      "Epoch 2/20  Iteration 346/3520 Training loss: 2.1878 0.0864 sec/batch\n",
      "Epoch 2/20  Iteration 347/3520 Training loss: 2.1872 0.0845 sec/batch\n",
      "Epoch 2/20  Iteration 348/3520 Training loss: 2.1865 0.0864 sec/batch\n",
      "Epoch 2/20  Iteration 349/3520 Training loss: 2.1859 0.0902 sec/batch\n",
      "Epoch 2/20  Iteration 350/3520 Training loss: 2.1853 0.0888 sec/batch\n",
      "Epoch 2/20  Iteration 351/3520 Training loss: 2.1846 0.0899 sec/batch\n",
      "Epoch 2/20  Iteration 352/3520 Training loss: 2.1840 0.0871 sec/batch\n",
      "Epoch 3/20  Iteration 353/3520 Training loss: 2.0924 0.0854 sec/batch\n",
      "Epoch 3/20  Iteration 354/3520 Training loss: 2.0531 0.0892 sec/batch\n",
      "Epoch 3/20  Iteration 355/3520 Training loss: 2.0538 0.0882 sec/batch\n",
      "Epoch 3/20  Iteration 356/3520 Training loss: 2.0595 0.0846 sec/batch\n",
      "Epoch 3/20  Iteration 357/3520 Training loss: 2.0611 0.0877 sec/batch\n",
      "Epoch 3/20  Iteration 358/3520 Training loss: 2.0628 0.0851 sec/batch\n",
      "Epoch 3/20  Iteration 359/3520 Training loss: 2.0641 0.0843 sec/batch\n",
      "Epoch 3/20  Iteration 360/3520 Training loss: 2.0670 0.0851 sec/batch\n",
      "Epoch 3/20  Iteration 361/3520 Training loss: 2.0648 0.0854 sec/batch\n",
      "Epoch 3/20  Iteration 362/3520 Training loss: 2.0665 0.0835 sec/batch\n",
      "Epoch 3/20  Iteration 363/3520 Training loss: 2.0656 0.0901 sec/batch\n",
      "Epoch 3/20  Iteration 364/3520 Training loss: 2.0652 0.0889 sec/batch\n",
      "Epoch 3/20  Iteration 365/3520 Training loss: 2.0626 0.0845 sec/batch\n",
      "Epoch 3/20  Iteration 366/3520 Training loss: 2.0626 0.0883 sec/batch\n",
      "Epoch 3/20  Iteration 367/3520 Training loss: 2.0604 0.0856 sec/batch\n",
      "Epoch 3/20  Iteration 368/3520 Training loss: 2.0599 0.0864 sec/batch\n",
      "Epoch 3/20  Iteration 369/3520 Training loss: 2.0596 0.0845 sec/batch\n",
      "Epoch 3/20  Iteration 370/3520 Training loss: 2.0596 0.0883 sec/batch\n",
      "Epoch 3/20  Iteration 371/3520 Training loss: 2.0583 0.0862 sec/batch\n",
      "Epoch 3/20  Iteration 372/3520 Training loss: 2.0586 0.0881 sec/batch\n",
      "Epoch 3/20  Iteration 373/3520 Training loss: 2.0588 0.0870 sec/batch\n",
      "Epoch 3/20  Iteration 374/3520 Training loss: 2.0584 0.0992 sec/batch\n",
      "Epoch 3/20  Iteration 375/3520 Training loss: 2.0590 0.0835 sec/batch\n",
      "Epoch 3/20  Iteration 376/3520 Training loss: 2.0585 0.0886 sec/batch\n",
      "Epoch 3/20  Iteration 377/3520 Training loss: 2.0583 0.0820 sec/batch\n",
      "Epoch 3/20  Iteration 378/3520 Training loss: 2.0568 0.0879 sec/batch\n",
      "Epoch 3/20  Iteration 379/3520 Training loss: 2.0548 0.0860 sec/batch\n",
      "Epoch 3/20  Iteration 380/3520 Training loss: 2.0540 0.0847 sec/batch\n",
      "Epoch 3/20  Iteration 381/3520 Training loss: 2.0527 0.0880 sec/batch\n",
      "Epoch 3/20  Iteration 382/3520 Training loss: 2.0520 0.0860 sec/batch\n",
      "Epoch 3/20  Iteration 383/3520 Training loss: 2.0515 0.0851 sec/batch\n",
      "Epoch 3/20  Iteration 384/3520 Training loss: 2.0510 0.0856 sec/batch\n",
      "Epoch 3/20  Iteration 385/3520 Training loss: 2.0505 0.0877 sec/batch\n",
      "Epoch 3/20  Iteration 386/3520 Training loss: 2.0505 0.0907 sec/batch\n",
      "Epoch 3/20  Iteration 387/3520 Training loss: 2.0496 0.0858 sec/batch\n",
      "Epoch 3/20  Iteration 388/3520 Training loss: 2.0488 0.0856 sec/batch\n",
      "Epoch 3/20  Iteration 389/3520 Training loss: 2.0478 0.0857 sec/batch\n",
      "Epoch 3/20  Iteration 390/3520 Training loss: 2.0482 0.0848 sec/batch\n",
      "Epoch 3/20  Iteration 391/3520 Training loss: 2.0487 0.0900 sec/batch\n",
      "Epoch 3/20  Iteration 392/3520 Training loss: 2.0482 0.0863 sec/batch\n",
      "Epoch 3/20  Iteration 393/3520 Training loss: 2.0476 0.0873 sec/batch\n",
      "Epoch 3/20  Iteration 394/3520 Training loss: 2.0460 0.0874 sec/batch\n",
      "Epoch 3/20  Iteration 395/3520 Training loss: 2.0457 0.0860 sec/batch\n",
      "Epoch 3/20  Iteration 396/3520 Training loss: 2.0445 0.0871 sec/batch\n",
      "Epoch 3/20  Iteration 397/3520 Training loss: 2.0442 0.0870 sec/batch\n",
      "Epoch 3/20  Iteration 398/3520 Training loss: 2.0431 0.0874 sec/batch\n",
      "Epoch 3/20  Iteration 399/3520 Training loss: 2.0424 0.0880 sec/batch\n",
      "Epoch 3/20  Iteration 400/3520 Training loss: 2.0418 0.0867 sec/batch\n",
      "Validation loss: 1.91993 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 401/3520 Training loss: 2.0419 0.0876 sec/batch\n",
      "Epoch 3/20  Iteration 402/3520 Training loss: 2.0417 0.0867 sec/batch\n",
      "Epoch 3/20  Iteration 403/3520 Training loss: 2.0413 0.0863 sec/batch\n",
      "Epoch 3/20  Iteration 404/3520 Training loss: 2.0411 0.0930 sec/batch\n",
      "Epoch 3/20  Iteration 405/3520 Training loss: 2.0410 0.0879 sec/batch\n",
      "Epoch 3/20  Iteration 406/3520 Training loss: 2.0406 0.0963 sec/batch\n",
      "Epoch 3/20  Iteration 407/3520 Training loss: 2.0395 0.0887 sec/batch\n",
      "Epoch 3/20  Iteration 408/3520 Training loss: 2.0385 0.0865 sec/batch\n",
      "Epoch 3/20  Iteration 409/3520 Training loss: 2.0377 0.0847 sec/batch\n",
      "Epoch 3/20  Iteration 410/3520 Training loss: 2.0372 0.0846 sec/batch\n",
      "Epoch 3/20  Iteration 411/3520 Training loss: 2.0365 0.0870 sec/batch\n",
      "Epoch 3/20  Iteration 412/3520 Training loss: 2.0362 0.0855 sec/batch\n",
      "Epoch 3/20  Iteration 413/3520 Training loss: 2.0356 0.0998 sec/batch\n",
      "Epoch 3/20  Iteration 414/3520 Training loss: 2.0353 0.0899 sec/batch\n",
      "Epoch 3/20  Iteration 415/3520 Training loss: 2.0347 0.0857 sec/batch\n",
      "Epoch 3/20  Iteration 416/3520 Training loss: 2.0342 0.0892 sec/batch\n",
      "Epoch 3/20  Iteration 417/3520 Training loss: 2.0333 0.0866 sec/batch\n",
      "Epoch 3/20  Iteration 418/3520 Training loss: 2.0327 0.0922 sec/batch\n",
      "Epoch 3/20  Iteration 419/3520 Training loss: 2.0321 0.0901 sec/batch\n",
      "Epoch 3/20  Iteration 420/3520 Training loss: 2.0312 0.0834 sec/batch\n",
      "Epoch 3/20  Iteration 421/3520 Training loss: 2.0306 0.0877 sec/batch\n",
      "Epoch 3/20  Iteration 422/3520 Training loss: 2.0305 0.0872 sec/batch\n",
      "Epoch 3/20  Iteration 423/3520 Training loss: 2.0298 0.0874 sec/batch\n",
      "Epoch 3/20  Iteration 424/3520 Training loss: 2.0293 0.0848 sec/batch\n",
      "Epoch 3/20  Iteration 425/3520 Training loss: 2.0287 0.0873 sec/batch\n",
      "Epoch 3/20  Iteration 426/3520 Training loss: 2.0277 0.0860 sec/batch\n",
      "Epoch 3/20  Iteration 427/3520 Training loss: 2.0270 0.0852 sec/batch\n",
      "Epoch 3/20  Iteration 428/3520 Training loss: 2.0268 0.0835 sec/batch\n",
      "Epoch 3/20  Iteration 429/3520 Training loss: 2.0262 0.0886 sec/batch\n",
      "Epoch 3/20  Iteration 430/3520 Training loss: 2.0257 0.0886 sec/batch\n",
      "Epoch 3/20  Iteration 431/3520 Training loss: 2.0249 0.0877 sec/batch\n",
      "Epoch 3/20  Iteration 432/3520 Training loss: 2.0245 0.0868 sec/batch\n",
      "Epoch 3/20  Iteration 433/3520 Training loss: 2.0235 0.0861 sec/batch\n",
      "Epoch 3/20  Iteration 434/3520 Training loss: 2.0233 0.0869 sec/batch\n",
      "Epoch 3/20  Iteration 435/3520 Training loss: 2.0226 0.0883 sec/batch\n",
      "Epoch 3/20  Iteration 436/3520 Training loss: 2.0223 0.0862 sec/batch\n",
      "Epoch 3/20  Iteration 437/3520 Training loss: 2.0218 0.0876 sec/batch\n",
      "Epoch 3/20  Iteration 438/3520 Training loss: 2.0211 0.0845 sec/batch\n",
      "Epoch 3/20  Iteration 439/3520 Training loss: 2.0209 0.0856 sec/batch\n",
      "Epoch 3/20  Iteration 440/3520 Training loss: 2.0203 0.0844 sec/batch\n",
      "Epoch 3/20  Iteration 441/3520 Training loss: 2.0199 0.0885 sec/batch\n",
      "Epoch 3/20  Iteration 442/3520 Training loss: 2.0194 0.0857 sec/batch\n",
      "Epoch 3/20  Iteration 443/3520 Training loss: 2.0190 0.0849 sec/batch\n",
      "Epoch 3/20  Iteration 444/3520 Training loss: 2.0189 0.0856 sec/batch\n",
      "Epoch 3/20  Iteration 445/3520 Training loss: 2.0185 0.0876 sec/batch\n",
      "Epoch 3/20  Iteration 446/3520 Training loss: 2.0180 0.0883 sec/batch\n",
      "Epoch 3/20  Iteration 447/3520 Training loss: 2.0175 0.0878 sec/batch\n",
      "Epoch 3/20  Iteration 448/3520 Training loss: 2.0167 0.0873 sec/batch\n",
      "Epoch 3/20  Iteration 449/3520 Training loss: 2.0165 0.0874 sec/batch\n",
      "Epoch 3/20  Iteration 450/3520 Training loss: 2.0162 0.0852 sec/batch\n",
      "Epoch 3/20  Iteration 451/3520 Training loss: 2.0159 0.0855 sec/batch\n",
      "Epoch 3/20  Iteration 452/3520 Training loss: 2.0151 0.0902 sec/batch\n",
      "Epoch 3/20  Iteration 453/3520 Training loss: 2.0146 0.0874 sec/batch\n",
      "Epoch 3/20  Iteration 454/3520 Training loss: 2.0143 0.0852 sec/batch\n",
      "Epoch 3/20  Iteration 455/3520 Training loss: 2.0136 0.0889 sec/batch\n",
      "Epoch 3/20  Iteration 456/3520 Training loss: 2.0128 0.0849 sec/batch\n",
      "Epoch 3/20  Iteration 457/3520 Training loss: 2.0121 0.0860 sec/batch\n",
      "Epoch 3/20  Iteration 458/3520 Training loss: 2.0119 0.0923 sec/batch\n",
      "Epoch 3/20  Iteration 459/3520 Training loss: 2.0112 0.0874 sec/batch\n",
      "Epoch 3/20  Iteration 460/3520 Training loss: 2.0103 0.0875 sec/batch\n",
      "Epoch 3/20  Iteration 461/3520 Training loss: 2.0099 0.0878 sec/batch\n",
      "Epoch 3/20  Iteration 462/3520 Training loss: 2.0094 0.0869 sec/batch\n",
      "Epoch 3/20  Iteration 463/3520 Training loss: 2.0088 0.0839 sec/batch\n",
      "Epoch 3/20  Iteration 464/3520 Training loss: 2.0084 0.0852 sec/batch\n",
      "Epoch 3/20  Iteration 465/3520 Training loss: 2.0080 0.0856 sec/batch\n",
      "Epoch 3/20  Iteration 466/3520 Training loss: 2.0075 0.0897 sec/batch\n",
      "Epoch 3/20  Iteration 467/3520 Training loss: 2.0069 0.0851 sec/batch\n",
      "Epoch 3/20  Iteration 468/3520 Training loss: 2.0065 0.0857 sec/batch\n",
      "Epoch 3/20  Iteration 469/3520 Training loss: 2.0059 0.0870 sec/batch\n",
      "Epoch 3/20  Iteration 470/3520 Training loss: 2.0054 0.0933 sec/batch\n",
      "Epoch 3/20  Iteration 471/3520 Training loss: 2.0050 0.0862 sec/batch\n",
      "Epoch 3/20  Iteration 472/3520 Training loss: 2.0045 0.0863 sec/batch\n",
      "Epoch 3/20  Iteration 473/3520 Training loss: 2.0041 0.0937 sec/batch\n",
      "Epoch 3/20  Iteration 474/3520 Training loss: 2.0037 0.0883 sec/batch\n",
      "Epoch 3/20  Iteration 475/3520 Training loss: 2.0031 0.0868 sec/batch\n",
      "Epoch 3/20  Iteration 476/3520 Training loss: 2.0027 0.0870 sec/batch\n",
      "Epoch 3/20  Iteration 477/3520 Training loss: 2.0024 0.0851 sec/batch\n",
      "Epoch 3/20  Iteration 478/3520 Training loss: 2.0018 0.0864 sec/batch\n",
      "Epoch 3/20  Iteration 479/3520 Training loss: 2.0010 0.0872 sec/batch\n",
      "Epoch 3/20  Iteration 480/3520 Training loss: 2.0005 0.0923 sec/batch\n",
      "Epoch 3/20  Iteration 481/3520 Training loss: 1.9999 0.0867 sec/batch\n",
      "Epoch 3/20  Iteration 482/3520 Training loss: 1.9997 0.0880 sec/batch\n",
      "Epoch 3/20  Iteration 483/3520 Training loss: 1.9992 0.0851 sec/batch\n",
      "Epoch 3/20  Iteration 484/3520 Training loss: 1.9989 0.0879 sec/batch\n",
      "Epoch 3/20  Iteration 485/3520 Training loss: 1.9987 0.0926 sec/batch\n",
      "Epoch 3/20  Iteration 486/3520 Training loss: 1.9983 0.0879 sec/batch\n",
      "Epoch 3/20  Iteration 487/3520 Training loss: 1.9979 0.0877 sec/batch\n",
      "Epoch 3/20  Iteration 488/3520 Training loss: 1.9976 0.0845 sec/batch\n",
      "Epoch 3/20  Iteration 489/3520 Training loss: 1.9973 0.0922 sec/batch\n",
      "Epoch 3/20  Iteration 490/3520 Training loss: 1.9967 0.0832 sec/batch\n",
      "Epoch 3/20  Iteration 491/3520 Training loss: 1.9964 0.0850 sec/batch\n",
      "Epoch 3/20  Iteration 492/3520 Training loss: 1.9961 0.0868 sec/batch\n",
      "Epoch 3/20  Iteration 493/3520 Training loss: 1.9958 0.0865 sec/batch\n",
      "Epoch 3/20  Iteration 494/3520 Training loss: 1.9954 0.0932 sec/batch\n",
      "Epoch 3/20  Iteration 495/3520 Training loss: 1.9949 0.0870 sec/batch\n",
      "Epoch 3/20  Iteration 496/3520 Training loss: 1.9945 0.0864 sec/batch\n",
      "Epoch 3/20  Iteration 497/3520 Training loss: 1.9942 0.0873 sec/batch\n",
      "Epoch 3/20  Iteration 498/3520 Training loss: 1.9938 0.0868 sec/batch\n",
      "Epoch 3/20  Iteration 499/3520 Training loss: 1.9933 0.0931 sec/batch\n",
      "Epoch 3/20  Iteration 500/3520 Training loss: 1.9929 0.0861 sec/batch\n",
      "Epoch 3/20  Iteration 501/3520 Training loss: 1.9926 0.0862 sec/batch\n",
      "Epoch 3/20  Iteration 502/3520 Training loss: 1.9924 0.0864 sec/batch\n",
      "Epoch 3/20  Iteration 503/3520 Training loss: 1.9920 0.0890 sec/batch\n",
      "Epoch 3/20  Iteration 504/3520 Training loss: 1.9914 0.0864 sec/batch\n",
      "Epoch 3/20  Iteration 505/3520 Training loss: 1.9909 0.0867 sec/batch\n",
      "Epoch 3/20  Iteration 506/3520 Training loss: 1.9904 0.0887 sec/batch\n",
      "Epoch 3/20  Iteration 507/3520 Training loss: 1.9900 0.0929 sec/batch\n",
      "Epoch 3/20  Iteration 508/3520 Training loss: 1.9896 0.0868 sec/batch\n",
      "Epoch 3/20  Iteration 509/3520 Training loss: 1.9891 0.0853 sec/batch\n",
      "Epoch 3/20  Iteration 510/3520 Training loss: 1.9886 0.0865 sec/batch\n",
      "Epoch 3/20  Iteration 511/3520 Training loss: 1.9882 0.0882 sec/batch\n",
      "Epoch 3/20  Iteration 512/3520 Training loss: 1.9879 0.0848 sec/batch\n",
      "Epoch 3/20  Iteration 513/3520 Training loss: 1.9874 0.0921 sec/batch\n",
      "Epoch 3/20  Iteration 514/3520 Training loss: 1.9869 0.0850 sec/batch\n",
      "Epoch 3/20  Iteration 515/3520 Training loss: 1.9866 0.0846 sec/batch\n",
      "Epoch 3/20  Iteration 516/3520 Training loss: 1.9861 0.0850 sec/batch\n",
      "Epoch 3/20  Iteration 517/3520 Training loss: 1.9855 0.0848 sec/batch\n",
      "Epoch 3/20  Iteration 518/3520 Training loss: 1.9850 0.0816 sec/batch\n",
      "Epoch 3/20  Iteration 519/3520 Training loss: 1.9845 0.0872 sec/batch\n",
      "Epoch 3/20  Iteration 520/3520 Training loss: 1.9841 0.0859 sec/batch\n",
      "Epoch 3/20  Iteration 521/3520 Training loss: 1.9837 0.0865 sec/batch\n",
      "Epoch 3/20  Iteration 522/3520 Training loss: 1.9833 0.0858 sec/batch\n",
      "Epoch 3/20  Iteration 523/3520 Training loss: 1.9829 0.0919 sec/batch\n",
      "Epoch 3/20  Iteration 524/3520 Training loss: 1.9825 0.0885 sec/batch\n",
      "Epoch 3/20  Iteration 525/3520 Training loss: 1.9820 0.0890 sec/batch\n",
      "Epoch 3/20  Iteration 526/3520 Training loss: 1.9816 0.0850 sec/batch\n",
      "Epoch 3/20  Iteration 527/3520 Training loss: 1.9812 0.0935 sec/batch\n",
      "Epoch 3/20  Iteration 528/3520 Training loss: 1.9809 0.0874 sec/batch\n",
      "Epoch 4/20  Iteration 529/3520 Training loss: 1.9167 0.0878 sec/batch\n",
      "Epoch 4/20  Iteration 530/3520 Training loss: 1.8820 0.0877 sec/batch\n",
      "Epoch 4/20  Iteration 531/3520 Training loss: 1.8883 0.0851 sec/batch\n",
      "Epoch 4/20  Iteration 532/3520 Training loss: 1.8931 0.0931 sec/batch\n",
      "Epoch 4/20  Iteration 533/3520 Training loss: 1.8950 0.0858 sec/batch\n",
      "Epoch 4/20  Iteration 534/3520 Training loss: 1.8979 0.0845 sec/batch\n",
      "Epoch 4/20  Iteration 535/3520 Training loss: 1.8988 0.0923 sec/batch\n",
      "Epoch 4/20  Iteration 536/3520 Training loss: 1.9016 0.0897 sec/batch\n",
      "Epoch 4/20  Iteration 537/3520 Training loss: 1.8991 0.0853 sec/batch\n",
      "Epoch 4/20  Iteration 538/3520 Training loss: 1.8997 0.0844 sec/batch\n",
      "Epoch 4/20  Iteration 539/3520 Training loss: 1.8997 0.0876 sec/batch\n",
      "Epoch 4/20  Iteration 540/3520 Training loss: 1.9002 0.0878 sec/batch\n",
      "Epoch 4/20  Iteration 541/3520 Training loss: 1.8975 0.0867 sec/batch\n",
      "Epoch 4/20  Iteration 542/3520 Training loss: 1.8984 0.0868 sec/batch\n",
      "Epoch 4/20  Iteration 543/3520 Training loss: 1.8981 0.0873 sec/batch\n",
      "Epoch 4/20  Iteration 544/3520 Training loss: 1.8984 0.0856 sec/batch\n",
      "Epoch 4/20  Iteration 545/3520 Training loss: 1.8981 0.0850 sec/batch\n",
      "Epoch 4/20  Iteration 546/3520 Training loss: 1.8988 0.0858 sec/batch\n",
      "Epoch 4/20  Iteration 547/3520 Training loss: 1.8975 0.0902 sec/batch\n",
      "Epoch 4/20  Iteration 548/3520 Training loss: 1.8988 0.0880 sec/batch\n",
      "Epoch 4/20  Iteration 549/3520 Training loss: 1.8994 0.0899 sec/batch\n",
      "Epoch 4/20  Iteration 550/3520 Training loss: 1.8992 0.0932 sec/batch\n",
      "Epoch 4/20  Iteration 551/3520 Training loss: 1.8998 0.0862 sec/batch\n",
      "Epoch 4/20  Iteration 552/3520 Training loss: 1.8998 0.0867 sec/batch\n",
      "Epoch 4/20  Iteration 553/3520 Training loss: 1.8997 0.0872 sec/batch\n",
      "Epoch 4/20  Iteration 554/3520 Training loss: 1.8988 0.0848 sec/batch\n",
      "Epoch 4/20  Iteration 555/3520 Training loss: 1.8969 0.0929 sec/batch\n",
      "Epoch 4/20  Iteration 556/3520 Training loss: 1.8963 0.0860 sec/batch\n",
      "Epoch 4/20  Iteration 557/3520 Training loss: 1.8957 0.0854 sec/batch\n",
      "Epoch 4/20  Iteration 558/3520 Training loss: 1.8952 0.0860 sec/batch\n",
      "Epoch 4/20  Iteration 559/3520 Training loss: 1.8948 0.0852 sec/batch\n",
      "Epoch 4/20  Iteration 560/3520 Training loss: 1.8944 0.0858 sec/batch\n",
      "Epoch 4/20  Iteration 561/3520 Training loss: 1.8948 0.0919 sec/batch\n",
      "Epoch 4/20  Iteration 562/3520 Training loss: 1.8945 0.0857 sec/batch\n",
      "Epoch 4/20  Iteration 563/3520 Training loss: 1.8941 0.0860 sec/batch\n",
      "Epoch 4/20  Iteration 564/3520 Training loss: 1.8939 0.0850 sec/batch\n",
      "Epoch 4/20  Iteration 565/3520 Training loss: 1.8932 0.0882 sec/batch\n",
      "Epoch 4/20  Iteration 566/3520 Training loss: 1.8939 0.0916 sec/batch\n",
      "Epoch 4/20  Iteration 567/3520 Training loss: 1.8948 0.0934 sec/batch\n",
      "Epoch 4/20  Iteration 568/3520 Training loss: 1.8945 0.0881 sec/batch\n",
      "Epoch 4/20  Iteration 569/3520 Training loss: 1.8941 0.0850 sec/batch\n",
      "Epoch 4/20  Iteration 570/3520 Training loss: 1.8926 0.0875 sec/batch\n",
      "Epoch 4/20  Iteration 571/3520 Training loss: 1.8925 0.0874 sec/batch\n",
      "Epoch 4/20  Iteration 572/3520 Training loss: 1.8913 0.0862 sec/batch\n",
      "Epoch 4/20  Iteration 573/3520 Training loss: 1.8908 0.0854 sec/batch\n",
      "Epoch 4/20  Iteration 574/3520 Training loss: 1.8898 0.0881 sec/batch\n",
      "Epoch 4/20  Iteration 575/3520 Training loss: 1.8893 0.0875 sec/batch\n",
      "Epoch 4/20  Iteration 576/3520 Training loss: 1.8889 0.0861 sec/batch\n",
      "Epoch 4/20  Iteration 577/3520 Training loss: 1.8886 0.0898 sec/batch\n",
      "Epoch 4/20  Iteration 578/3520 Training loss: 1.8883 0.0868 sec/batch\n",
      "Epoch 4/20  Iteration 579/3520 Training loss: 1.8881 0.0854 sec/batch\n",
      "Epoch 4/20  Iteration 580/3520 Training loss: 1.8880 0.0872 sec/batch\n",
      "Epoch 4/20  Iteration 581/3520 Training loss: 1.8880 0.0873 sec/batch\n",
      "Epoch 4/20  Iteration 582/3520 Training loss: 1.8877 0.0853 sec/batch\n",
      "Epoch 4/20  Iteration 583/3520 Training loss: 1.8870 0.0886 sec/batch\n",
      "Epoch 4/20  Iteration 584/3520 Training loss: 1.8860 0.0868 sec/batch\n",
      "Epoch 4/20  Iteration 585/3520 Training loss: 1.8854 0.0848 sec/batch\n",
      "Epoch 4/20  Iteration 586/3520 Training loss: 1.8850 0.0931 sec/batch\n",
      "Epoch 4/20  Iteration 587/3520 Training loss: 1.8844 0.0847 sec/batch\n",
      "Epoch 4/20  Iteration 588/3520 Training loss: 1.8845 0.0911 sec/batch\n",
      "Epoch 4/20  Iteration 589/3520 Training loss: 1.8841 0.0852 sec/batch\n",
      "Epoch 4/20  Iteration 590/3520 Training loss: 1.8840 0.0854 sec/batch\n",
      "Epoch 4/20  Iteration 591/3520 Training loss: 1.8836 0.0945 sec/batch\n",
      "Epoch 4/20  Iteration 592/3520 Training loss: 1.8835 0.0877 sec/batch\n",
      "Epoch 4/20  Iteration 593/3520 Training loss: 1.8828 0.0859 sec/batch\n",
      "Epoch 4/20  Iteration 594/3520 Training loss: 1.8824 0.0920 sec/batch\n",
      "Epoch 4/20  Iteration 595/3520 Training loss: 1.8818 0.0890 sec/batch\n",
      "Epoch 4/20  Iteration 596/3520 Training loss: 1.8811 0.0893 sec/batch\n",
      "Epoch 4/20  Iteration 597/3520 Training loss: 1.8806 0.0847 sec/batch\n",
      "Epoch 4/20  Iteration 598/3520 Training loss: 1.8805 0.0869 sec/batch\n",
      "Epoch 4/20  Iteration 599/3520 Training loss: 1.8800 0.0847 sec/batch\n",
      "Epoch 4/20  Iteration 600/3520 Training loss: 1.8797 0.0925 sec/batch\n",
      "Validation loss: 1.75014 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 601/3520 Training loss: 1.8799 0.0864 sec/batch\n",
      "Epoch 4/20  Iteration 602/3520 Training loss: 1.8791 0.0844 sec/batch\n",
      "Epoch 4/20  Iteration 603/3520 Training loss: 1.8789 0.0918 sec/batch\n",
      "Epoch 4/20  Iteration 604/3520 Training loss: 1.8789 0.0874 sec/batch\n",
      "Epoch 4/20  Iteration 605/3520 Training loss: 1.8786 0.0929 sec/batch\n",
      "Epoch 4/20  Iteration 606/3520 Training loss: 1.8785 0.0841 sec/batch\n",
      "Epoch 4/20  Iteration 607/3520 Training loss: 1.8779 0.0844 sec/batch\n",
      "Epoch 4/20  Iteration 608/3520 Training loss: 1.8776 0.0852 sec/batch\n",
      "Epoch 4/20  Iteration 609/3520 Training loss: 1.8768 0.0844 sec/batch\n",
      "Epoch 4/20  Iteration 610/3520 Training loss: 1.8767 0.0897 sec/batch\n",
      "Epoch 4/20  Iteration 611/3520 Training loss: 1.8761 0.0873 sec/batch\n",
      "Epoch 4/20  Iteration 612/3520 Training loss: 1.8761 0.0973 sec/batch\n",
      "Epoch 4/20  Iteration 613/3520 Training loss: 1.8758 0.0922 sec/batch\n",
      "Epoch 4/20  Iteration 614/3520 Training loss: 1.8753 0.0888 sec/batch\n",
      "Epoch 4/20  Iteration 615/3520 Training loss: 1.8753 0.0854 sec/batch\n",
      "Epoch 4/20  Iteration 616/3520 Training loss: 1.8750 0.0885 sec/batch\n",
      "Epoch 4/20  Iteration 617/3520 Training loss: 1.8748 0.0941 sec/batch\n",
      "Epoch 4/20  Iteration 618/3520 Training loss: 1.8743 0.0879 sec/batch\n",
      "Epoch 4/20  Iteration 619/3520 Training loss: 1.8741 0.0849 sec/batch\n",
      "Epoch 4/20  Iteration 620/3520 Training loss: 1.8741 0.0860 sec/batch\n",
      "Epoch 4/20  Iteration 621/3520 Training loss: 1.8738 0.0884 sec/batch\n",
      "Epoch 4/20  Iteration 622/3520 Training loss: 1.8734 0.0875 sec/batch\n",
      "Epoch 4/20  Iteration 623/3520 Training loss: 1.8730 0.0859 sec/batch\n",
      "Epoch 4/20  Iteration 624/3520 Training loss: 1.8724 0.0840 sec/batch\n",
      "Epoch 4/20  Iteration 625/3520 Training loss: 1.8724 0.0848 sec/batch\n",
      "Epoch 4/20  Iteration 626/3520 Training loss: 1.8721 0.0877 sec/batch\n",
      "Epoch 4/20  Iteration 627/3520 Training loss: 1.8721 0.0881 sec/batch\n",
      "Epoch 4/20  Iteration 628/3520 Training loss: 1.8716 0.0943 sec/batch\n",
      "Epoch 4/20  Iteration 629/3520 Training loss: 1.8713 0.0868 sec/batch\n",
      "Epoch 4/20  Iteration 630/3520 Training loss: 1.8711 0.0837 sec/batch\n",
      "Epoch 4/20  Iteration 631/3520 Training loss: 1.8706 0.0861 sec/batch\n",
      "Epoch 4/20  Iteration 632/3520 Training loss: 1.8699 0.0883 sec/batch\n",
      "Epoch 4/20  Iteration 633/3520 Training loss: 1.8693 0.0872 sec/batch\n",
      "Epoch 4/20  Iteration 634/3520 Training loss: 1.8693 0.0878 sec/batch\n",
      "Epoch 4/20  Iteration 635/3520 Training loss: 1.8687 0.0868 sec/batch\n",
      "Epoch 4/20  Iteration 636/3520 Training loss: 1.8680 0.0870 sec/batch\n",
      "Epoch 4/20  Iteration 637/3520 Training loss: 1.8678 0.0838 sec/batch\n",
      "Epoch 4/20  Iteration 638/3520 Training loss: 1.8674 0.0869 sec/batch\n",
      "Epoch 4/20  Iteration 639/3520 Training loss: 1.8670 0.0847 sec/batch\n",
      "Epoch 4/20  Iteration 640/3520 Training loss: 1.8668 0.0823 sec/batch\n",
      "Epoch 4/20  Iteration 641/3520 Training loss: 1.8666 0.0855 sec/batch\n",
      "Epoch 4/20  Iteration 642/3520 Training loss: 1.8662 0.0857 sec/batch\n",
      "Epoch 4/20  Iteration 643/3520 Training loss: 1.8658 0.0915 sec/batch\n",
      "Epoch 4/20  Iteration 644/3520 Training loss: 1.8655 0.0877 sec/batch\n",
      "Epoch 4/20  Iteration 645/3520 Training loss: 1.8650 0.0867 sec/batch\n",
      "Epoch 4/20  Iteration 646/3520 Training loss: 1.8648 0.0869 sec/batch\n",
      "Epoch 4/20  Iteration 647/3520 Training loss: 1.8645 0.0829 sec/batch\n",
      "Epoch 4/20  Iteration 648/3520 Training loss: 1.8642 0.0879 sec/batch\n",
      "Epoch 4/20  Iteration 649/3520 Training loss: 1.8640 0.0858 sec/batch\n",
      "Epoch 4/20  Iteration 650/3520 Training loss: 1.8637 0.0874 sec/batch\n",
      "Epoch 4/20  Iteration 651/3520 Training loss: 1.8634 0.0874 sec/batch\n",
      "Epoch 4/20  Iteration 652/3520 Training loss: 1.8630 0.0849 sec/batch\n",
      "Epoch 4/20  Iteration 653/3520 Training loss: 1.8628 0.0877 sec/batch\n",
      "Epoch 4/20  Iteration 654/3520 Training loss: 1.8622 0.0865 sec/batch\n",
      "Epoch 4/20  Iteration 655/3520 Training loss: 1.8616 0.0890 sec/batch\n",
      "Epoch 4/20  Iteration 656/3520 Training loss: 1.8613 0.0855 sec/batch\n",
      "Epoch 4/20  Iteration 657/3520 Training loss: 1.8608 0.0923 sec/batch\n",
      "Epoch 4/20  Iteration 658/3520 Training loss: 1.8607 0.0861 sec/batch\n",
      "Epoch 4/20  Iteration 659/3520 Training loss: 1.8603 0.0859 sec/batch\n",
      "Epoch 4/20  Iteration 660/3520 Training loss: 1.8602 0.0874 sec/batch\n",
      "Epoch 4/20  Iteration 661/3520 Training loss: 1.8600 0.0868 sec/batch\n",
      "Epoch 4/20  Iteration 662/3520 Training loss: 1.8597 0.0936 sec/batch\n",
      "Epoch 4/20  Iteration 663/3520 Training loss: 1.8594 0.0837 sec/batch\n",
      "Epoch 4/20  Iteration 664/3520 Training loss: 1.8591 0.0878 sec/batch\n",
      "Epoch 4/20  Iteration 665/3520 Training loss: 1.8589 0.0837 sec/batch\n",
      "Epoch 4/20  Iteration 666/3520 Training loss: 1.8585 0.0850 sec/batch\n",
      "Epoch 4/20  Iteration 667/3520 Training loss: 1.8584 0.0880 sec/batch\n",
      "Epoch 4/20  Iteration 668/3520 Training loss: 1.8582 0.0857 sec/batch\n",
      "Epoch 4/20  Iteration 669/3520 Training loss: 1.8580 0.0864 sec/batch\n",
      "Epoch 4/20  Iteration 670/3520 Training loss: 1.8577 0.0847 sec/batch\n",
      "Epoch 4/20  Iteration 671/3520 Training loss: 1.8572 0.0867 sec/batch\n",
      "Epoch 4/20  Iteration 672/3520 Training loss: 1.8569 0.0931 sec/batch\n",
      "Epoch 4/20  Iteration 673/3520 Training loss: 1.8566 0.0930 sec/batch\n",
      "Epoch 4/20  Iteration 674/3520 Training loss: 1.8563 0.0846 sec/batch\n",
      "Epoch 4/20  Iteration 675/3520 Training loss: 1.8559 0.0857 sec/batch\n",
      "Epoch 4/20  Iteration 676/3520 Training loss: 1.8557 0.0871 sec/batch\n",
      "Epoch 4/20  Iteration 677/3520 Training loss: 1.8554 0.0852 sec/batch\n",
      "Epoch 4/20  Iteration 678/3520 Training loss: 1.8553 0.0873 sec/batch\n",
      "Epoch 4/20  Iteration 679/3520 Training loss: 1.8550 0.0929 sec/batch\n",
      "Epoch 4/20  Iteration 680/3520 Training loss: 1.8546 0.0888 sec/batch\n",
      "Epoch 4/20  Iteration 681/3520 Training loss: 1.8543 0.0886 sec/batch\n",
      "Epoch 4/20  Iteration 682/3520 Training loss: 1.8539 0.0857 sec/batch\n",
      "Epoch 4/20  Iteration 683/3520 Training loss: 1.8535 0.0940 sec/batch\n",
      "Epoch 4/20  Iteration 684/3520 Training loss: 1.8532 0.0913 sec/batch\n",
      "Epoch 4/20  Iteration 685/3520 Training loss: 1.8528 0.0862 sec/batch\n",
      "Epoch 4/20  Iteration 686/3520 Training loss: 1.8525 0.0874 sec/batch\n",
      "Epoch 4/20  Iteration 687/3520 Training loss: 1.8523 0.0886 sec/batch\n",
      "Epoch 4/20  Iteration 688/3520 Training loss: 1.8521 0.0916 sec/batch\n",
      "Epoch 4/20  Iteration 689/3520 Training loss: 1.8517 0.0849 sec/batch\n",
      "Epoch 4/20  Iteration 690/3520 Training loss: 1.8514 0.0861 sec/batch\n",
      "Epoch 4/20  Iteration 691/3520 Training loss: 1.8513 0.0855 sec/batch\n",
      "Epoch 4/20  Iteration 692/3520 Training loss: 1.8508 0.0928 sec/batch\n",
      "Epoch 4/20  Iteration 693/3520 Training loss: 1.8503 0.0902 sec/batch\n",
      "Epoch 4/20  Iteration 694/3520 Training loss: 1.8500 0.0839 sec/batch\n",
      "Epoch 4/20  Iteration 695/3520 Training loss: 1.8495 0.0878 sec/batch\n",
      "Epoch 4/20  Iteration 696/3520 Training loss: 1.8493 0.0858 sec/batch\n",
      "Epoch 4/20  Iteration 697/3520 Training loss: 1.8491 0.0846 sec/batch\n",
      "Epoch 4/20  Iteration 698/3520 Training loss: 1.8487 0.0854 sec/batch\n",
      "Epoch 4/20  Iteration 699/3520 Training loss: 1.8485 0.0844 sec/batch\n",
      "Epoch 4/20  Iteration 700/3520 Training loss: 1.8482 0.0879 sec/batch\n",
      "Epoch 4/20  Iteration 701/3520 Training loss: 1.8479 0.0862 sec/batch\n",
      "Epoch 4/20  Iteration 702/3520 Training loss: 1.8476 0.0885 sec/batch\n",
      "Epoch 4/20  Iteration 703/3520 Training loss: 1.8473 0.0881 sec/batch\n",
      "Epoch 4/20  Iteration 704/3520 Training loss: 1.8472 0.0875 sec/batch\n",
      "Epoch 5/20  Iteration 705/3520 Training loss: 1.8123 0.0870 sec/batch\n",
      "Epoch 5/20  Iteration 706/3520 Training loss: 1.7747 0.0852 sec/batch\n",
      "Epoch 5/20  Iteration 707/3520 Training loss: 1.7796 0.0864 sec/batch\n",
      "Epoch 5/20  Iteration 708/3520 Training loss: 1.7846 0.0872 sec/batch\n",
      "Epoch 5/20  Iteration 709/3520 Training loss: 1.7868 0.0874 sec/batch\n",
      "Epoch 5/20  Iteration 710/3520 Training loss: 1.7891 0.0843 sec/batch\n",
      "Epoch 5/20  Iteration 711/3520 Training loss: 1.7889 0.0877 sec/batch\n",
      "Epoch 5/20  Iteration 712/3520 Training loss: 1.7928 0.0865 sec/batch\n",
      "Epoch 5/20  Iteration 713/3520 Training loss: 1.7918 0.0896 sec/batch\n",
      "Epoch 5/20  Iteration 714/3520 Training loss: 1.7929 0.0857 sec/batch\n",
      "Epoch 5/20  Iteration 715/3520 Training loss: 1.7930 0.0903 sec/batch\n",
      "Epoch 5/20  Iteration 716/3520 Training loss: 1.7923 0.0937 sec/batch\n",
      "Epoch 5/20  Iteration 717/3520 Training loss: 1.7900 0.0870 sec/batch\n",
      "Epoch 5/20  Iteration 718/3520 Training loss: 1.7914 0.0876 sec/batch\n",
      "Epoch 5/20  Iteration 719/3520 Training loss: 1.7907 0.0858 sec/batch\n",
      "Epoch 5/20  Iteration 720/3520 Training loss: 1.7902 0.0845 sec/batch\n",
      "Epoch 5/20  Iteration 721/3520 Training loss: 1.7905 0.0870 sec/batch\n",
      "Epoch 5/20  Iteration 722/3520 Training loss: 1.7908 0.0845 sec/batch\n",
      "Epoch 5/20  Iteration 723/3520 Training loss: 1.7896 0.0886 sec/batch\n",
      "Epoch 5/20  Iteration 724/3520 Training loss: 1.7907 0.0851 sec/batch\n",
      "Epoch 5/20  Iteration 725/3520 Training loss: 1.7914 0.0866 sec/batch\n",
      "Epoch 5/20  Iteration 726/3520 Training loss: 1.7910 0.0878 sec/batch\n",
      "Epoch 5/20  Iteration 727/3520 Training loss: 1.7916 0.0865 sec/batch\n",
      "Epoch 5/20  Iteration 728/3520 Training loss: 1.7918 0.0875 sec/batch\n",
      "Epoch 5/20  Iteration 729/3520 Training loss: 1.7922 0.0881 sec/batch\n",
      "Epoch 5/20  Iteration 730/3520 Training loss: 1.7914 0.0867 sec/batch\n",
      "Epoch 5/20  Iteration 731/3520 Training loss: 1.7894 0.0870 sec/batch\n",
      "Epoch 5/20  Iteration 732/3520 Training loss: 1.7889 0.0891 sec/batch\n",
      "Epoch 5/20  Iteration 733/3520 Training loss: 1.7879 0.0827 sec/batch\n",
      "Epoch 5/20  Iteration 734/3520 Training loss: 1.7877 0.0886 sec/batch\n",
      "Epoch 5/20  Iteration 735/3520 Training loss: 1.7877 0.0859 sec/batch\n",
      "Epoch 5/20  Iteration 736/3520 Training loss: 1.7873 0.0855 sec/batch\n",
      "Epoch 5/20  Iteration 737/3520 Training loss: 1.7878 0.0866 sec/batch\n",
      "Epoch 5/20  Iteration 738/3520 Training loss: 1.7874 0.0932 sec/batch\n",
      "Epoch 5/20  Iteration 739/3520 Training loss: 1.7873 0.0852 sec/batch\n",
      "Epoch 5/20  Iteration 740/3520 Training loss: 1.7874 0.0857 sec/batch\n",
      "Epoch 5/20  Iteration 741/3520 Training loss: 1.7866 0.0879 sec/batch\n",
      "Epoch 5/20  Iteration 742/3520 Training loss: 1.7875 0.0844 sec/batch\n",
      "Epoch 5/20  Iteration 743/3520 Training loss: 1.7886 0.0861 sec/batch\n",
      "Epoch 5/20  Iteration 744/3520 Training loss: 1.7884 0.0865 sec/batch\n",
      "Epoch 5/20  Iteration 745/3520 Training loss: 1.7880 0.0883 sec/batch\n",
      "Epoch 5/20  Iteration 746/3520 Training loss: 1.7866 0.0882 sec/batch\n",
      "Epoch 5/20  Iteration 747/3520 Training loss: 1.7868 0.0874 sec/batch\n",
      "Epoch 5/20  Iteration 748/3520 Training loss: 1.7859 0.0835 sec/batch\n",
      "Epoch 5/20  Iteration 749/3520 Training loss: 1.7858 0.0879 sec/batch\n",
      "Epoch 5/20  Iteration 750/3520 Training loss: 1.7848 0.0857 sec/batch\n",
      "Epoch 5/20  Iteration 751/3520 Training loss: 1.7845 0.0852 sec/batch\n",
      "Epoch 5/20  Iteration 752/3520 Training loss: 1.7841 0.0870 sec/batch\n",
      "Epoch 5/20  Iteration 753/3520 Training loss: 1.7839 0.0883 sec/batch\n",
      "Epoch 5/20  Iteration 754/3520 Training loss: 1.7837 0.0858 sec/batch\n",
      "Epoch 5/20  Iteration 755/3520 Training loss: 1.7836 0.0828 sec/batch\n",
      "Epoch 5/20  Iteration 756/3520 Training loss: 1.7836 0.0861 sec/batch\n",
      "Epoch 5/20  Iteration 757/3520 Training loss: 1.7838 0.0878 sec/batch\n",
      "Epoch 5/20  Iteration 758/3520 Training loss: 1.7836 0.0869 sec/batch\n",
      "Epoch 5/20  Iteration 759/3520 Training loss: 1.7831 0.0855 sec/batch\n",
      "Epoch 5/20  Iteration 760/3520 Training loss: 1.7824 0.0863 sec/batch\n",
      "Epoch 5/20  Iteration 761/3520 Training loss: 1.7820 0.0873 sec/batch\n",
      "Epoch 5/20  Iteration 762/3520 Training loss: 1.7817 0.0920 sec/batch\n",
      "Epoch 5/20  Iteration 763/3520 Training loss: 1.7811 0.0861 sec/batch\n",
      "Epoch 5/20  Iteration 764/3520 Training loss: 1.7815 0.0850 sec/batch\n",
      "Epoch 5/20  Iteration 765/3520 Training loss: 1.7811 0.0916 sec/batch\n",
      "Epoch 5/20  Iteration 766/3520 Training loss: 1.7810 0.0896 sec/batch\n",
      "Epoch 5/20  Iteration 767/3520 Training loss: 1.7808 0.0854 sec/batch\n",
      "Epoch 5/20  Iteration 768/3520 Training loss: 1.7808 0.0882 sec/batch\n",
      "Epoch 5/20  Iteration 769/3520 Training loss: 1.7801 0.0844 sec/batch\n",
      "Epoch 5/20  Iteration 770/3520 Training loss: 1.7798 0.0871 sec/batch\n",
      "Epoch 5/20  Iteration 771/3520 Training loss: 1.7794 0.0846 sec/batch\n",
      "Epoch 5/20  Iteration 772/3520 Training loss: 1.7789 0.0904 sec/batch\n",
      "Epoch 5/20  Iteration 773/3520 Training loss: 1.7786 0.0852 sec/batch\n",
      "Epoch 5/20  Iteration 774/3520 Training loss: 1.7786 0.0861 sec/batch\n",
      "Epoch 5/20  Iteration 775/3520 Training loss: 1.7782 0.0861 sec/batch\n",
      "Epoch 5/20  Iteration 776/3520 Training loss: 1.7779 0.0943 sec/batch\n",
      "Epoch 5/20  Iteration 777/3520 Training loss: 1.7775 0.0951 sec/batch\n",
      "Epoch 5/20  Iteration 778/3520 Training loss: 1.7767 0.0936 sec/batch\n",
      "Epoch 5/20  Iteration 779/3520 Training loss: 1.7764 0.0874 sec/batch\n",
      "Epoch 5/20  Iteration 780/3520 Training loss: 1.7764 0.0950 sec/batch\n",
      "Epoch 5/20  Iteration 781/3520 Training loss: 1.7763 0.0843 sec/batch\n",
      "Epoch 5/20  Iteration 782/3520 Training loss: 1.7763 0.0870 sec/batch\n",
      "Epoch 5/20  Iteration 783/3520 Training loss: 1.7758 0.0938 sec/batch\n",
      "Epoch 5/20  Iteration 784/3520 Training loss: 1.7757 0.0871 sec/batch\n",
      "Epoch 5/20  Iteration 785/3520 Training loss: 1.7749 0.0937 sec/batch\n",
      "Epoch 5/20  Iteration 786/3520 Training loss: 1.7751 0.0853 sec/batch\n",
      "Epoch 5/20  Iteration 787/3520 Training loss: 1.7746 0.0850 sec/batch\n",
      "Epoch 5/20  Iteration 788/3520 Training loss: 1.7747 0.0863 sec/batch\n",
      "Epoch 5/20  Iteration 789/3520 Training loss: 1.7744 0.0854 sec/batch\n",
      "Epoch 5/20  Iteration 790/3520 Training loss: 1.7739 0.0860 sec/batch\n",
      "Epoch 5/20  Iteration 791/3520 Training loss: 1.7741 0.0863 sec/batch\n",
      "Epoch 5/20  Iteration 792/3520 Training loss: 1.7738 0.0880 sec/batch\n",
      "Epoch 5/20  Iteration 793/3520 Training loss: 1.7737 0.0844 sec/batch\n",
      "Epoch 5/20  Iteration 794/3520 Training loss: 1.7734 0.0849 sec/batch\n",
      "Epoch 5/20  Iteration 795/3520 Training loss: 1.7733 0.0846 sec/batch\n",
      "Epoch 5/20  Iteration 796/3520 Training loss: 1.7734 0.0861 sec/batch\n",
      "Epoch 5/20  Iteration 797/3520 Training loss: 1.7731 0.0870 sec/batch\n",
      "Epoch 5/20  Iteration 798/3520 Training loss: 1.7727 0.0881 sec/batch\n",
      "Epoch 5/20  Iteration 799/3520 Training loss: 1.7724 0.0855 sec/batch\n",
      "Epoch 5/20  Iteration 800/3520 Training loss: 1.7718 0.0943 sec/batch\n",
      "Validation loss: 1.64302 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 801/3520 Training loss: 1.7725 0.0875 sec/batch\n",
      "Epoch 5/20  Iteration 802/3520 Training loss: 1.7724 0.0865 sec/batch\n",
      "Epoch 5/20  Iteration 803/3520 Training loss: 1.7725 0.0855 sec/batch\n",
      "Epoch 5/20  Iteration 804/3520 Training loss: 1.7720 0.0843 sec/batch\n",
      "Epoch 5/20  Iteration 805/3520 Training loss: 1.7718 0.0857 sec/batch\n",
      "Epoch 5/20  Iteration 806/3520 Training loss: 1.7718 0.0855 sec/batch\n",
      "Epoch 5/20  Iteration 807/3520 Training loss: 1.7713 0.0847 sec/batch\n",
      "Epoch 5/20  Iteration 808/3520 Training loss: 1.7707 0.0843 sec/batch\n",
      "Epoch 5/20  Iteration 809/3520 Training loss: 1.7703 0.0888 sec/batch\n",
      "Epoch 5/20  Iteration 810/3520 Training loss: 1.7704 0.0876 sec/batch\n",
      "Epoch 5/20  Iteration 811/3520 Training loss: 1.7701 0.0843 sec/batch\n",
      "Epoch 5/20  Iteration 812/3520 Training loss: 1.7695 0.0849 sec/batch\n",
      "Epoch 5/20  Iteration 813/3520 Training loss: 1.7692 0.0932 sec/batch\n",
      "Epoch 5/20  Iteration 814/3520 Training loss: 1.7689 0.0848 sec/batch\n",
      "Epoch 5/20  Iteration 815/3520 Training loss: 1.7686 0.0850 sec/batch\n",
      "Epoch 5/20  Iteration 816/3520 Training loss: 1.7684 0.0876 sec/batch\n",
      "Epoch 5/20  Iteration 817/3520 Training loss: 1.7684 0.0940 sec/batch\n",
      "Epoch 5/20  Iteration 818/3520 Training loss: 1.7681 0.0884 sec/batch\n",
      "Epoch 5/20  Iteration 819/3520 Training loss: 1.7678 0.0842 sec/batch\n",
      "Epoch 5/20  Iteration 820/3520 Training loss: 1.7676 0.0849 sec/batch\n",
      "Epoch 5/20  Iteration 821/3520 Training loss: 1.7672 0.0926 sec/batch\n",
      "Epoch 5/20  Iteration 822/3520 Training loss: 1.7672 0.0862 sec/batch\n",
      "Epoch 5/20  Iteration 823/3520 Training loss: 1.7669 0.0882 sec/batch\n",
      "Epoch 5/20  Iteration 824/3520 Training loss: 1.7666 0.0874 sec/batch\n",
      "Epoch 5/20  Iteration 825/3520 Training loss: 1.7664 0.0931 sec/batch\n",
      "Epoch 5/20  Iteration 826/3520 Training loss: 1.7662 0.0881 sec/batch\n",
      "Epoch 5/20  Iteration 827/3520 Training loss: 1.7659 0.0849 sec/batch\n",
      "Epoch 5/20  Iteration 828/3520 Training loss: 1.7657 0.0855 sec/batch\n",
      "Epoch 5/20  Iteration 829/3520 Training loss: 1.7655 0.0861 sec/batch\n",
      "Epoch 5/20  Iteration 830/3520 Training loss: 1.7650 0.0860 sec/batch\n",
      "Epoch 5/20  Iteration 831/3520 Training loss: 1.7645 0.0874 sec/batch\n",
      "Epoch 5/20  Iteration 832/3520 Training loss: 1.7643 0.0869 sec/batch\n",
      "Epoch 5/20  Iteration 833/3520 Training loss: 1.7641 0.0850 sec/batch\n",
      "Epoch 5/20  Iteration 834/3520 Training loss: 1.7641 0.0869 sec/batch\n",
      "Epoch 5/20  Iteration 835/3520 Training loss: 1.7638 0.0854 sec/batch\n",
      "Epoch 5/20  Iteration 836/3520 Training loss: 1.7639 0.0865 sec/batch\n",
      "Epoch 5/20  Iteration 837/3520 Training loss: 1.7638 0.0872 sec/batch\n",
      "Epoch 5/20  Iteration 838/3520 Training loss: 1.7636 0.0872 sec/batch\n",
      "Epoch 5/20  Iteration 839/3520 Training loss: 1.7635 0.0877 sec/batch\n",
      "Epoch 5/20  Iteration 840/3520 Training loss: 1.7633 0.0875 sec/batch\n",
      "Epoch 5/20  Iteration 841/3520 Training loss: 1.7630 0.0875 sec/batch\n",
      "Epoch 5/20  Iteration 842/3520 Training loss: 1.7628 0.0877 sec/batch\n",
      "Epoch 5/20  Iteration 843/3520 Training loss: 1.7627 0.0849 sec/batch\n",
      "Epoch 5/20  Iteration 844/3520 Training loss: 1.7626 0.0895 sec/batch\n",
      "Epoch 5/20  Iteration 845/3520 Training loss: 1.7625 0.0944 sec/batch\n",
      "Epoch 5/20  Iteration 846/3520 Training loss: 1.7622 0.0875 sec/batch\n",
      "Epoch 5/20  Iteration 847/3520 Training loss: 1.7618 0.0872 sec/batch\n",
      "Epoch 5/20  Iteration 848/3520 Training loss: 1.7615 0.0856 sec/batch\n",
      "Epoch 5/20  Iteration 849/3520 Training loss: 1.7613 0.0866 sec/batch\n",
      "Epoch 5/20  Iteration 850/3520 Training loss: 1.7612 0.0877 sec/batch\n",
      "Epoch 5/20  Iteration 851/3520 Training loss: 1.7609 0.0866 sec/batch\n",
      "Epoch 5/20  Iteration 852/3520 Training loss: 1.7608 0.0880 sec/batch\n",
      "Epoch 5/20  Iteration 853/3520 Training loss: 1.7606 0.0861 sec/batch\n",
      "Epoch 5/20  Iteration 854/3520 Training loss: 1.7606 0.0885 sec/batch\n",
      "Epoch 5/20  Iteration 855/3520 Training loss: 1.7603 0.0937 sec/batch\n",
      "Epoch 5/20  Iteration 856/3520 Training loss: 1.7600 0.0876 sec/batch\n",
      "Epoch 5/20  Iteration 857/3520 Training loss: 1.7598 0.0871 sec/batch\n",
      "Epoch 5/20  Iteration 858/3520 Training loss: 1.7594 0.0876 sec/batch\n",
      "Epoch 5/20  Iteration 859/3520 Training loss: 1.7592 0.0856 sec/batch\n",
      "Epoch 5/20  Iteration 860/3520 Training loss: 1.7590 0.0846 sec/batch\n",
      "Epoch 5/20  Iteration 861/3520 Training loss: 1.7587 0.0853 sec/batch\n",
      "Epoch 5/20  Iteration 862/3520 Training loss: 1.7585 0.0845 sec/batch\n",
      "Epoch 5/20  Iteration 863/3520 Training loss: 1.7583 0.0848 sec/batch\n",
      "Epoch 5/20  Iteration 864/3520 Training loss: 1.7583 0.0855 sec/batch\n",
      "Epoch 5/20  Iteration 865/3520 Training loss: 1.7579 0.0849 sec/batch\n",
      "Epoch 5/20  Iteration 866/3520 Training loss: 1.7578 0.0882 sec/batch\n",
      "Epoch 5/20  Iteration 867/3520 Training loss: 1.7577 0.0907 sec/batch\n",
      "Epoch 5/20  Iteration 868/3520 Training loss: 1.7574 0.0944 sec/batch\n",
      "Epoch 5/20  Iteration 869/3520 Training loss: 1.7569 0.0940 sec/batch\n",
      "Epoch 5/20  Iteration 870/3520 Training loss: 1.7566 0.0863 sec/batch\n",
      "Epoch 5/20  Iteration 871/3520 Training loss: 1.7563 0.0842 sec/batch\n",
      "Epoch 5/20  Iteration 872/3520 Training loss: 1.7561 0.0884 sec/batch\n",
      "Epoch 5/20  Iteration 873/3520 Training loss: 1.7561 0.0878 sec/batch\n",
      "Epoch 5/20  Iteration 874/3520 Training loss: 1.7558 0.0871 sec/batch\n",
      "Epoch 5/20  Iteration 875/3520 Training loss: 1.7556 0.0885 sec/batch\n",
      "Epoch 5/20  Iteration 876/3520 Training loss: 1.7555 0.0843 sec/batch\n",
      "Epoch 5/20  Iteration 877/3520 Training loss: 1.7552 0.0877 sec/batch\n",
      "Epoch 5/20  Iteration 878/3520 Training loss: 1.7551 0.0834 sec/batch\n",
      "Epoch 5/20  Iteration 879/3520 Training loss: 1.7549 0.0880 sec/batch\n",
      "Epoch 5/20  Iteration 880/3520 Training loss: 1.7549 0.0877 sec/batch\n",
      "Epoch 6/20  Iteration 881/3520 Training loss: 1.7294 0.0867 sec/batch\n",
      "Epoch 6/20  Iteration 882/3520 Training loss: 1.6948 0.0885 sec/batch\n",
      "Epoch 6/20  Iteration 883/3520 Training loss: 1.7044 0.0855 sec/batch\n",
      "Epoch 6/20  Iteration 884/3520 Training loss: 1.7091 0.0868 sec/batch\n",
      "Epoch 6/20  Iteration 885/3520 Training loss: 1.7110 0.0925 sec/batch\n",
      "Epoch 6/20  Iteration 886/3520 Training loss: 1.7128 0.0949 sec/batch\n",
      "Epoch 6/20  Iteration 887/3520 Training loss: 1.7115 0.0920 sec/batch\n",
      "Epoch 6/20  Iteration 888/3520 Training loss: 1.7143 0.0848 sec/batch\n",
      "Epoch 6/20  Iteration 889/3520 Training loss: 1.7113 0.0882 sec/batch\n",
      "Epoch 6/20  Iteration 890/3520 Training loss: 1.7123 0.0880 sec/batch\n",
      "Epoch 6/20  Iteration 891/3520 Training loss: 1.7125 0.0868 sec/batch\n",
      "Epoch 6/20  Iteration 892/3520 Training loss: 1.7131 0.0848 sec/batch\n",
      "Epoch 6/20  Iteration 893/3520 Training loss: 1.7113 0.0864 sec/batch\n",
      "Epoch 6/20  Iteration 894/3520 Training loss: 1.7128 0.0884 sec/batch\n",
      "Epoch 6/20  Iteration 895/3520 Training loss: 1.7131 0.0837 sec/batch\n",
      "Epoch 6/20  Iteration 896/3520 Training loss: 1.7132 0.0933 sec/batch\n",
      "Epoch 6/20  Iteration 897/3520 Training loss: 1.7129 0.0878 sec/batch\n",
      "Epoch 6/20  Iteration 898/3520 Training loss: 1.7136 0.0843 sec/batch\n",
      "Epoch 6/20  Iteration 899/3520 Training loss: 1.7127 0.0936 sec/batch\n",
      "Epoch 6/20  Iteration 900/3520 Training loss: 1.7140 0.0861 sec/batch\n",
      "Epoch 6/20  Iteration 901/3520 Training loss: 1.7146 0.0853 sec/batch\n",
      "Epoch 6/20  Iteration 902/3520 Training loss: 1.7137 0.0878 sec/batch\n",
      "Epoch 6/20  Iteration 903/3520 Training loss: 1.7147 0.0863 sec/batch\n",
      "Epoch 6/20  Iteration 904/3520 Training loss: 1.7151 0.0916 sec/batch\n",
      "Epoch 6/20  Iteration 905/3520 Training loss: 1.7153 0.0876 sec/batch\n",
      "Epoch 6/20  Iteration 906/3520 Training loss: 1.7140 0.0871 sec/batch\n",
      "Epoch 6/20  Iteration 907/3520 Training loss: 1.7130 0.0939 sec/batch\n",
      "Epoch 6/20  Iteration 908/3520 Training loss: 1.7129 0.0868 sec/batch\n",
      "Epoch 6/20  Iteration 909/3520 Training loss: 1.7120 0.0868 sec/batch\n",
      "Epoch 6/20  Iteration 910/3520 Training loss: 1.7117 0.0865 sec/batch\n",
      "Epoch 6/20  Iteration 911/3520 Training loss: 1.7114 0.0868 sec/batch\n",
      "Epoch 6/20  Iteration 912/3520 Training loss: 1.7112 0.0923 sec/batch\n",
      "Epoch 6/20  Iteration 913/3520 Training loss: 1.7119 0.0853 sec/batch\n",
      "Epoch 6/20  Iteration 914/3520 Training loss: 1.7115 0.0842 sec/batch\n",
      "Epoch 6/20  Iteration 915/3520 Training loss: 1.7114 0.0837 sec/batch\n",
      "Epoch 6/20  Iteration 916/3520 Training loss: 1.7116 0.0861 sec/batch\n",
      "Epoch 6/20  Iteration 917/3520 Training loss: 1.7106 0.0866 sec/batch\n",
      "Epoch 6/20  Iteration 918/3520 Training loss: 1.7114 0.0885 sec/batch\n",
      "Epoch 6/20  Iteration 919/3520 Training loss: 1.7122 0.0855 sec/batch\n",
      "Epoch 6/20  Iteration 920/3520 Training loss: 1.7120 0.0880 sec/batch\n",
      "Epoch 6/20  Iteration 921/3520 Training loss: 1.7117 0.0875 sec/batch\n",
      "Epoch 6/20  Iteration 922/3520 Training loss: 1.7108 0.0867 sec/batch\n",
      "Epoch 6/20  Iteration 923/3520 Training loss: 1.7110 0.0865 sec/batch\n",
      "Epoch 6/20  Iteration 924/3520 Training loss: 1.7102 0.0872 sec/batch\n",
      "Epoch 6/20  Iteration 925/3520 Training loss: 1.7100 0.0900 sec/batch\n",
      "Epoch 6/20  Iteration 926/3520 Training loss: 1.7095 0.0858 sec/batch\n",
      "Epoch 6/20  Iteration 927/3520 Training loss: 1.7091 0.0871 sec/batch\n",
      "Epoch 6/20  Iteration 928/3520 Training loss: 1.7091 0.0894 sec/batch\n",
      "Epoch 6/20  Iteration 929/3520 Training loss: 1.7088 0.0875 sec/batch\n",
      "Epoch 6/20  Iteration 930/3520 Training loss: 1.7088 0.0862 sec/batch\n",
      "Epoch 6/20  Iteration 931/3520 Training loss: 1.7086 0.0847 sec/batch\n",
      "Epoch 6/20  Iteration 932/3520 Training loss: 1.7086 0.0855 sec/batch\n",
      "Epoch 6/20  Iteration 933/3520 Training loss: 1.7087 0.0877 sec/batch\n",
      "Epoch 6/20  Iteration 934/3520 Training loss: 1.7085 0.0866 sec/batch\n",
      "Epoch 6/20  Iteration 935/3520 Training loss: 1.7080 0.0910 sec/batch\n",
      "Epoch 6/20  Iteration 936/3520 Training loss: 1.7075 0.0839 sec/batch\n",
      "Epoch 6/20  Iteration 937/3520 Training loss: 1.7070 0.0855 sec/batch\n",
      "Epoch 6/20  Iteration 938/3520 Training loss: 1.7068 0.0872 sec/batch\n",
      "Epoch 6/20  Iteration 939/3520 Training loss: 1.7065 0.0878 sec/batch\n",
      "Epoch 6/20  Iteration 940/3520 Training loss: 1.7069 0.0863 sec/batch\n",
      "Epoch 6/20  Iteration 941/3520 Training loss: 1.7064 0.0888 sec/batch\n",
      "Epoch 6/20  Iteration 942/3520 Training loss: 1.7062 0.0935 sec/batch\n",
      "Epoch 6/20  Iteration 943/3520 Training loss: 1.7059 0.0860 sec/batch\n",
      "Epoch 6/20  Iteration 944/3520 Training loss: 1.7061 0.0861 sec/batch\n",
      "Epoch 6/20  Iteration 945/3520 Training loss: 1.7053 0.0909 sec/batch\n",
      "Epoch 6/20  Iteration 946/3520 Training loss: 1.7050 0.0938 sec/batch\n",
      "Epoch 6/20  Iteration 947/3520 Training loss: 1.7047 0.0902 sec/batch\n",
      "Epoch 6/20  Iteration 948/3520 Training loss: 1.7041 0.0858 sec/batch\n",
      "Epoch 6/20  Iteration 949/3520 Training loss: 1.7038 0.0862 sec/batch\n",
      "Epoch 6/20  Iteration 950/3520 Training loss: 1.7039 0.0888 sec/batch\n",
      "Epoch 6/20  Iteration 951/3520 Training loss: 1.7036 0.0848 sec/batch\n",
      "Epoch 6/20  Iteration 952/3520 Training loss: 1.7032 0.0834 sec/batch\n",
      "Epoch 6/20  Iteration 953/3520 Training loss: 1.7027 0.0877 sec/batch\n",
      "Epoch 6/20  Iteration 954/3520 Training loss: 1.7022 0.0871 sec/batch\n",
      "Epoch 6/20  Iteration 955/3520 Training loss: 1.7020 0.0857 sec/batch\n",
      "Epoch 6/20  Iteration 956/3520 Training loss: 1.7021 0.0859 sec/batch\n",
      "Epoch 6/20  Iteration 957/3520 Training loss: 1.7021 0.0854 sec/batch\n",
      "Epoch 6/20  Iteration 958/3520 Training loss: 1.7023 0.0927 sec/batch\n",
      "Epoch 6/20  Iteration 959/3520 Training loss: 1.7019 0.0844 sec/batch\n",
      "Epoch 6/20  Iteration 960/3520 Training loss: 1.7017 0.0875 sec/batch\n",
      "Epoch 6/20  Iteration 961/3520 Training loss: 1.7011 0.0871 sec/batch\n",
      "Epoch 6/20  Iteration 962/3520 Training loss: 1.7013 0.0859 sec/batch\n",
      "Epoch 6/20  Iteration 963/3520 Training loss: 1.7009 0.0872 sec/batch\n",
      "Epoch 6/20  Iteration 964/3520 Training loss: 1.7010 0.0919 sec/batch\n",
      "Epoch 6/20  Iteration 965/3520 Training loss: 1.7007 0.0838 sec/batch\n",
      "Epoch 6/20  Iteration 966/3520 Training loss: 1.7001 0.0879 sec/batch\n",
      "Epoch 6/20  Iteration 967/3520 Training loss: 1.7004 0.0842 sec/batch\n",
      "Epoch 6/20  Iteration 968/3520 Training loss: 1.7002 0.0870 sec/batch\n",
      "Epoch 6/20  Iteration 969/3520 Training loss: 1.7001 0.0852 sec/batch\n",
      "Epoch 6/20  Iteration 970/3520 Training loss: 1.6998 0.0867 sec/batch\n",
      "Epoch 6/20  Iteration 971/3520 Training loss: 1.6997 0.0849 sec/batch\n",
      "Epoch 6/20  Iteration 972/3520 Training loss: 1.6999 0.0912 sec/batch\n",
      "Epoch 6/20  Iteration 973/3520 Training loss: 1.6996 0.0879 sec/batch\n",
      "Epoch 6/20  Iteration 974/3520 Training loss: 1.6992 0.0931 sec/batch\n",
      "Epoch 6/20  Iteration 975/3520 Training loss: 1.6989 0.0865 sec/batch\n",
      "Epoch 6/20  Iteration 976/3520 Training loss: 1.6985 0.0841 sec/batch\n",
      "Epoch 6/20  Iteration 977/3520 Training loss: 1.6987 0.0906 sec/batch\n",
      "Epoch 6/20  Iteration 978/3520 Training loss: 1.6985 0.0890 sec/batch\n",
      "Epoch 6/20  Iteration 979/3520 Training loss: 1.6986 0.0898 sec/batch\n",
      "Epoch 6/20  Iteration 980/3520 Training loss: 1.6982 0.0862 sec/batch\n",
      "Epoch 6/20  Iteration 981/3520 Training loss: 1.6981 0.0872 sec/batch\n",
      "Epoch 6/20  Iteration 982/3520 Training loss: 1.6981 0.0873 sec/batch\n",
      "Epoch 6/20  Iteration 983/3520 Training loss: 1.6977 0.0947 sec/batch\n",
      "Epoch 6/20  Iteration 984/3520 Training loss: 1.6972 0.0948 sec/batch\n",
      "Epoch 6/20  Iteration 985/3520 Training loss: 1.6968 0.0952 sec/batch\n",
      "Epoch 6/20  Iteration 986/3520 Training loss: 1.6969 0.0872 sec/batch\n",
      "Epoch 6/20  Iteration 987/3520 Training loss: 1.6966 0.0872 sec/batch\n",
      "Epoch 6/20  Iteration 988/3520 Training loss: 1.6962 0.0857 sec/batch\n",
      "Epoch 6/20  Iteration 989/3520 Training loss: 1.6961 0.0863 sec/batch\n",
      "Epoch 6/20  Iteration 990/3520 Training loss: 1.6959 0.0932 sec/batch\n",
      "Epoch 6/20  Iteration 991/3520 Training loss: 1.6958 0.0844 sec/batch\n",
      "Epoch 6/20  Iteration 992/3520 Training loss: 1.6957 0.0851 sec/batch\n",
      "Epoch 6/20  Iteration 993/3520 Training loss: 1.6959 0.0860 sec/batch\n",
      "Epoch 6/20  Iteration 994/3520 Training loss: 1.6957 0.0869 sec/batch\n",
      "Epoch 6/20  Iteration 995/3520 Training loss: 1.6954 0.0946 sec/batch\n",
      "Epoch 6/20  Iteration 996/3520 Training loss: 1.6953 0.0872 sec/batch\n",
      "Epoch 6/20  Iteration 997/3520 Training loss: 1.6950 0.0843 sec/batch\n",
      "Epoch 6/20  Iteration 998/3520 Training loss: 1.6950 0.0854 sec/batch\n",
      "Epoch 6/20  Iteration 999/3520 Training loss: 1.6947 0.0904 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3520 Training loss: 1.6945 0.0878 sec/batch\n",
      "Validation loss: 1.56562 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1001/3520 Training loss: 1.6950 0.1269 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3520 Training loss: 1.6949 0.1281 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3520 Training loss: 1.6947 0.1310 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3520 Training loss: 1.6946 0.1264 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3520 Training loss: 1.6945 0.1178 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3520 Training loss: 1.6940 0.1035 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3520 Training loss: 1.6935 0.1049 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3520 Training loss: 1.6934 0.0986 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3520 Training loss: 1.6931 0.1000 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3520 Training loss: 1.6933 0.0957 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3520 Training loss: 1.6932 0.0904 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3520 Training loss: 1.6933 0.0895 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3520 Training loss: 1.6933 0.0870 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3520 Training loss: 1.6932 0.0875 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3520 Training loss: 1.6931 0.0876 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3520 Training loss: 1.6930 0.0867 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3520 Training loss: 1.6929 0.0862 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3520 Training loss: 1.6926 0.0870 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3520 Training loss: 1.6926 0.0877 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3520 Training loss: 1.6925 0.0868 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3520 Training loss: 1.6924 0.0874 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3520 Training loss: 1.6922 0.0828 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3520 Training loss: 1.6918 0.0906 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3520 Training loss: 1.6916 0.0839 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3520 Training loss: 1.6916 0.0888 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3520 Training loss: 1.6915 0.0934 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3520 Training loss: 1.6913 0.0946 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3520 Training loss: 1.6912 0.0866 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3520 Training loss: 1.6910 0.0919 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3520 Training loss: 1.6911 0.0868 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3520 Training loss: 1.6909 0.0874 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3520 Training loss: 1.6907 0.0868 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3520 Training loss: 1.6905 0.0876 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3520 Training loss: 1.6901 0.0881 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3520 Training loss: 1.6899 0.0949 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3520 Training loss: 1.6897 0.0870 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3520 Training loss: 1.6895 0.0900 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3520 Training loss: 1.6894 0.0864 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3520 Training loss: 1.6893 0.0847 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3520 Training loss: 1.6893 0.0906 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3520 Training loss: 1.6889 0.0853 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3520 Training loss: 1.6889 0.0850 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3520 Training loss: 1.6889 0.0864 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3520 Training loss: 1.6886 0.0873 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3520 Training loss: 1.6881 0.0906 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3520 Training loss: 1.6879 0.0875 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3520 Training loss: 1.6875 0.0938 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3520 Training loss: 1.6874 0.0882 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3520 Training loss: 1.6874 0.0924 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3520 Training loss: 1.6872 0.0895 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3520 Training loss: 1.6871 0.0840 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3520 Training loss: 1.6869 0.0871 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3520 Training loss: 1.6867 0.0854 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3520 Training loss: 1.6866 0.0857 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3520 Training loss: 1.6865 0.0852 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3520 Training loss: 1.6865 0.0848 sec/batch\n",
      "Epoch 7/20  Iteration 1057/3520 Training loss: 1.6818 0.0865 sec/batch\n",
      "Epoch 7/20  Iteration 1058/3520 Training loss: 1.6439 0.0951 sec/batch\n",
      "Epoch 7/20  Iteration 1059/3520 Training loss: 1.6470 0.0847 sec/batch\n",
      "Epoch 7/20  Iteration 1060/3520 Training loss: 1.6495 0.0846 sec/batch\n",
      "Epoch 7/20  Iteration 1061/3520 Training loss: 1.6527 0.0875 sec/batch\n",
      "Epoch 7/20  Iteration 1062/3520 Training loss: 1.6540 0.0879 sec/batch\n",
      "Epoch 7/20  Iteration 1063/3520 Training loss: 1.6532 0.0868 sec/batch\n",
      "Epoch 7/20  Iteration 1064/3520 Training loss: 1.6561 0.0870 sec/batch\n",
      "Epoch 7/20  Iteration 1065/3520 Training loss: 1.6535 0.0877 sec/batch\n",
      "Epoch 7/20  Iteration 1066/3520 Training loss: 1.6533 0.0865 sec/batch\n",
      "Epoch 7/20  Iteration 1067/3520 Training loss: 1.6541 0.0871 sec/batch\n",
      "Epoch 7/20  Iteration 1068/3520 Training loss: 1.6544 0.0854 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3520 Training loss: 1.6526 0.0904 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3520 Training loss: 1.6540 0.0851 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3520 Training loss: 1.6533 0.0885 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3520 Training loss: 1.6536 0.0883 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3520 Training loss: 1.6541 0.0883 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3520 Training loss: 1.6547 0.0863 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3520 Training loss: 1.6533 0.0841 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3520 Training loss: 1.6550 0.0873 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3520 Training loss: 1.6559 0.0937 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3520 Training loss: 1.6555 0.0872 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3520 Training loss: 1.6558 0.0926 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3520 Training loss: 1.6557 0.0946 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3520 Training loss: 1.6560 0.0856 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3520 Training loss: 1.6548 0.0848 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3520 Training loss: 1.6536 0.0904 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3520 Training loss: 1.6534 0.0931 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3520 Training loss: 1.6526 0.0885 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3520 Training loss: 1.6525 0.0860 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3520 Training loss: 1.6526 0.0951 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3520 Training loss: 1.6524 0.0932 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3520 Training loss: 1.6531 0.0867 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3520 Training loss: 1.6524 0.0852 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3520 Training loss: 1.6519 0.0844 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3520 Training loss: 1.6521 0.0856 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3520 Training loss: 1.6511 0.0931 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3520 Training loss: 1.6518 0.0873 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3520 Training loss: 1.6529 0.0881 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3520 Training loss: 1.6529 0.0955 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3520 Training loss: 1.6526 0.0887 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3520 Training loss: 1.6515 0.0929 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3520 Training loss: 1.6517 0.0872 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3520 Training loss: 1.6509 0.0842 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3520 Training loss: 1.6505 0.0854 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3520 Training loss: 1.6499 0.0914 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3520 Training loss: 1.6497 0.0863 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3520 Training loss: 1.6496 0.0946 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3520 Training loss: 1.6493 0.0888 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3520 Training loss: 1.6492 0.0887 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3520 Training loss: 1.6492 0.0847 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3520 Training loss: 1.6493 0.0844 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3520 Training loss: 1.6496 0.0855 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3520 Training loss: 1.6495 0.0873 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3520 Training loss: 1.6490 0.0837 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3520 Training loss: 1.6485 0.0871 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3520 Training loss: 1.6482 0.0883 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3520 Training loss: 1.6481 0.0852 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3520 Training loss: 1.6475 0.0850 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3520 Training loss: 1.6481 0.0880 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3520 Training loss: 1.6478 0.0874 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3520 Training loss: 1.6477 0.0879 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3520 Training loss: 1.6475 0.0882 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3520 Training loss: 1.6476 0.0864 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3520 Training loss: 1.6469 0.0921 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3520 Training loss: 1.6467 0.0948 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3520 Training loss: 1.6465 0.0902 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3520 Training loss: 1.6458 0.0854 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3520 Training loss: 1.6457 0.0860 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3520 Training loss: 1.6457 0.0869 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3520 Training loss: 1.6455 0.0858 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3520 Training loss: 1.6452 0.0882 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3520 Training loss: 1.6447 0.0880 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3520 Training loss: 1.6441 0.0867 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3520 Training loss: 1.6440 0.0882 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3520 Training loss: 1.6441 0.0858 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3520 Training loss: 1.6442 0.0946 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3520 Training loss: 1.6444 0.0861 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3520 Training loss: 1.6441 0.0878 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3520 Training loss: 1.6439 0.0861 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3520 Training loss: 1.6434 0.0897 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3520 Training loss: 1.6436 0.0857 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3520 Training loss: 1.6431 0.0851 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3520 Training loss: 1.6431 0.0833 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3520 Training loss: 1.6428 0.0885 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3520 Training loss: 1.6422 0.0875 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3520 Training loss: 1.6425 0.0877 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3520 Training loss: 1.6423 0.0917 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3520 Training loss: 1.6425 0.0853 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3520 Training loss: 1.6423 0.0852 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3520 Training loss: 1.6422 0.0866 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3520 Training loss: 1.6424 0.0900 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3520 Training loss: 1.6422 0.0911 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3520 Training loss: 1.6419 0.0869 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3520 Training loss: 1.6417 0.0940 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3520 Training loss: 1.6411 0.0918 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3520 Training loss: 1.6414 0.0856 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3520 Training loss: 1.6413 0.0870 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3520 Training loss: 1.6413 0.0848 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3520 Training loss: 1.6410 0.0852 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3520 Training loss: 1.6409 0.0877 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3520 Training loss: 1.6408 0.0875 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3520 Training loss: 1.6404 0.0867 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3520 Training loss: 1.6397 0.0862 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3520 Training loss: 1.6394 0.0834 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3520 Training loss: 1.6395 0.0872 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3520 Training loss: 1.6393 0.0920 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3520 Training loss: 1.6389 0.0861 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3520 Training loss: 1.6388 0.0947 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3520 Training loss: 1.6386 0.0876 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3520 Training loss: 1.6385 0.0851 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3520 Training loss: 1.6386 0.0850 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3520 Training loss: 1.6387 0.0870 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3520 Training loss: 1.6386 0.0884 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3520 Training loss: 1.6383 0.0844 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3520 Training loss: 1.6383 0.0845 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3520 Training loss: 1.6380 0.0842 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3520 Training loss: 1.6381 0.0846 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3520 Training loss: 1.6379 0.0881 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3520 Training loss: 1.6378 0.0866 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3520 Training loss: 1.6376 0.0846 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3520 Training loss: 1.6375 0.0859 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3520 Training loss: 1.6374 0.0860 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3520 Training loss: 1.6373 0.0881 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3520 Training loss: 1.6372 0.0858 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3520 Training loss: 1.6367 0.0839 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3520 Training loss: 1.6364 0.0883 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3520 Training loss: 1.6362 0.0860 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3520 Training loss: 1.6360 0.0997 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3520 Training loss: 1.6363 0.0940 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3520 Training loss: 1.6361 0.0845 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3520 Training loss: 1.6363 0.0944 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3520 Training loss: 1.6363 0.0883 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3520 Training loss: 1.6363 0.0934 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3520 Training loss: 1.6362 0.0918 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3520 Training loss: 1.6361 0.0883 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3520 Training loss: 1.6361 0.0905 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3520 Training loss: 1.6359 0.0862 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3520 Training loss: 1.6360 0.0871 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3520 Training loss: 1.6360 0.0874 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3520 Training loss: 1.6359 0.0865 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3520 Training loss: 1.6357 0.0867 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3520 Training loss: 1.6353 0.0880 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3520 Training loss: 1.6351 0.0868 sec/batch\n",
      "Validation loss: 1.50702 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1201/3520 Training loss: 1.6357 0.0958 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3520 Training loss: 1.6356 0.0874 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3520 Training loss: 1.6354 0.0879 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3520 Training loss: 1.6354 0.0913 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3520 Training loss: 1.6353 0.0856 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3520 Training loss: 1.6354 0.0883 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3520 Training loss: 1.6351 0.0875 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3520 Training loss: 1.6350 0.0859 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3520 Training loss: 1.6347 0.0853 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3520 Training loss: 1.6345 0.0879 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3520 Training loss: 1.6344 0.0856 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3520 Training loss: 1.6342 0.0870 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3520 Training loss: 1.6340 0.0861 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3520 Training loss: 1.6339 0.0848 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3520 Training loss: 1.6339 0.0956 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3520 Training loss: 1.6339 0.0835 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3520 Training loss: 1.6336 0.0876 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3520 Training loss: 1.6336 0.0854 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3520 Training loss: 1.6336 0.0838 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3520 Training loss: 1.6333 0.0845 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3520 Training loss: 1.6329 0.0876 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3520 Training loss: 1.6327 0.0853 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3520 Training loss: 1.6324 0.0866 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3520 Training loss: 1.6324 0.0847 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3520 Training loss: 1.6324 0.0860 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3520 Training loss: 1.6322 0.0897 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3520 Training loss: 1.6321 0.0847 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3520 Training loss: 1.6320 0.0853 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3520 Training loss: 1.6319 0.0842 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3520 Training loss: 1.6319 0.0881 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3520 Training loss: 1.6317 0.0875 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3520 Training loss: 1.6318 0.0877 sec/batch\n",
      "Epoch 8/20  Iteration 1233/3520 Training loss: 1.6223 0.0869 sec/batch\n",
      "Epoch 8/20  Iteration 1234/3520 Training loss: 1.5878 0.0866 sec/batch\n",
      "Epoch 8/20  Iteration 1235/3520 Training loss: 1.5946 0.0870 sec/batch\n",
      "Epoch 8/20  Iteration 1236/3520 Training loss: 1.6001 0.0877 sec/batch\n",
      "Epoch 8/20  Iteration 1237/3520 Training loss: 1.6021 0.0833 sec/batch\n",
      "Epoch 8/20  Iteration 1238/3520 Training loss: 1.6035 0.0896 sec/batch\n",
      "Epoch 8/20  Iteration 1239/3520 Training loss: 1.6039 0.0875 sec/batch\n",
      "Epoch 8/20  Iteration 1240/3520 Training loss: 1.6059 0.0851 sec/batch\n",
      "Epoch 8/20  Iteration 1241/3520 Training loss: 1.6041 0.0881 sec/batch\n",
      "Epoch 8/20  Iteration 1242/3520 Training loss: 1.6044 0.0856 sec/batch\n",
      "Epoch 8/20  Iteration 1243/3520 Training loss: 1.6057 0.0848 sec/batch\n",
      "Epoch 8/20  Iteration 1244/3520 Training loss: 1.6065 0.0879 sec/batch\n",
      "Epoch 8/20  Iteration 1245/3520 Training loss: 1.6047 0.0851 sec/batch\n",
      "Epoch 8/20  Iteration 1246/3520 Training loss: 1.6062 0.0877 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3520 Training loss: 1.6055 0.0879 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3520 Training loss: 1.6054 0.0941 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3520 Training loss: 1.6055 0.0831 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3520 Training loss: 1.6064 0.0850 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3520 Training loss: 1.6051 0.0887 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3520 Training loss: 1.6063 0.0869 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3520 Training loss: 1.6072 0.0850 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3520 Training loss: 1.6065 0.0918 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3520 Training loss: 1.6070 0.0863 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3520 Training loss: 1.6071 0.0861 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3520 Training loss: 1.6077 0.0899 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3520 Training loss: 1.6064 0.0886 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3520 Training loss: 1.6051 0.0871 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3520 Training loss: 1.6051 0.0867 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3520 Training loss: 1.6044 0.0830 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3520 Training loss: 1.6044 0.0864 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3520 Training loss: 1.6041 0.0868 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3520 Training loss: 1.6040 0.0884 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3520 Training loss: 1.6047 0.0861 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3520 Training loss: 1.6042 0.0870 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3520 Training loss: 1.6038 0.0873 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3520 Training loss: 1.6040 0.0876 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3520 Training loss: 1.6029 0.0850 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3520 Training loss: 1.6039 0.0871 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3520 Training loss: 1.6046 0.0861 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3520 Training loss: 1.6045 0.0870 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3520 Training loss: 1.6038 0.0869 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3520 Training loss: 1.6029 0.0881 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3520 Training loss: 1.6034 0.0861 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3520 Training loss: 1.6024 0.0866 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3520 Training loss: 1.6022 0.0883 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3520 Training loss: 1.6018 0.0877 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3520 Training loss: 1.6016 0.0874 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3520 Training loss: 1.6015 0.0854 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3520 Training loss: 1.6013 0.0928 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3520 Training loss: 1.6011 0.0827 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3520 Training loss: 1.6010 0.0869 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3520 Training loss: 1.6010 0.0879 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3520 Training loss: 1.6011 0.0843 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3520 Training loss: 1.6009 0.0852 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3520 Training loss: 1.6006 0.0855 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3520 Training loss: 1.6001 0.0868 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3520 Training loss: 1.5999 0.0845 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3520 Training loss: 1.5999 0.0918 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3520 Training loss: 1.5995 0.0876 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3520 Training loss: 1.6002 0.0865 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3520 Training loss: 1.6000 0.0932 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3520 Training loss: 1.5997 0.0915 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3520 Training loss: 1.5998 0.0876 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3520 Training loss: 1.5998 0.0834 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3520 Training loss: 1.5991 0.0927 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3520 Training loss: 1.5990 0.0844 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3520 Training loss: 1.5987 0.0887 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3520 Training loss: 1.5980 0.0883 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3520 Training loss: 1.5978 0.0841 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3520 Training loss: 1.5979 0.0872 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3520 Training loss: 1.5976 0.0938 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3520 Training loss: 1.5973 0.0942 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3520 Training loss: 1.5969 0.0873 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3520 Training loss: 1.5963 0.0876 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3520 Training loss: 1.5963 0.0869 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3520 Training loss: 1.5965 0.0865 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3520 Training loss: 1.5966 0.0871 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3520 Training loss: 1.5970 0.0871 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3520 Training loss: 1.5966 0.0857 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3520 Training loss: 1.5965 0.0853 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3520 Training loss: 1.5960 0.0834 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3520 Training loss: 1.5963 0.0862 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3520 Training loss: 1.5959 0.0942 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3520 Training loss: 1.5960 0.0921 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3520 Training loss: 1.5958 0.0877 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3520 Training loss: 1.5953 0.0918 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3520 Training loss: 1.5957 0.0853 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3520 Training loss: 1.5955 0.0856 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3520 Training loss: 1.5956 0.0916 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3520 Training loss: 1.5953 0.0918 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3520 Training loss: 1.5952 0.0885 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3520 Training loss: 1.5955 0.0875 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3520 Training loss: 1.5952 0.0850 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3520 Training loss: 1.5949 0.0820 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3520 Training loss: 1.5947 0.0874 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3520 Training loss: 1.5942 0.0839 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3520 Training loss: 1.5944 0.0863 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3520 Training loss: 1.5944 0.0862 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3520 Training loss: 1.5945 0.0893 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3520 Training loss: 1.5943 0.0881 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3520 Training loss: 1.5941 0.0906 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3520 Training loss: 1.5942 0.0847 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3520 Training loss: 1.5938 0.0868 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3520 Training loss: 1.5932 0.0882 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3520 Training loss: 1.5928 0.0886 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3520 Training loss: 1.5929 0.0859 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3520 Training loss: 1.5928 0.0908 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3520 Training loss: 1.5924 0.0854 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3520 Training loss: 1.5923 0.0883 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3520 Training loss: 1.5922 0.0858 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3520 Training loss: 1.5921 0.0878 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3520 Training loss: 1.5921 0.0851 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3520 Training loss: 1.5923 0.0879 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3520 Training loss: 1.5921 0.0856 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3520 Training loss: 1.5919 0.0867 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3520 Training loss: 1.5919 0.0852 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3520 Training loss: 1.5916 0.0874 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3520 Training loss: 1.5918 0.0869 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3520 Training loss: 1.5916 0.0855 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3520 Training loss: 1.5916 0.0908 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3520 Training loss: 1.5915 0.0849 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3520 Training loss: 1.5914 0.0850 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3520 Training loss: 1.5913 0.0849 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3520 Training loss: 1.5913 0.0875 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3520 Training loss: 1.5913 0.0872 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3520 Training loss: 1.5908 0.0836 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3520 Training loss: 1.5906 0.0882 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3520 Training loss: 1.5905 0.0875 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3520 Training loss: 1.5904 0.0839 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3520 Training loss: 1.5906 0.0861 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3520 Training loss: 1.5905 0.0829 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3520 Training loss: 1.5907 0.0839 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3520 Training loss: 1.5906 0.0910 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3520 Training loss: 1.5907 0.0873 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3520 Training loss: 1.5906 0.0875 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3520 Training loss: 1.5905 0.0875 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3520 Training loss: 1.5904 0.0873 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3520 Training loss: 1.5903 0.0865 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3520 Training loss: 1.5904 0.0857 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3520 Training loss: 1.5904 0.0945 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3520 Training loss: 1.5903 0.0860 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3520 Training loss: 1.5901 0.0870 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3520 Training loss: 1.5898 0.0930 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3520 Training loss: 1.5896 0.0884 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3520 Training loss: 1.5897 0.0841 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3520 Training loss: 1.5896 0.0872 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3520 Training loss: 1.5894 0.0863 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3520 Training loss: 1.5894 0.0843 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3520 Training loss: 1.5893 0.0851 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3520 Training loss: 1.5893 0.0866 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3520 Training loss: 1.5891 0.0943 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3520 Training loss: 1.5889 0.0935 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3520 Training loss: 1.5887 0.0866 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3520 Training loss: 1.5885 0.0858 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3520 Training loss: 1.5884 0.0854 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3520 Training loss: 1.5883 0.0865 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3520 Training loss: 1.5881 0.0914 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3520 Training loss: 1.5880 0.0941 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3520 Training loss: 1.5879 0.0854 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3520 Training loss: 1.5880 0.0851 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3520 Training loss: 1.5877 0.0855 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3520 Training loss: 1.5876 0.0850 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3520 Training loss: 1.5876 0.0877 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3520 Training loss: 1.5874 0.0932 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3520 Training loss: 1.5869 0.0843 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3520 Training loss: 1.5868 0.0840 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3520 Training loss: 1.5865 0.0877 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3520 Training loss: 1.5864 0.0870 sec/batch\n",
      "Validation loss: 1.46819 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 1401/3520 Training loss: 1.5869 0.0867 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3520 Training loss: 1.5868 0.0957 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3520 Training loss: 1.5867 0.0868 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3520 Training loss: 1.5866 0.0881 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3520 Training loss: 1.5865 0.0857 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3520 Training loss: 1.5865 0.0852 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3520 Training loss: 1.5864 0.0880 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3520 Training loss: 1.5865 0.0883 sec/batch\n",
      "Epoch 9/20  Iteration 1409/3520 Training loss: 1.6009 0.0876 sec/batch\n",
      "Epoch 9/20  Iteration 1410/3520 Training loss: 1.5583 0.0984 sec/batch\n",
      "Epoch 9/20  Iteration 1411/3520 Training loss: 1.5620 0.0842 sec/batch\n",
      "Epoch 9/20  Iteration 1412/3520 Training loss: 1.5619 0.0840 sec/batch\n",
      "Epoch 9/20  Iteration 1413/3520 Training loss: 1.5632 0.0869 sec/batch\n",
      "Epoch 9/20  Iteration 1414/3520 Training loss: 1.5645 0.0875 sec/batch\n",
      "Epoch 9/20  Iteration 1415/3520 Training loss: 1.5630 0.0855 sec/batch\n",
      "Epoch 9/20  Iteration 1416/3520 Training loss: 1.5649 0.0851 sec/batch\n",
      "Epoch 9/20  Iteration 1417/3520 Training loss: 1.5624 0.0857 sec/batch\n",
      "Epoch 9/20  Iteration 1418/3520 Training loss: 1.5628 0.0856 sec/batch\n",
      "Epoch 9/20  Iteration 1419/3520 Training loss: 1.5630 0.0878 sec/batch\n",
      "Epoch 9/20  Iteration 1420/3520 Training loss: 1.5642 0.0876 sec/batch\n",
      "Epoch 9/20  Iteration 1421/3520 Training loss: 1.5626 0.0880 sec/batch\n",
      "Epoch 9/20  Iteration 1422/3520 Training loss: 1.5644 0.0909 sec/batch\n",
      "Epoch 9/20  Iteration 1423/3520 Training loss: 1.5642 0.0877 sec/batch\n",
      "Epoch 9/20  Iteration 1424/3520 Training loss: 1.5640 0.0872 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3520 Training loss: 1.5634 0.0856 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3520 Training loss: 1.5642 0.0857 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3520 Training loss: 1.5632 0.0869 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3520 Training loss: 1.5645 0.0853 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3520 Training loss: 1.5650 0.0857 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3520 Training loss: 1.5650 0.0841 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3520 Training loss: 1.5657 0.0947 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3520 Training loss: 1.5658 0.0948 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3520 Training loss: 1.5663 0.0871 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3520 Training loss: 1.5655 0.0881 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3520 Training loss: 1.5646 0.0926 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3520 Training loss: 1.5646 0.0865 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3520 Training loss: 1.5640 0.0849 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3520 Training loss: 1.5642 0.0870 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3520 Training loss: 1.5640 0.0869 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3520 Training loss: 1.5640 0.0849 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3520 Training loss: 1.5646 0.0886 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3520 Training loss: 1.5641 0.0877 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3520 Training loss: 1.5638 0.0879 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3520 Training loss: 1.5644 0.0861 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3520 Training loss: 1.5635 0.0836 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3520 Training loss: 1.5643 0.0859 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3520 Training loss: 1.5653 0.0942 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3520 Training loss: 1.5654 0.0848 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3520 Training loss: 1.5649 0.0849 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3520 Training loss: 1.5638 0.0864 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3520 Training loss: 1.5639 0.0854 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3520 Training loss: 1.5629 0.0863 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3520 Training loss: 1.5628 0.0870 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3520 Training loss: 1.5623 0.0853 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3520 Training loss: 1.5620 0.0879 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3520 Training loss: 1.5620 0.0939 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3520 Training loss: 1.5620 0.0872 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3520 Training loss: 1.5619 0.0839 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3520 Training loss: 1.5618 0.0881 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3520 Training loss: 1.5619 0.0929 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3520 Training loss: 1.5621 0.0918 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3520 Training loss: 1.5621 0.0864 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3520 Training loss: 1.5619 0.0848 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3520 Training loss: 1.5615 0.0844 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3520 Training loss: 1.5613 0.0874 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3520 Training loss: 1.5610 0.0879 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3520 Training loss: 1.5605 0.0852 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3520 Training loss: 1.5612 0.0854 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3520 Training loss: 1.5609 0.0881 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3520 Training loss: 1.5609 0.0860 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3520 Training loss: 1.5609 0.0840 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3520 Training loss: 1.5609 0.0881 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3520 Training loss: 1.5602 0.0862 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3520 Training loss: 1.5601 0.0839 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3520 Training loss: 1.5599 0.0941 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3520 Training loss: 1.5592 0.0871 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3520 Training loss: 1.5589 0.0874 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3520 Training loss: 1.5592 0.0862 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3520 Training loss: 1.5590 0.0885 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3520 Training loss: 1.5586 0.0866 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3520 Training loss: 1.5583 0.0848 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3520 Training loss: 1.5579 0.0849 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3520 Training loss: 1.5578 0.0910 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3520 Training loss: 1.5581 0.0875 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3520 Training loss: 1.5583 0.0851 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3520 Training loss: 1.5587 0.0922 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3520 Training loss: 1.5584 0.0857 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3520 Training loss: 1.5584 0.0844 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3520 Training loss: 1.5579 0.0874 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3520 Training loss: 1.5582 0.0870 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3520 Training loss: 1.5580 0.0862 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3520 Training loss: 1.5581 0.0936 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3520 Training loss: 1.5579 0.0856 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3520 Training loss: 1.5574 0.0876 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3520 Training loss: 1.5579 0.0848 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3520 Training loss: 1.5577 0.0881 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3520 Training loss: 1.5578 0.0932 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3520 Training loss: 1.5576 0.0826 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3520 Training loss: 1.5575 0.0870 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3520 Training loss: 1.5577 0.0869 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3520 Training loss: 1.5574 0.0842 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3520 Training loss: 1.5571 0.0870 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3520 Training loss: 1.5569 0.0876 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3520 Training loss: 1.5565 0.0853 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3520 Training loss: 1.5568 0.0869 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3520 Training loss: 1.5567 0.0881 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3520 Training loss: 1.5568 0.0959 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3520 Training loss: 1.5566 0.0890 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3520 Training loss: 1.5564 0.0835 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3520 Training loss: 1.5565 0.0885 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3520 Training loss: 1.5561 0.0858 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3520 Training loss: 1.5554 0.0886 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3520 Training loss: 1.5551 0.0869 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3520 Training loss: 1.5552 0.0875 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3520 Training loss: 1.5551 0.0886 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3520 Training loss: 1.5547 0.0846 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3520 Training loss: 1.5547 0.0845 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3520 Training loss: 1.5545 0.0871 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3520 Training loss: 1.5545 0.0869 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3520 Training loss: 1.5544 0.0842 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3520 Training loss: 1.5546 0.0868 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3520 Training loss: 1.5544 0.0861 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3520 Training loss: 1.5542 0.0946 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3520 Training loss: 1.5541 0.0935 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3520 Training loss: 1.5539 0.0835 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3520 Training loss: 1.5540 0.0850 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3520 Training loss: 1.5539 0.0956 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3520 Training loss: 1.5538 0.0873 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3520 Training loss: 1.5536 0.0856 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3520 Training loss: 1.5536 0.0874 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3520 Training loss: 1.5536 0.0854 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3520 Training loss: 1.5535 0.0880 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3520 Training loss: 1.5534 0.0858 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3520 Training loss: 1.5530 0.0851 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3520 Training loss: 1.5527 0.0862 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3520 Training loss: 1.5527 0.0867 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3520 Training loss: 1.5525 0.0882 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3520 Training loss: 1.5527 0.0896 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3520 Training loss: 1.5527 0.0949 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3520 Training loss: 1.5529 0.0872 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3520 Training loss: 1.5529 0.0860 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3520 Training loss: 1.5529 0.0891 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3520 Training loss: 1.5528 0.0877 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3520 Training loss: 1.5528 0.0869 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3520 Training loss: 1.5528 0.0873 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3520 Training loss: 1.5526 0.0854 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3520 Training loss: 1.5528 0.0855 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3520 Training loss: 1.5528 0.0939 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3520 Training loss: 1.5529 0.0868 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3520 Training loss: 1.5526 0.0850 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3520 Training loss: 1.5523 0.0848 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3520 Training loss: 1.5521 0.0876 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3520 Training loss: 1.5522 0.0872 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3520 Training loss: 1.5522 0.0854 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3520 Training loss: 1.5520 0.0855 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3520 Training loss: 1.5519 0.0852 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3520 Training loss: 1.5518 0.0869 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3520 Training loss: 1.5520 0.0851 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3520 Training loss: 1.5518 0.0959 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3520 Training loss: 1.5517 0.0851 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3520 Training loss: 1.5514 0.0864 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3520 Training loss: 1.5512 0.0862 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3520 Training loss: 1.5511 0.0909 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3520 Training loss: 1.5510 0.0957 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3520 Training loss: 1.5508 0.0858 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3520 Training loss: 1.5508 0.0839 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3520 Training loss: 1.5508 0.0852 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3520 Training loss: 1.5508 0.0878 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3520 Training loss: 1.5505 0.0842 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3520 Training loss: 1.5504 0.0942 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3520 Training loss: 1.5505 0.0864 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3520 Training loss: 1.5502 0.0877 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3520 Training loss: 1.5498 0.0950 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3520 Training loss: 1.5496 0.0885 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3520 Training loss: 1.5494 0.0874 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3520 Training loss: 1.5494 0.0891 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3520 Training loss: 1.5495 0.0873 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3520 Training loss: 1.5493 0.0872 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3520 Training loss: 1.5493 0.0862 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3520 Training loss: 1.5492 0.0949 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3520 Training loss: 1.5492 0.0871 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3520 Training loss: 1.5491 0.0884 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3520 Training loss: 1.5490 0.0868 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3520 Training loss: 1.5492 0.0865 sec/batch\n",
      "Epoch 10/20  Iteration 1585/3520 Training loss: 1.5730 0.0944 sec/batch\n",
      "Epoch 10/20  Iteration 1586/3520 Training loss: 1.5291 0.0884 sec/batch\n",
      "Epoch 10/20  Iteration 1587/3520 Training loss: 1.5296 0.0876 sec/batch\n",
      "Epoch 10/20  Iteration 1588/3520 Training loss: 1.5307 0.0864 sec/batch\n",
      "Epoch 10/20  Iteration 1589/3520 Training loss: 1.5328 0.0854 sec/batch\n",
      "Epoch 10/20  Iteration 1590/3520 Training loss: 1.5321 0.0871 sec/batch\n",
      "Epoch 10/20  Iteration 1591/3520 Training loss: 1.5306 0.0862 sec/batch\n",
      "Epoch 10/20  Iteration 1592/3520 Training loss: 1.5324 0.0880 sec/batch\n",
      "Epoch 10/20  Iteration 1593/3520 Training loss: 1.5295 0.0878 sec/batch\n",
      "Epoch 10/20  Iteration 1594/3520 Training loss: 1.5290 0.0865 sec/batch\n",
      "Epoch 10/20  Iteration 1595/3520 Training loss: 1.5301 0.0864 sec/batch\n",
      "Epoch 10/20  Iteration 1596/3520 Training loss: 1.5305 0.0953 sec/batch\n",
      "Epoch 10/20  Iteration 1597/3520 Training loss: 1.5289 0.0889 sec/batch\n",
      "Epoch 10/20  Iteration 1598/3520 Training loss: 1.5305 0.0880 sec/batch\n",
      "Epoch 10/20  Iteration 1599/3520 Training loss: 1.5299 0.0842 sec/batch\n",
      "Epoch 10/20  Iteration 1600/3520 Training loss: 1.5299 0.0847 sec/batch\n",
      "Validation loss: 1.43041 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 1601/3520 Training loss: 1.5349 0.0851 sec/batch\n",
      "Epoch 10/20  Iteration 1602/3520 Training loss: 1.5357 0.0859 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3520 Training loss: 1.5341 0.0849 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3520 Training loss: 1.5353 0.0927 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3520 Training loss: 1.5355 0.0874 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3520 Training loss: 1.5348 0.0878 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3520 Training loss: 1.5357 0.0831 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3520 Training loss: 1.5359 0.0862 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3520 Training loss: 1.5365 0.0863 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3520 Training loss: 1.5356 0.0843 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3520 Training loss: 1.5345 0.0864 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3520 Training loss: 1.5344 0.0861 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3520 Training loss: 1.5337 0.0878 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3520 Training loss: 1.5338 0.0876 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3520 Training loss: 1.5330 0.0875 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3520 Training loss: 1.5330 0.0930 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3520 Training loss: 1.5340 0.0901 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3520 Training loss: 1.5333 0.0853 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3520 Training loss: 1.5332 0.0875 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3520 Training loss: 1.5335 0.0887 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3520 Training loss: 1.5324 0.0918 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3520 Training loss: 1.5332 0.0848 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3520 Training loss: 1.5341 0.0855 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3520 Training loss: 1.5340 0.0871 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3520 Training loss: 1.5333 0.0882 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3520 Training loss: 1.5324 0.0929 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3520 Training loss: 1.5326 0.0901 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3520 Training loss: 1.5319 0.0865 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3520 Training loss: 1.5316 0.0867 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3520 Training loss: 1.5312 0.0856 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3520 Training loss: 1.5311 0.0865 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3520 Training loss: 1.5310 0.0851 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3520 Training loss: 1.5309 0.0874 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3520 Training loss: 1.5308 0.0832 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3520 Training loss: 1.5308 0.0844 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3520 Training loss: 1.5308 0.0851 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3520 Training loss: 1.5307 0.0881 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3520 Training loss: 1.5306 0.0864 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3520 Training loss: 1.5301 0.0881 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3520 Training loss: 1.5297 0.0876 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3520 Training loss: 1.5294 0.0837 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3520 Training loss: 1.5293 0.0881 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3520 Training loss: 1.5288 0.0858 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3520 Training loss: 1.5295 0.0837 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3520 Training loss: 1.5293 0.0887 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3520 Training loss: 1.5292 0.0871 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3520 Training loss: 1.5291 0.0867 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3520 Training loss: 1.5291 0.0854 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3520 Training loss: 1.5286 0.0864 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3520 Training loss: 1.5285 0.0885 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3520 Training loss: 1.5281 0.0874 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3520 Training loss: 1.5274 0.0883 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3520 Training loss: 1.5272 0.0871 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3520 Training loss: 1.5273 0.0841 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3520 Training loss: 1.5270 0.0882 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3520 Training loss: 1.5268 0.0867 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3520 Training loss: 1.5264 0.0870 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3520 Training loss: 1.5258 0.0856 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3520 Training loss: 1.5257 0.0858 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3520 Training loss: 1.5259 0.0852 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3520 Training loss: 1.5261 0.0869 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3520 Training loss: 1.5265 0.0884 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3520 Training loss: 1.5263 0.0905 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3520 Training loss: 1.5262 0.0839 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3520 Training loss: 1.5258 0.0908 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3520 Training loss: 1.5260 0.0906 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3520 Training loss: 1.5257 0.0886 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3520 Training loss: 1.5259 0.0945 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3520 Training loss: 1.5256 0.0885 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3520 Training loss: 1.5251 0.0863 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3520 Training loss: 1.5254 0.0867 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3520 Training loss: 1.5252 0.0939 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3520 Training loss: 1.5254 0.0945 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3520 Training loss: 1.5252 0.0867 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3520 Training loss: 1.5251 0.0925 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3520 Training loss: 1.5252 0.0914 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3520 Training loss: 1.5248 0.0874 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3520 Training loss: 1.5244 0.0869 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3520 Training loss: 1.5243 0.0875 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3520 Training loss: 1.5239 0.0856 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3520 Training loss: 1.5240 0.0848 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3520 Training loss: 1.5239 0.0859 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3520 Training loss: 1.5240 0.0922 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3520 Training loss: 1.5238 0.0843 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3520 Training loss: 1.5236 0.0867 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3520 Training loss: 1.5237 0.0826 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3520 Training loss: 1.5233 0.0914 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3520 Training loss: 1.5228 0.0883 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3520 Training loss: 1.5224 0.0883 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3520 Training loss: 1.5226 0.0945 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3520 Training loss: 1.5225 0.0870 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3520 Training loss: 1.5221 0.0834 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3520 Training loss: 1.5221 0.0861 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3520 Training loss: 1.5219 0.0838 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3520 Training loss: 1.5220 0.0861 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3520 Training loss: 1.5219 0.0863 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3520 Training loss: 1.5221 0.0845 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3520 Training loss: 1.5220 0.0852 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3520 Training loss: 1.5218 0.0874 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3520 Training loss: 1.5217 0.0854 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3520 Training loss: 1.5214 0.0871 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3520 Training loss: 1.5216 0.0867 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3520 Training loss: 1.5216 0.0872 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3520 Training loss: 1.5215 0.0887 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3520 Training loss: 1.5214 0.0871 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3520 Training loss: 1.5213 0.0880 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3520 Training loss: 1.5212 0.0843 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3520 Training loss: 1.5212 0.0866 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3520 Training loss: 1.5211 0.0850 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3520 Training loss: 1.5207 0.0940 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3520 Training loss: 1.5205 0.0862 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3520 Training loss: 1.5204 0.0941 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3520 Training loss: 1.5203 0.0876 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3520 Training loss: 1.5205 0.0874 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3520 Training loss: 1.5205 0.0882 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3520 Training loss: 1.5207 0.0855 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3520 Training loss: 1.5208 0.0879 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3520 Training loss: 1.5208 0.0872 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3520 Training loss: 1.5209 0.0860 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3520 Training loss: 1.5208 0.0878 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3520 Training loss: 1.5208 0.0867 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3520 Training loss: 1.5207 0.0874 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3520 Training loss: 1.5208 0.0864 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3520 Training loss: 1.5209 0.0886 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3520 Training loss: 1.5209 0.0876 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3520 Training loss: 1.5207 0.0846 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3520 Training loss: 1.5203 0.0863 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3520 Training loss: 1.5203 0.0877 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3520 Training loss: 1.5204 0.0879 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3520 Training loss: 1.5204 0.0856 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3520 Training loss: 1.5203 0.0849 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3520 Training loss: 1.5202 0.0872 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3520 Training loss: 1.5201 0.0858 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3520 Training loss: 1.5203 0.0860 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3520 Training loss: 1.5202 0.0932 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3520 Training loss: 1.5201 0.0946 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3520 Training loss: 1.5199 0.0876 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3520 Training loss: 1.5197 0.0867 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3520 Training loss: 1.5196 0.0855 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3520 Training loss: 1.5195 0.0847 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3520 Training loss: 1.5193 0.0857 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3520 Training loss: 1.5193 0.0886 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3520 Training loss: 1.5192 0.0878 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3520 Training loss: 1.5193 0.0857 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3520 Training loss: 1.5191 0.0866 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3520 Training loss: 1.5190 0.0931 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3520 Training loss: 1.5190 0.0876 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3520 Training loss: 1.5188 0.0862 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3520 Training loss: 1.5184 0.0951 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3520 Training loss: 1.5181 0.0940 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3520 Training loss: 1.5179 0.0895 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3520 Training loss: 1.5179 0.0854 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3520 Training loss: 1.5179 0.0851 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3520 Training loss: 1.5178 0.0862 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3520 Training loss: 1.5177 0.0872 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3520 Training loss: 1.5177 0.0842 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3520 Training loss: 1.5176 0.0948 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3520 Training loss: 1.5176 0.0869 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3520 Training loss: 1.5175 0.0878 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3520 Training loss: 1.5176 0.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1761/3520 Training loss: 1.5389 0.0885 sec/batch\n",
      "Epoch 11/20  Iteration 1762/3520 Training loss: 1.4966 0.0866 sec/batch\n",
      "Epoch 11/20  Iteration 1763/3520 Training loss: 1.4981 0.0902 sec/batch\n",
      "Epoch 11/20  Iteration 1764/3520 Training loss: 1.4990 0.0873 sec/batch\n",
      "Epoch 11/20  Iteration 1765/3520 Training loss: 1.5024 0.0855 sec/batch\n",
      "Epoch 11/20  Iteration 1766/3520 Training loss: 1.5014 0.0929 sec/batch\n",
      "Epoch 11/20  Iteration 1767/3520 Training loss: 1.4996 0.0961 sec/batch\n",
      "Epoch 11/20  Iteration 1768/3520 Training loss: 1.5021 0.0879 sec/batch\n",
      "Epoch 11/20  Iteration 1769/3520 Training loss: 1.4991 0.0864 sec/batch\n",
      "Epoch 11/20  Iteration 1770/3520 Training loss: 1.4993 0.0839 sec/batch\n",
      "Epoch 11/20  Iteration 1771/3520 Training loss: 1.5000 0.0954 sec/batch\n",
      "Epoch 11/20  Iteration 1772/3520 Training loss: 1.5007 0.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1773/3520 Training loss: 1.4995 0.0933 sec/batch\n",
      "Epoch 11/20  Iteration 1774/3520 Training loss: 1.5010 0.0875 sec/batch\n",
      "Epoch 11/20  Iteration 1775/3520 Training loss: 1.5004 0.0868 sec/batch\n",
      "Epoch 11/20  Iteration 1776/3520 Training loss: 1.4996 0.0887 sec/batch\n",
      "Epoch 11/20  Iteration 1777/3520 Training loss: 1.4994 0.0864 sec/batch\n",
      "Epoch 11/20  Iteration 1778/3520 Training loss: 1.5005 0.0965 sec/batch\n",
      "Epoch 11/20  Iteration 1779/3520 Training loss: 1.4997 0.0865 sec/batch\n",
      "Epoch 11/20  Iteration 1780/3520 Training loss: 1.5008 0.0874 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3520 Training loss: 1.5010 0.0944 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3520 Training loss: 1.5007 0.0876 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3520 Training loss: 1.5015 0.0835 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3520 Training loss: 1.5019 0.0868 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3520 Training loss: 1.5024 0.0866 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3520 Training loss: 1.5016 0.0882 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3520 Training loss: 1.5008 0.0876 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3520 Training loss: 1.5007 0.0859 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3520 Training loss: 1.4999 0.0872 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3520 Training loss: 1.5001 0.0859 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3520 Training loss: 1.4998 0.0875 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3520 Training loss: 1.4995 0.0997 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3520 Training loss: 1.5004 0.0843 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3520 Training loss: 1.5003 0.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3520 Training loss: 1.5003 0.0857 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3520 Training loss: 1.5007 0.0835 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3520 Training loss: 1.4996 0.0952 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3520 Training loss: 1.5005 0.0872 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3520 Training loss: 1.5012 0.0851 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3520 Training loss: 1.5012 0.0875 sec/batch\n",
      "Validation loss: 1.39682 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 1801/3520 Training loss: 1.5034 0.0918 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3520 Training loss: 1.5027 0.0868 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3520 Training loss: 1.5029 0.0916 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3520 Training loss: 1.5021 0.0921 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3520 Training loss: 1.5019 0.0852 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3520 Training loss: 1.5015 0.0850 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3520 Training loss: 1.5013 0.0852 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3520 Training loss: 1.5015 0.0884 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3520 Training loss: 1.5014 0.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3520 Training loss: 1.5013 0.0881 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3520 Training loss: 1.5011 0.0862 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3520 Training loss: 1.5010 0.0947 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3520 Training loss: 1.5011 0.0869 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3520 Training loss: 1.5010 0.0856 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3520 Training loss: 1.5006 0.0846 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3520 Training loss: 1.5004 0.0946 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3520 Training loss: 1.5001 0.0849 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3520 Training loss: 1.4999 0.0867 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3520 Training loss: 1.4993 0.0938 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3520 Training loss: 1.4998 0.0925 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3520 Training loss: 1.4995 0.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3520 Training loss: 1.4994 0.0945 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3520 Training loss: 1.4996 0.0861 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3520 Training loss: 1.4997 0.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3520 Training loss: 1.4991 0.0920 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3520 Training loss: 1.4990 0.0864 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3520 Training loss: 1.4988 0.0842 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3520 Training loss: 1.4982 0.0853 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3520 Training loss: 1.4979 0.0858 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3520 Training loss: 1.4980 0.0849 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3520 Training loss: 1.4978 0.0914 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3520 Training loss: 1.4974 0.0904 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3520 Training loss: 1.4972 0.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3520 Training loss: 1.4967 0.0883 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3520 Training loss: 1.4966 0.0868 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3520 Training loss: 1.4969 0.0865 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3520 Training loss: 1.4970 0.0845 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3520 Training loss: 1.4974 0.0869 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3520 Training loss: 1.4971 0.0855 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3520 Training loss: 1.4971 0.0880 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3520 Training loss: 1.4966 0.0931 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3520 Training loss: 1.4969 0.0918 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3520 Training loss: 1.4966 0.0869 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3520 Training loss: 1.4968 0.0856 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3520 Training loss: 1.4967 0.0928 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3520 Training loss: 1.4961 0.0906 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3520 Training loss: 1.4965 0.0878 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3520 Training loss: 1.4963 0.0854 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3520 Training loss: 1.4964 0.0880 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3520 Training loss: 1.4962 0.0947 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3520 Training loss: 1.4961 0.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3520 Training loss: 1.4964 0.0852 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3520 Training loss: 1.4960 0.0965 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3520 Training loss: 1.4957 0.0856 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3520 Training loss: 1.4956 0.0841 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3520 Training loss: 1.4952 0.0954 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3520 Training loss: 1.4955 0.0880 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3520 Training loss: 1.4954 0.0882 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3520 Training loss: 1.4955 0.0931 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3520 Training loss: 1.4952 0.0885 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3520 Training loss: 1.4951 0.0843 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3520 Training loss: 1.4951 0.0887 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3520 Training loss: 1.4948 0.0851 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3520 Training loss: 1.4942 0.0848 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3520 Training loss: 1.4939 0.0840 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3520 Training loss: 1.4940 0.0854 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3520 Training loss: 1.4940 0.0861 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3520 Training loss: 1.4937 0.0875 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3520 Training loss: 1.4938 0.0868 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3520 Training loss: 1.4936 0.0846 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3520 Training loss: 1.4937 0.0837 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3520 Training loss: 1.4937 0.0855 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3520 Training loss: 1.4939 0.0872 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3520 Training loss: 1.4938 0.0837 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3520 Training loss: 1.4936 0.0851 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3520 Training loss: 1.4936 0.0864 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3520 Training loss: 1.4934 0.0857 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3520 Training loss: 1.4936 0.0831 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3520 Training loss: 1.4935 0.0879 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3520 Training loss: 1.4934 0.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3520 Training loss: 1.4935 0.0851 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3520 Training loss: 1.4934 0.0867 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3520 Training loss: 1.4934 0.0870 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3520 Training loss: 1.4933 0.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3520 Training loss: 1.4933 0.0857 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3520 Training loss: 1.4930 0.0859 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3520 Training loss: 1.4928 0.0841 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3520 Training loss: 1.4928 0.0888 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3520 Training loss: 1.4927 0.0825 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3520 Training loss: 1.4929 0.0868 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3520 Training loss: 1.4929 0.0927 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3520 Training loss: 1.4932 0.0871 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3520 Training loss: 1.4932 0.0874 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3520 Training loss: 1.4933 0.0846 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3520 Training loss: 1.4934 0.0875 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3520 Training loss: 1.4934 0.0875 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3520 Training loss: 1.4934 0.0867 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3520 Training loss: 1.4932 0.0823 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3520 Training loss: 1.4934 0.0934 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3520 Training loss: 1.4935 0.0879 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3520 Training loss: 1.4935 0.0864 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3520 Training loss: 1.4934 0.0858 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3520 Training loss: 1.4930 0.0866 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3520 Training loss: 1.4929 0.0864 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3520 Training loss: 1.4930 0.0907 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3520 Training loss: 1.4930 0.0877 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3520 Training loss: 1.4929 0.0850 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3520 Training loss: 1.4927 0.0870 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3520 Training loss: 1.4927 0.0872 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3520 Training loss: 1.4928 0.0848 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3520 Training loss: 1.4927 0.0848 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3520 Training loss: 1.4925 0.0869 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3520 Training loss: 1.4923 0.0853 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3520 Training loss: 1.4921 0.0858 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3520 Training loss: 1.4920 0.0863 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3520 Training loss: 1.4919 0.0839 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3520 Training loss: 1.4917 0.0859 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3520 Training loss: 1.4918 0.0891 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3520 Training loss: 1.4918 0.0855 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3520 Training loss: 1.4919 0.0844 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3520 Training loss: 1.4916 0.0888 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3520 Training loss: 1.4915 0.0896 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3520 Training loss: 1.4916 0.0854 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3520 Training loss: 1.4913 0.0867 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3520 Training loss: 1.4910 0.0845 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3520 Training loss: 1.4908 0.0868 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3520 Training loss: 1.4906 0.0952 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3520 Training loss: 1.4906 0.0875 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3520 Training loss: 1.4907 0.0879 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3520 Training loss: 1.4906 0.0878 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3520 Training loss: 1.4906 0.0946 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3520 Training loss: 1.4905 0.0842 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3520 Training loss: 1.4904 0.0883 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3520 Training loss: 1.4905 0.0880 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3520 Training loss: 1.4904 0.0938 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3520 Training loss: 1.4906 0.0848 sec/batch\n",
      "Epoch 12/20  Iteration 1937/3520 Training loss: 1.5096 0.0876 sec/batch\n",
      "Epoch 12/20  Iteration 1938/3520 Training loss: 1.4694 0.0860 sec/batch\n",
      "Epoch 12/20  Iteration 1939/3520 Training loss: 1.4698 0.0860 sec/batch\n",
      "Epoch 12/20  Iteration 1940/3520 Training loss: 1.4693 0.0871 sec/batch\n",
      "Epoch 12/20  Iteration 1941/3520 Training loss: 1.4733 0.0864 sec/batch\n",
      "Epoch 12/20  Iteration 1942/3520 Training loss: 1.4733 0.0833 sec/batch\n",
      "Epoch 12/20  Iteration 1943/3520 Training loss: 1.4718 0.0885 sec/batch\n",
      "Epoch 12/20  Iteration 1944/3520 Training loss: 1.4743 0.0945 sec/batch\n",
      "Epoch 12/20  Iteration 1945/3520 Training loss: 1.4715 0.0852 sec/batch\n",
      "Epoch 12/20  Iteration 1946/3520 Training loss: 1.4717 0.0866 sec/batch\n",
      "Epoch 12/20  Iteration 1947/3520 Training loss: 1.4727 0.0872 sec/batch\n",
      "Epoch 12/20  Iteration 1948/3520 Training loss: 1.4730 0.0841 sec/batch\n",
      "Epoch 12/20  Iteration 1949/3520 Training loss: 1.4723 0.0855 sec/batch\n",
      "Epoch 12/20  Iteration 1950/3520 Training loss: 1.4738 0.0888 sec/batch\n",
      "Epoch 12/20  Iteration 1951/3520 Training loss: 1.4737 0.0879 sec/batch\n",
      "Epoch 12/20  Iteration 1952/3520 Training loss: 1.4731 0.0855 sec/batch\n",
      "Epoch 12/20  Iteration 1953/3520 Training loss: 1.4730 0.0871 sec/batch\n",
      "Epoch 12/20  Iteration 1954/3520 Training loss: 1.4739 0.0829 sec/batch\n",
      "Epoch 12/20  Iteration 1955/3520 Training loss: 1.4727 0.0852 sec/batch\n",
      "Epoch 12/20  Iteration 1956/3520 Training loss: 1.4742 0.0852 sec/batch\n",
      "Epoch 12/20  Iteration 1957/3520 Training loss: 1.4753 0.0896 sec/batch\n",
      "Epoch 12/20  Iteration 1958/3520 Training loss: 1.4753 0.0881 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3520 Training loss: 1.4760 0.0867 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3520 Training loss: 1.4767 0.0950 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3520 Training loss: 1.4773 0.0869 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3520 Training loss: 1.4767 0.0876 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3520 Training loss: 1.4761 0.0849 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3520 Training loss: 1.4765 0.0861 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3520 Training loss: 1.4757 0.0872 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3520 Training loss: 1.4759 0.0857 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3520 Training loss: 1.4754 0.0868 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3520 Training loss: 1.4752 0.0947 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3520 Training loss: 1.4760 0.0840 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3520 Training loss: 1.4757 0.0872 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3520 Training loss: 1.4755 0.0911 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3520 Training loss: 1.4763 0.0848 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3520 Training loss: 1.4749 0.0876 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3520 Training loss: 1.4758 0.0888 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3520 Training loss: 1.4767 0.0881 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3520 Training loss: 1.4770 0.0870 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3520 Training loss: 1.4767 0.0876 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3520 Training loss: 1.4757 0.0884 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3520 Training loss: 1.4758 0.0996 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3520 Training loss: 1.4751 0.0861 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3520 Training loss: 1.4748 0.0872 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3520 Training loss: 1.4744 0.0980 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3520 Training loss: 1.4742 0.0860 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3520 Training loss: 1.4744 0.0852 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3520 Training loss: 1.4742 0.0863 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3520 Training loss: 1.4744 0.0870 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3520 Training loss: 1.4743 0.0891 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3520 Training loss: 1.4744 0.0876 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3520 Training loss: 1.4746 0.0935 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3520 Training loss: 1.4745 0.0851 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3520 Training loss: 1.4742 0.0884 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3520 Training loss: 1.4739 0.0867 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3520 Training loss: 1.4737 0.0855 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3520 Training loss: 1.4736 0.0858 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3520 Training loss: 1.4731 0.0848 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3520 Training loss: 1.4737 0.0930 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3520 Training loss: 1.4735 0.0952 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3520 Training loss: 1.4735 0.0863 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3520 Training loss: 1.4736 0.0876 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3520 Training loss: 1.4736 0.0941 sec/batch\n",
      "Validation loss: 1.37196 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 2001/3520 Training loss: 1.4747 0.0839 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3520 Training loss: 1.4746 0.0868 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3520 Training loss: 1.4746 0.0876 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3520 Training loss: 1.4739 0.0884 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3520 Training loss: 1.4737 0.0878 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3520 Training loss: 1.4738 0.0878 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3520 Training loss: 1.4737 0.0869 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3520 Training loss: 1.4735 0.0956 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3520 Training loss: 1.4730 0.0953 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3520 Training loss: 1.4725 0.0866 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3520 Training loss: 1.4725 0.0835 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3520 Training loss: 1.4726 0.0849 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3520 Training loss: 1.4727 0.0931 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3520 Training loss: 1.4732 0.0970 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3520 Training loss: 1.4729 0.0856 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3520 Training loss: 1.4729 0.0867 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3520 Training loss: 1.4724 0.0874 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3520 Training loss: 1.4728 0.0882 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3520 Training loss: 1.4725 0.0862 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3520 Training loss: 1.4726 0.0843 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3520 Training loss: 1.4725 0.0866 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3520 Training loss: 1.4721 0.0843 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3520 Training loss: 1.4724 0.0859 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3520 Training loss: 1.4723 0.0901 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3520 Training loss: 1.4725 0.0839 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3520 Training loss: 1.4723 0.0860 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3520 Training loss: 1.4721 0.0865 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3520 Training loss: 1.4723 0.0877 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3520 Training loss: 1.4720 0.0890 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3520 Training loss: 1.4719 0.0851 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3520 Training loss: 1.4718 0.0877 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3520 Training loss: 1.4714 0.0881 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3520 Training loss: 1.4716 0.0878 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3520 Training loss: 1.4716 0.0862 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3520 Training loss: 1.4718 0.0843 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3520 Training loss: 1.4716 0.0872 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3520 Training loss: 1.4716 0.0871 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3520 Training loss: 1.4717 0.0872 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3520 Training loss: 1.4714 0.0868 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3520 Training loss: 1.4709 0.0859 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3520 Training loss: 1.4706 0.0877 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3520 Training loss: 1.4707 0.0869 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3520 Training loss: 1.4707 0.0852 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3520 Training loss: 1.4703 0.0874 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3520 Training loss: 1.4703 0.0870 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3520 Training loss: 1.4702 0.0869 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3520 Training loss: 1.4702 0.0929 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3520 Training loss: 1.4703 0.0916 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3520 Training loss: 1.4705 0.0876 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3520 Training loss: 1.4704 0.0871 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3520 Training loss: 1.4703 0.0867 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3520 Training loss: 1.4702 0.0945 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3520 Training loss: 1.4700 0.0872 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3520 Training loss: 1.4701 0.0872 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3520 Training loss: 1.4701 0.0867 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3520 Training loss: 1.4699 0.0854 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3520 Training loss: 1.4699 0.0843 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3520 Training loss: 1.4699 0.0857 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3520 Training loss: 1.4698 0.0869 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3520 Training loss: 1.4698 0.0888 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3520 Training loss: 1.4696 0.0944 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3520 Training loss: 1.4692 0.0878 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3520 Training loss: 1.4689 0.0856 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3520 Training loss: 1.4689 0.0853 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3520 Training loss: 1.4689 0.0881 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3520 Training loss: 1.4692 0.0953 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3520 Training loss: 1.4691 0.0877 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3520 Training loss: 1.4695 0.0868 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3520 Training loss: 1.4695 0.0844 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3520 Training loss: 1.4697 0.0853 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3520 Training loss: 1.4697 0.0858 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3520 Training loss: 1.4696 0.0874 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3520 Training loss: 1.4697 0.0866 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3520 Training loss: 1.4695 0.0861 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3520 Training loss: 1.4697 0.0828 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3520 Training loss: 1.4698 0.0879 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3520 Training loss: 1.4697 0.0869 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3520 Training loss: 1.4696 0.0876 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3520 Training loss: 1.4693 0.0963 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3520 Training loss: 1.4692 0.0858 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3520 Training loss: 1.4693 0.0844 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3520 Training loss: 1.4694 0.0859 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3520 Training loss: 1.4693 0.0848 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3520 Training loss: 1.4693 0.0864 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3520 Training loss: 1.4693 0.0859 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3520 Training loss: 1.4695 0.0878 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3520 Training loss: 1.4693 0.0861 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3520 Training loss: 1.4692 0.0871 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3520 Training loss: 1.4691 0.0952 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3520 Training loss: 1.4689 0.0919 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3520 Training loss: 1.4688 0.0842 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3520 Training loss: 1.4687 0.0841 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3520 Training loss: 1.4685 0.0848 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3520 Training loss: 1.4686 0.0876 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3520 Training loss: 1.4686 0.0846 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3520 Training loss: 1.4687 0.0860 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3520 Training loss: 1.4685 0.0865 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3520 Training loss: 1.4685 0.0879 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3520 Training loss: 1.4685 0.0850 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3520 Training loss: 1.4683 0.0858 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3520 Training loss: 1.4680 0.0849 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3520 Training loss: 1.4679 0.0861 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3520 Training loss: 1.4676 0.0879 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3520 Training loss: 1.4676 0.0855 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3520 Training loss: 1.4677 0.0857 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3520 Training loss: 1.4677 0.0867 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3520 Training loss: 1.4676 0.0849 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3520 Training loss: 1.4676 0.0866 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3520 Training loss: 1.4675 0.0853 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3520 Training loss: 1.4675 0.0948 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3520 Training loss: 1.4674 0.0872 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3520 Training loss: 1.4677 0.0861 sec/batch\n",
      "Epoch 13/20  Iteration 2113/3520 Training loss: 1.4870 0.0852 sec/batch\n",
      "Epoch 13/20  Iteration 2114/3520 Training loss: 1.4485 0.0892 sec/batch\n",
      "Epoch 13/20  Iteration 2115/3520 Training loss: 1.4482 0.0868 sec/batch\n",
      "Epoch 13/20  Iteration 2116/3520 Training loss: 1.4496 0.0957 sec/batch\n",
      "Epoch 13/20  Iteration 2117/3520 Training loss: 1.4519 0.0879 sec/batch\n",
      "Epoch 13/20  Iteration 2118/3520 Training loss: 1.4513 0.0881 sec/batch\n",
      "Epoch 13/20  Iteration 2119/3520 Training loss: 1.4501 0.0933 sec/batch\n",
      "Epoch 13/20  Iteration 2120/3520 Training loss: 1.4521 0.0878 sec/batch\n",
      "Epoch 13/20  Iteration 2121/3520 Training loss: 1.4488 0.0887 sec/batch\n",
      "Epoch 13/20  Iteration 2122/3520 Training loss: 1.4480 0.0845 sec/batch\n",
      "Epoch 13/20  Iteration 2123/3520 Training loss: 1.4495 0.0934 sec/batch\n",
      "Epoch 13/20  Iteration 2124/3520 Training loss: 1.4506 0.0903 sec/batch\n",
      "Epoch 13/20  Iteration 2125/3520 Training loss: 1.4497 0.0882 sec/batch\n",
      "Epoch 13/20  Iteration 2126/3520 Training loss: 1.4515 0.0859 sec/batch\n",
      "Epoch 13/20  Iteration 2127/3520 Training loss: 1.4510 0.0942 sec/batch\n",
      "Epoch 13/20  Iteration 2128/3520 Training loss: 1.4507 0.0877 sec/batch\n",
      "Epoch 13/20  Iteration 2129/3520 Training loss: 1.4502 0.0860 sec/batch\n",
      "Epoch 13/20  Iteration 2130/3520 Training loss: 1.4508 0.0840 sec/batch\n",
      "Epoch 13/20  Iteration 2131/3520 Training loss: 1.4503 0.0866 sec/batch\n",
      "Epoch 13/20  Iteration 2132/3520 Training loss: 1.4519 0.0864 sec/batch\n",
      "Epoch 13/20  Iteration 2133/3520 Training loss: 1.4524 0.0893 sec/batch\n",
      "Epoch 13/20  Iteration 2134/3520 Training loss: 1.4521 0.0856 sec/batch\n",
      "Epoch 13/20  Iteration 2135/3520 Training loss: 1.4532 0.0850 sec/batch\n",
      "Epoch 13/20  Iteration 2136/3520 Training loss: 1.4540 0.0871 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3520 Training loss: 1.4545 0.0944 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3520 Training loss: 1.4536 0.0863 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3520 Training loss: 1.4530 0.0886 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3520 Training loss: 1.4532 0.0921 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3520 Training loss: 1.4529 0.0943 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3520 Training loss: 1.4531 0.0843 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3520 Training loss: 1.4527 0.0876 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3520 Training loss: 1.4528 0.0845 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3520 Training loss: 1.4537 0.0849 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3520 Training loss: 1.4534 0.0852 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3520 Training loss: 1.4533 0.0883 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3520 Training loss: 1.4539 0.0937 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3520 Training loss: 1.4530 0.0868 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3520 Training loss: 1.4540 0.0886 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3520 Training loss: 1.4546 0.0852 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3520 Training loss: 1.4547 0.0871 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3520 Training loss: 1.4543 0.0916 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3520 Training loss: 1.4535 0.0855 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3520 Training loss: 1.4535 0.0862 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3520 Training loss: 1.4527 0.0871 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3520 Training loss: 1.4522 0.0868 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3520 Training loss: 1.4519 0.0860 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3520 Training loss: 1.4517 0.0855 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3520 Training loss: 1.4520 0.0876 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3520 Training loss: 1.4520 0.0876 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3520 Training loss: 1.4521 0.0952 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3520 Training loss: 1.4520 0.0883 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3520 Training loss: 1.4520 0.0878 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3520 Training loss: 1.4524 0.0848 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3520 Training loss: 1.4523 0.0870 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3520 Training loss: 1.4519 0.0859 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3520 Training loss: 1.4517 0.0866 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3520 Training loss: 1.4516 0.0857 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3520 Training loss: 1.4515 0.0854 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3520 Training loss: 1.4509 0.0847 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3520 Training loss: 1.4514 0.0860 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3520 Training loss: 1.4513 0.0847 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3520 Training loss: 1.4511 0.0856 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3520 Training loss: 1.4511 0.0925 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3520 Training loss: 1.4511 0.0877 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3520 Training loss: 1.4507 0.0851 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3520 Training loss: 1.4506 0.0852 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3520 Training loss: 1.4505 0.0915 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3520 Training loss: 1.4499 0.0891 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3520 Training loss: 1.4497 0.0852 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3520 Training loss: 1.4498 0.0879 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3520 Training loss: 1.4497 0.0862 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3520 Training loss: 1.4495 0.0870 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3520 Training loss: 1.4492 0.0957 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3520 Training loss: 1.4488 0.0972 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3520 Training loss: 1.4486 0.0776 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3520 Training loss: 1.4488 0.0960 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3520 Training loss: 1.4490 0.0894 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3520 Training loss: 1.4494 0.0820 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3520 Training loss: 1.4492 0.0896 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3520 Training loss: 1.4493 0.0964 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3520 Training loss: 1.4489 0.0809 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3520 Training loss: 1.4492 0.0926 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3520 Training loss: 1.4489 0.0787 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3520 Training loss: 1.4491 0.0960 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3520 Training loss: 1.4489 0.0879 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3520 Training loss: 1.4485 0.0975 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3520 Training loss: 1.4488 0.0943 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3520 Training loss: 1.4487 0.0915 sec/batch\n",
      "Validation loss: 1.35489 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 2201/3520 Training loss: 1.4501 0.0865 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3520 Training loss: 1.4501 0.0863 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3520 Training loss: 1.4500 0.0867 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3520 Training loss: 1.4502 0.0863 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3520 Training loss: 1.4500 0.0948 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3520 Training loss: 1.4499 0.0888 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3520 Training loss: 1.4498 0.0872 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3520 Training loss: 1.4495 0.0945 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3520 Training loss: 1.4498 0.0879 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3520 Training loss: 1.4499 0.0935 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3520 Training loss: 1.4501 0.0942 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3520 Training loss: 1.4499 0.0859 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3520 Training loss: 1.4499 0.0872 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3520 Training loss: 1.4500 0.0845 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3520 Training loss: 1.4497 0.0863 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3520 Training loss: 1.4492 0.0860 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3520 Training loss: 1.4490 0.0873 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3520 Training loss: 1.4492 0.0879 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3520 Training loss: 1.4492 0.0870 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3520 Training loss: 1.4489 0.0874 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3520 Training loss: 1.4490 0.0965 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3520 Training loss: 1.4488 0.0902 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3520 Training loss: 1.4489 0.1027 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3520 Training loss: 1.4489 0.0987 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3520 Training loss: 1.4492 0.0939 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3520 Training loss: 1.4491 0.0876 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3520 Training loss: 1.4489 0.0974 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3520 Training loss: 1.4488 0.0800 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3520 Training loss: 1.4486 0.0788 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3520 Training loss: 1.4488 0.0912 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3520 Training loss: 1.4488 0.0935 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3520 Training loss: 1.4487 0.0888 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3520 Training loss: 1.4486 0.0819 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3520 Training loss: 1.4485 0.0983 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3520 Training loss: 1.4485 0.0952 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3520 Training loss: 1.4485 0.0962 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3520 Training loss: 1.4485 0.0953 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3520 Training loss: 1.4481 0.0821 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3520 Training loss: 1.4479 0.0894 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3520 Training loss: 1.4479 0.0886 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3520 Training loss: 1.4478 0.0943 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3520 Training loss: 1.4481 0.0922 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3520 Training loss: 1.4481 0.0789 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3520 Training loss: 1.4484 0.0869 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3520 Training loss: 1.4484 0.0932 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3520 Training loss: 1.4486 0.0953 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3520 Training loss: 1.4486 0.0884 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3520 Training loss: 1.4486 0.0957 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3520 Training loss: 1.4487 0.0869 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3520 Training loss: 1.4485 0.0942 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3520 Training loss: 1.4487 0.0884 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3520 Training loss: 1.4488 0.0853 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3520 Training loss: 1.4488 0.0985 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3520 Training loss: 1.4486 0.0901 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3520 Training loss: 1.4483 0.0966 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3520 Training loss: 1.4483 0.0864 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3520 Training loss: 1.4484 0.0776 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3520 Training loss: 1.4485 0.0940 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3520 Training loss: 1.4483 0.0970 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3520 Training loss: 1.4483 0.0935 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3520 Training loss: 1.4483 0.0782 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3520 Training loss: 1.4484 0.0841 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3520 Training loss: 1.4483 0.0873 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3520 Training loss: 1.4482 0.0861 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3520 Training loss: 1.4481 0.0846 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3520 Training loss: 1.4478 0.0932 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3520 Training loss: 1.4478 0.0861 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3520 Training loss: 1.4477 0.0885 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3520 Training loss: 1.4476 0.0910 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3520 Training loss: 1.4476 0.0846 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3520 Training loss: 1.4477 0.0880 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3520 Training loss: 1.4477 0.0879 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3520 Training loss: 1.4475 0.0859 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3520 Training loss: 1.4475 0.0862 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3520 Training loss: 1.4476 0.0873 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3520 Training loss: 1.4474 0.0929 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3520 Training loss: 1.4471 0.0854 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3520 Training loss: 1.4469 0.0881 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3520 Training loss: 1.4467 0.0855 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3520 Training loss: 1.4467 0.0849 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3520 Training loss: 1.4468 0.0868 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3520 Training loss: 1.4467 0.0941 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3520 Training loss: 1.4468 0.0877 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3520 Training loss: 1.4467 0.0849 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3520 Training loss: 1.4466 0.0877 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3520 Training loss: 1.4466 0.0853 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3520 Training loss: 1.4465 0.0868 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3520 Training loss: 1.4468 0.0856 sec/batch\n",
      "Epoch 14/20  Iteration 2289/3520 Training loss: 1.4807 0.0861 sec/batch\n",
      "Epoch 14/20  Iteration 2290/3520 Training loss: 1.4388 0.0869 sec/batch\n",
      "Epoch 14/20  Iteration 2291/3520 Training loss: 1.4393 0.0853 sec/batch\n",
      "Epoch 14/20  Iteration 2292/3520 Training loss: 1.4363 0.0846 sec/batch\n",
      "Epoch 14/20  Iteration 2293/3520 Training loss: 1.4379 0.0883 sec/batch\n",
      "Epoch 14/20  Iteration 2294/3520 Training loss: 1.4365 0.0882 sec/batch\n",
      "Epoch 14/20  Iteration 2295/3520 Training loss: 1.4332 0.0847 sec/batch\n",
      "Epoch 14/20  Iteration 2296/3520 Training loss: 1.4351 0.0918 sec/batch\n",
      "Epoch 14/20  Iteration 2297/3520 Training loss: 1.4321 0.0849 sec/batch\n",
      "Epoch 14/20  Iteration 2298/3520 Training loss: 1.4318 0.0856 sec/batch\n",
      "Epoch 14/20  Iteration 2299/3520 Training loss: 1.4331 0.0841 sec/batch\n",
      "Epoch 14/20  Iteration 2300/3520 Training loss: 1.4341 0.0851 sec/batch\n",
      "Epoch 14/20  Iteration 2301/3520 Training loss: 1.4334 0.0855 sec/batch\n",
      "Epoch 14/20  Iteration 2302/3520 Training loss: 1.4348 0.0877 sec/batch\n",
      "Epoch 14/20  Iteration 2303/3520 Training loss: 1.4340 0.0845 sec/batch\n",
      "Epoch 14/20  Iteration 2304/3520 Training loss: 1.4338 0.0872 sec/batch\n",
      "Epoch 14/20  Iteration 2305/3520 Training loss: 1.4333 0.0865 sec/batch\n",
      "Epoch 14/20  Iteration 2306/3520 Training loss: 1.4344 0.0859 sec/batch\n",
      "Epoch 14/20  Iteration 2307/3520 Training loss: 1.4332 0.0878 sec/batch\n",
      "Epoch 14/20  Iteration 2308/3520 Training loss: 1.4345 0.0952 sec/batch\n",
      "Epoch 14/20  Iteration 2309/3520 Training loss: 1.4354 0.0870 sec/batch\n",
      "Epoch 14/20  Iteration 2310/3520 Training loss: 1.4347 0.0866 sec/batch\n",
      "Epoch 14/20  Iteration 2311/3520 Training loss: 1.4356 0.0873 sec/batch\n",
      "Epoch 14/20  Iteration 2312/3520 Training loss: 1.4364 0.0877 sec/batch\n",
      "Epoch 14/20  Iteration 2313/3520 Training loss: 1.4372 0.0820 sec/batch\n",
      "Epoch 14/20  Iteration 2314/3520 Training loss: 1.4360 0.0859 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3520 Training loss: 1.4351 0.0849 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3520 Training loss: 1.4350 0.0884 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3520 Training loss: 1.4341 0.0862 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3520 Training loss: 1.4344 0.0938 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3520 Training loss: 1.4340 0.0882 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3520 Training loss: 1.4341 0.0848 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3520 Training loss: 1.4349 0.0949 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3520 Training loss: 1.4344 0.0878 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3520 Training loss: 1.4344 0.0864 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3520 Training loss: 1.4349 0.0867 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3520 Training loss: 1.4339 0.0865 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3520 Training loss: 1.4350 0.0880 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3520 Training loss: 1.4358 0.0871 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3520 Training loss: 1.4359 0.0934 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3520 Training loss: 1.4353 0.0837 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3520 Training loss: 1.4346 0.0850 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3520 Training loss: 1.4350 0.0872 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3520 Training loss: 1.4343 0.0856 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3520 Training loss: 1.4339 0.0853 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3520 Training loss: 1.4334 0.0849 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3520 Training loss: 1.4331 0.0861 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3520 Training loss: 1.4332 0.0951 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3520 Training loss: 1.4332 0.0851 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3520 Training loss: 1.4332 0.0856 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3520 Training loss: 1.4334 0.0883 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3520 Training loss: 1.4333 0.0882 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3520 Training loss: 1.4335 0.0872 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3520 Training loss: 1.4333 0.0862 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3520 Training loss: 1.4329 0.0879 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3520 Training loss: 1.4328 0.0847 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3520 Training loss: 1.4327 0.0955 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3520 Training loss: 1.4327 0.0905 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3520 Training loss: 1.4323 0.0869 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3520 Training loss: 1.4330 0.0835 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3520 Training loss: 1.4327 0.0920 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3520 Training loss: 1.4328 0.0843 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3520 Training loss: 1.4329 0.0866 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3520 Training loss: 1.4331 0.0844 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3520 Training loss: 1.4327 0.0956 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3520 Training loss: 1.4327 0.0880 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3520 Training loss: 1.4325 0.0862 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3520 Training loss: 1.4320 0.0863 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3520 Training loss: 1.4317 0.0867 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3520 Training loss: 1.4319 0.0881 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3520 Training loss: 1.4317 0.0882 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3520 Training loss: 1.4315 0.0869 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3520 Training loss: 1.4312 0.0911 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3520 Training loss: 1.4308 0.0873 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3520 Training loss: 1.4308 0.0952 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3520 Training loss: 1.4310 0.0849 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3520 Training loss: 1.4313 0.0954 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3520 Training loss: 1.4318 0.0897 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3520 Training loss: 1.4316 0.0853 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3520 Training loss: 1.4315 0.0880 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3520 Training loss: 1.4312 0.0929 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3520 Training loss: 1.4315 0.0877 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3520 Training loss: 1.4311 0.0869 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3520 Training loss: 1.4314 0.0849 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3520 Training loss: 1.4313 0.0830 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3520 Training loss: 1.4309 0.0882 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3520 Training loss: 1.4313 0.0851 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3520 Training loss: 1.4312 0.0859 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3520 Training loss: 1.4315 0.0876 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3520 Training loss: 1.4314 0.0881 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3520 Training loss: 1.4313 0.0880 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3520 Training loss: 1.4316 0.0883 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3520 Training loss: 1.4312 0.0870 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3520 Training loss: 1.4310 0.0947 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3520 Training loss: 1.4310 0.0848 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3520 Training loss: 1.4306 0.0861 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3520 Training loss: 1.4309 0.0923 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3520 Training loss: 1.4308 0.0868 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3520 Training loss: 1.4309 0.0866 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3520 Training loss: 1.4307 0.0878 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3520 Training loss: 1.4305 0.0880 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3520 Training loss: 1.4306 0.0891 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3520 Training loss: 1.4304 0.0940 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3520 Training loss: 1.4298 0.0878 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3520 Training loss: 1.4295 0.0856 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3520 Training loss: 1.4296 0.0888 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3520 Training loss: 1.4296 0.0827 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3520 Training loss: 1.4293 0.0848 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3520 Training loss: 1.4294 0.0863 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3520 Training loss: 1.4294 0.0839 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3520 Training loss: 1.4295 0.0877 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3520 Training loss: 1.4295 0.0883 sec/batch\n",
      "Validation loss: 1.33612 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 2401/3520 Training loss: 1.4306 0.0871 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3520 Training loss: 1.4304 0.0850 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3520 Training loss: 1.4302 0.0876 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3520 Training loss: 1.4302 0.0877 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3520 Training loss: 1.4300 0.0849 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3520 Training loss: 1.4303 0.0849 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3520 Training loss: 1.4303 0.0919 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3520 Training loss: 1.4302 0.0809 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3520 Training loss: 1.4302 0.0951 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3520 Training loss: 1.4301 0.0927 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3520 Training loss: 1.4301 0.0890 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3520 Training loss: 1.4301 0.0904 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3520 Training loss: 1.4301 0.0872 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3520 Training loss: 1.4297 0.0879 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3520 Training loss: 1.4295 0.0859 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3520 Training loss: 1.4295 0.0932 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3520 Training loss: 1.4294 0.0884 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3520 Training loss: 1.4297 0.0851 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3520 Training loss: 1.4296 0.0863 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3520 Training loss: 1.4299 0.0881 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3520 Training loss: 1.4298 0.0922 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3520 Training loss: 1.4300 0.0877 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3520 Training loss: 1.4301 0.0874 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3520 Training loss: 1.4301 0.0909 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3520 Training loss: 1.4301 0.0868 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3520 Training loss: 1.4300 0.0863 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3520 Training loss: 1.4301 0.0853 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3520 Training loss: 1.4302 0.0851 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3520 Training loss: 1.4302 0.0868 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3520 Training loss: 1.4300 0.0851 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3520 Training loss: 1.4297 0.0887 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3520 Training loss: 1.4296 0.0876 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3520 Training loss: 1.4297 0.0865 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3520 Training loss: 1.4297 0.0869 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3520 Training loss: 1.4296 0.0853 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3520 Training loss: 1.4296 0.0958 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3520 Training loss: 1.4296 0.0867 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3520 Training loss: 1.4297 0.0947 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3520 Training loss: 1.4296 0.0909 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3520 Training loss: 1.4295 0.0843 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3520 Training loss: 1.4294 0.0875 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3520 Training loss: 1.4292 0.0844 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3520 Training loss: 1.4291 0.0944 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3520 Training loss: 1.4290 0.0956 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3520 Training loss: 1.4289 0.0880 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3520 Training loss: 1.4290 0.0873 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3520 Training loss: 1.4291 0.0875 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3520 Training loss: 1.4292 0.0924 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3520 Training loss: 1.4289 0.0908 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3520 Training loss: 1.4289 0.0940 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3520 Training loss: 1.4289 0.0882 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3520 Training loss: 1.4287 0.0840 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3520 Training loss: 1.4283 0.0848 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3520 Training loss: 1.4282 0.0940 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3520 Training loss: 1.4280 0.0823 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3520 Training loss: 1.4280 0.0842 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3520 Training loss: 1.4281 0.0869 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3520 Training loss: 1.4280 0.0856 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3520 Training loss: 1.4280 0.0870 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3520 Training loss: 1.4280 0.0867 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3520 Training loss: 1.4280 0.0950 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3520 Training loss: 1.4280 0.0876 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3520 Training loss: 1.4279 0.0938 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3520 Training loss: 1.4281 0.0870 sec/batch\n",
      "Epoch 15/20  Iteration 2465/3520 Training loss: 1.4490 0.0878 sec/batch\n",
      "Epoch 15/20  Iteration 2466/3520 Training loss: 1.4123 0.0872 sec/batch\n",
      "Epoch 15/20  Iteration 2467/3520 Training loss: 1.4122 0.0867 sec/batch\n",
      "Epoch 15/20  Iteration 2468/3520 Training loss: 1.4120 0.0883 sec/batch\n",
      "Epoch 15/20  Iteration 2469/3520 Training loss: 1.4156 0.0864 sec/batch\n",
      "Epoch 15/20  Iteration 2470/3520 Training loss: 1.4168 0.0845 sec/batch\n",
      "Epoch 15/20  Iteration 2471/3520 Training loss: 1.4151 0.0942 sec/batch\n",
      "Epoch 15/20  Iteration 2472/3520 Training loss: 1.4170 0.0879 sec/batch\n",
      "Epoch 15/20  Iteration 2473/3520 Training loss: 1.4148 0.0864 sec/batch\n",
      "Epoch 15/20  Iteration 2474/3520 Training loss: 1.4149 0.0854 sec/batch\n",
      "Epoch 15/20  Iteration 2475/3520 Training loss: 1.4163 0.0878 sec/batch\n",
      "Epoch 15/20  Iteration 2476/3520 Training loss: 1.4166 0.0881 sec/batch\n",
      "Epoch 15/20  Iteration 2477/3520 Training loss: 1.4157 0.0915 sec/batch\n",
      "Epoch 15/20  Iteration 2478/3520 Training loss: 1.4177 0.0869 sec/batch\n",
      "Epoch 15/20  Iteration 2479/3520 Training loss: 1.4176 0.0868 sec/batch\n",
      "Epoch 15/20  Iteration 2480/3520 Training loss: 1.4174 0.0842 sec/batch\n",
      "Epoch 15/20  Iteration 2481/3520 Training loss: 1.4171 0.0861 sec/batch\n",
      "Epoch 15/20  Iteration 2482/3520 Training loss: 1.4179 0.0869 sec/batch\n",
      "Epoch 15/20  Iteration 2483/3520 Training loss: 1.4168 0.0879 sec/batch\n",
      "Epoch 15/20  Iteration 2484/3520 Training loss: 1.4179 0.0877 sec/batch\n",
      "Epoch 15/20  Iteration 2485/3520 Training loss: 1.4184 0.0880 sec/batch\n",
      "Epoch 15/20  Iteration 2486/3520 Training loss: 1.4177 0.0883 sec/batch\n",
      "Epoch 15/20  Iteration 2487/3520 Training loss: 1.4185 0.0882 sec/batch\n",
      "Epoch 15/20  Iteration 2488/3520 Training loss: 1.4194 0.0955 sec/batch\n",
      "Epoch 15/20  Iteration 2489/3520 Training loss: 1.4200 0.0827 sec/batch\n",
      "Epoch 15/20  Iteration 2490/3520 Training loss: 1.4190 0.0816 sec/batch\n",
      "Epoch 15/20  Iteration 2491/3520 Training loss: 1.4184 0.0871 sec/batch\n",
      "Epoch 15/20  Iteration 2492/3520 Training loss: 1.4185 0.0844 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3520 Training loss: 1.4179 0.0838 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3520 Training loss: 1.4181 0.0866 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3520 Training loss: 1.4175 0.0898 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3520 Training loss: 1.4174 0.0881 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3520 Training loss: 1.4184 0.0885 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3520 Training loss: 1.4183 0.0845 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3520 Training loss: 1.4182 0.0847 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3520 Training loss: 1.4190 0.0880 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3520 Training loss: 1.4180 0.0852 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3520 Training loss: 1.4189 0.0858 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3520 Training loss: 1.4199 0.0871 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3520 Training loss: 1.4201 0.0855 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3520 Training loss: 1.4196 0.0938 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3520 Training loss: 1.4189 0.0868 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3520 Training loss: 1.4191 0.0873 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3520 Training loss: 1.4183 0.0847 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3520 Training loss: 1.4181 0.0968 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3520 Training loss: 1.4176 0.0903 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3520 Training loss: 1.4175 0.0934 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3520 Training loss: 1.4177 0.0881 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3520 Training loss: 1.4176 0.0896 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3520 Training loss: 1.4178 0.0858 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3520 Training loss: 1.4177 0.0867 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3520 Training loss: 1.4179 0.0843 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3520 Training loss: 1.4182 0.0880 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3520 Training loss: 1.4181 0.0870 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3520 Training loss: 1.4179 0.0883 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3520 Training loss: 1.4178 0.0852 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3520 Training loss: 1.4177 0.0881 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3520 Training loss: 1.4176 0.0884 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3520 Training loss: 1.4172 0.0858 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3520 Training loss: 1.4177 0.0850 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3520 Training loss: 1.4175 0.0888 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3520 Training loss: 1.4175 0.0885 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3520 Training loss: 1.4176 0.0863 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3520 Training loss: 1.4176 0.0872 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3520 Training loss: 1.4172 0.0976 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3520 Training loss: 1.4172 0.0852 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3520 Training loss: 1.4171 0.0915 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3520 Training loss: 1.4164 0.0876 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3520 Training loss: 1.4163 0.0947 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3520 Training loss: 1.4164 0.0939 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3520 Training loss: 1.4163 0.0864 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3520 Training loss: 1.4162 0.0856 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3520 Training loss: 1.4158 0.0951 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3520 Training loss: 1.4154 0.0829 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3520 Training loss: 1.4153 0.0951 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3520 Training loss: 1.4155 0.0867 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3520 Training loss: 1.4157 0.0851 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3520 Training loss: 1.4161 0.0860 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3520 Training loss: 1.4160 0.0881 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3520 Training loss: 1.4159 0.0985 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3520 Training loss: 1.4155 0.0847 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3520 Training loss: 1.4158 0.0863 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3520 Training loss: 1.4155 0.0869 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3520 Training loss: 1.4157 0.0879 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3520 Training loss: 1.4155 0.0878 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3520 Training loss: 1.4151 0.0831 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3520 Training loss: 1.4155 0.0853 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3520 Training loss: 1.4154 0.0874 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3520 Training loss: 1.4156 0.0874 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3520 Training loss: 1.4155 0.0875 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3520 Training loss: 1.4154 0.0938 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3520 Training loss: 1.4155 0.0877 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3520 Training loss: 1.4152 0.0906 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3520 Training loss: 1.4150 0.0847 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3520 Training loss: 1.4150 0.0931 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3520 Training loss: 1.4147 0.0867 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3520 Training loss: 1.4150 0.0853 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3520 Training loss: 1.4150 0.0854 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3520 Training loss: 1.4151 0.0848 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3520 Training loss: 1.4149 0.0960 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3520 Training loss: 1.4148 0.0873 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3520 Training loss: 1.4149 0.0881 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3520 Training loss: 1.4145 0.0856 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3520 Training loss: 1.4141 0.0924 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3520 Training loss: 1.4138 0.0855 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3520 Training loss: 1.4139 0.0857 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3520 Training loss: 1.4139 0.0877 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3520 Training loss: 1.4136 0.0840 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3520 Training loss: 1.4137 0.0926 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3520 Training loss: 1.4136 0.0861 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3520 Training loss: 1.4137 0.0873 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3520 Training loss: 1.4138 0.0829 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3520 Training loss: 1.4140 0.0887 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3520 Training loss: 1.4139 0.0875 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3520 Training loss: 1.4136 0.0864 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3520 Training loss: 1.4136 0.0851 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3520 Training loss: 1.4135 0.0943 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3520 Training loss: 1.4137 0.0864 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3520 Training loss: 1.4137 0.0853 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3520 Training loss: 1.4136 0.0873 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3520 Training loss: 1.4136 0.0894 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3520 Training loss: 1.4136 0.0903 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3520 Training loss: 1.4135 0.0895 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3520 Training loss: 1.4136 0.0868 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3520 Training loss: 1.4135 0.0856 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3520 Training loss: 1.4131 0.0862 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3520 Training loss: 1.4129 0.0869 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3520 Training loss: 1.4130 0.0884 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3520 Training loss: 1.4129 0.0863 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3520 Training loss: 1.4131 0.0835 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3520 Training loss: 1.4130 0.0872 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3520 Training loss: 1.4133 0.0863 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3520 Training loss: 1.4133 0.0879 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3520 Training loss: 1.4135 0.0826 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3520 Training loss: 1.4135 0.0865 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3520 Training loss: 1.4135 0.0840 sec/batch\n",
      "Validation loss: 1.32079 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 2601/3520 Training loss: 1.4142 0.0841 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3520 Training loss: 1.4142 0.0920 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3520 Training loss: 1.4143 0.0883 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3520 Training loss: 1.4144 0.0838 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3520 Training loss: 1.4144 0.0863 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3520 Training loss: 1.4143 0.0871 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3520 Training loss: 1.4141 0.0871 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3520 Training loss: 1.4140 0.0911 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3520 Training loss: 1.4141 0.0857 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3520 Training loss: 1.4141 0.0852 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3520 Training loss: 1.4141 0.0876 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3520 Training loss: 1.4141 0.0848 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3520 Training loss: 1.4141 0.0896 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3520 Training loss: 1.4142 0.0846 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3520 Training loss: 1.4142 0.0878 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3520 Training loss: 1.4141 0.0886 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3520 Training loss: 1.4140 0.0863 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3520 Training loss: 1.4137 0.0853 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3520 Training loss: 1.4137 0.0947 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3520 Training loss: 1.4137 0.0882 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3520 Training loss: 1.4135 0.0860 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3520 Training loss: 1.4136 0.0865 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3520 Training loss: 1.4137 0.0852 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3520 Training loss: 1.4138 0.0832 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3520 Training loss: 1.4136 0.0938 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3520 Training loss: 1.4136 0.0878 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3520 Training loss: 1.4137 0.0862 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3520 Training loss: 1.4135 0.0864 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3520 Training loss: 1.4132 0.0953 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3520 Training loss: 1.4130 0.0847 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3520 Training loss: 1.4128 0.0948 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3520 Training loss: 1.4128 0.0859 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3520 Training loss: 1.4129 0.0879 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3520 Training loss: 1.4129 0.0864 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3520 Training loss: 1.4129 0.0863 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3520 Training loss: 1.4128 0.0844 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3520 Training loss: 1.4128 0.0871 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3520 Training loss: 1.4129 0.0851 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3520 Training loss: 1.4128 0.0957 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3520 Training loss: 1.4131 0.0861 sec/batch\n",
      "Epoch 16/20  Iteration 2641/3520 Training loss: 1.4476 0.0857 sec/batch\n",
      "Epoch 16/20  Iteration 2642/3520 Training loss: 1.4077 0.0866 sec/batch\n",
      "Epoch 16/20  Iteration 2643/3520 Training loss: 1.4055 0.0851 sec/batch\n",
      "Epoch 16/20  Iteration 2644/3520 Training loss: 1.4049 0.0877 sec/batch\n",
      "Epoch 16/20  Iteration 2645/3520 Training loss: 1.4067 0.0947 sec/batch\n",
      "Epoch 16/20  Iteration 2646/3520 Training loss: 1.4038 0.0940 sec/batch\n",
      "Epoch 16/20  Iteration 2647/3520 Training loss: 1.4010 0.0837 sec/batch\n",
      "Epoch 16/20  Iteration 2648/3520 Training loss: 1.4037 0.0959 sec/batch\n",
      "Epoch 16/20  Iteration 2649/3520 Training loss: 1.4004 0.0830 sec/batch\n",
      "Epoch 16/20  Iteration 2650/3520 Training loss: 1.4004 0.0869 sec/batch\n",
      "Epoch 16/20  Iteration 2651/3520 Training loss: 1.4022 0.0859 sec/batch\n",
      "Epoch 16/20  Iteration 2652/3520 Training loss: 1.4030 0.0884 sec/batch\n",
      "Epoch 16/20  Iteration 2653/3520 Training loss: 1.4025 0.0858 sec/batch\n",
      "Epoch 16/20  Iteration 2654/3520 Training loss: 1.4039 0.0854 sec/batch\n",
      "Epoch 16/20  Iteration 2655/3520 Training loss: 1.4034 0.0853 sec/batch\n",
      "Epoch 16/20  Iteration 2656/3520 Training loss: 1.4034 0.0856 sec/batch\n",
      "Epoch 16/20  Iteration 2657/3520 Training loss: 1.4035 0.0847 sec/batch\n",
      "Epoch 16/20  Iteration 2658/3520 Training loss: 1.4047 0.0934 sec/batch\n",
      "Epoch 16/20  Iteration 2659/3520 Training loss: 1.4038 0.0904 sec/batch\n",
      "Epoch 16/20  Iteration 2660/3520 Training loss: 1.4051 0.0878 sec/batch\n",
      "Epoch 16/20  Iteration 2661/3520 Training loss: 1.4059 0.0876 sec/batch\n",
      "Epoch 16/20  Iteration 2662/3520 Training loss: 1.4054 0.0856 sec/batch\n",
      "Epoch 16/20  Iteration 2663/3520 Training loss: 1.4063 0.0838 sec/batch\n",
      "Epoch 16/20  Iteration 2664/3520 Training loss: 1.4073 0.0847 sec/batch\n",
      "Epoch 16/20  Iteration 2665/3520 Training loss: 1.4079 0.0875 sec/batch\n",
      "Epoch 16/20  Iteration 2666/3520 Training loss: 1.4073 0.0931 sec/batch\n",
      "Epoch 16/20  Iteration 2667/3520 Training loss: 1.4067 0.0865 sec/batch\n",
      "Epoch 16/20  Iteration 2668/3520 Training loss: 1.4068 0.0956 sec/batch\n",
      "Epoch 16/20  Iteration 2669/3520 Training loss: 1.4060 0.0814 sec/batch\n",
      "Epoch 16/20  Iteration 2670/3520 Training loss: 1.4062 0.0868 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3520 Training loss: 1.4056 0.0873 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3520 Training loss: 1.4056 0.0859 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3520 Training loss: 1.4061 0.0853 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3520 Training loss: 1.4058 0.0868 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3520 Training loss: 1.4059 0.0842 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3520 Training loss: 1.4063 0.0885 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3520 Training loss: 1.4051 0.0851 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3520 Training loss: 1.4060 0.0926 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3520 Training loss: 1.4068 0.0876 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3520 Training loss: 1.4069 0.0857 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3520 Training loss: 1.4067 0.0874 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3520 Training loss: 1.4061 0.0844 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3520 Training loss: 1.4063 0.0926 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3520 Training loss: 1.4055 0.0951 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3520 Training loss: 1.4052 0.0886 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3520 Training loss: 1.4048 0.0888 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3520 Training loss: 1.4047 0.0879 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3520 Training loss: 1.4048 0.0918 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3520 Training loss: 1.4047 0.0888 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3520 Training loss: 1.4048 0.0865 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3520 Training loss: 1.4048 0.0853 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3520 Training loss: 1.4047 0.0851 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3520 Training loss: 1.4049 0.0848 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3520 Training loss: 1.4049 0.0886 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3520 Training loss: 1.4044 0.0871 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3520 Training loss: 1.4043 0.0872 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3520 Training loss: 1.4041 0.0877 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3520 Training loss: 1.4042 0.0865 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3520 Training loss: 1.4037 0.0842 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3520 Training loss: 1.4043 0.0928 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3520 Training loss: 1.4041 0.0850 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3520 Training loss: 1.4040 0.0876 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3520 Training loss: 1.4040 0.0858 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3520 Training loss: 1.4040 0.0881 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3520 Training loss: 1.4034 0.0886 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3520 Training loss: 1.4035 0.0886 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3520 Training loss: 1.4033 0.0872 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3520 Training loss: 1.4027 0.0969 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3520 Training loss: 1.4025 0.0868 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3520 Training loss: 1.4025 0.0856 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3520 Training loss: 1.4025 0.0869 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3520 Training loss: 1.4023 0.0952 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3520 Training loss: 1.4020 0.0880 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3520 Training loss: 1.4016 0.0850 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3520 Training loss: 1.4015 0.0864 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3520 Training loss: 1.4018 0.0871 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3520 Training loss: 1.4021 0.0878 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3520 Training loss: 1.4025 0.0950 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3520 Training loss: 1.4023 0.0881 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3520 Training loss: 1.4023 0.0849 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3520 Training loss: 1.4019 0.0879 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3520 Training loss: 1.4023 0.0883 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3520 Training loss: 1.4019 0.0887 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3520 Training loss: 1.4022 0.0860 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3520 Training loss: 1.4020 0.0867 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3520 Training loss: 1.4017 0.0871 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3520 Training loss: 1.4022 0.0848 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3520 Training loss: 1.4021 0.0876 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3520 Training loss: 1.4022 0.0847 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3520 Training loss: 1.4021 0.0860 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3520 Training loss: 1.4020 0.0874 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3520 Training loss: 1.4022 0.0875 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3520 Training loss: 1.4019 0.0859 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3520 Training loss: 1.4017 0.0875 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3520 Training loss: 1.4016 0.0958 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3520 Training loss: 1.4012 0.0818 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3520 Training loss: 1.4015 0.0860 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3520 Training loss: 1.4015 0.0853 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3520 Training loss: 1.4016 0.0945 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3520 Training loss: 1.4014 0.0847 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3520 Training loss: 1.4013 0.0852 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3520 Training loss: 1.4014 0.0845 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3520 Training loss: 1.4013 0.0851 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3520 Training loss: 1.4008 0.0853 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3520 Training loss: 1.4005 0.0929 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3520 Training loss: 1.4006 0.0874 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3520 Training loss: 1.4006 0.0873 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3520 Training loss: 1.4005 0.0963 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3520 Training loss: 1.4005 0.0871 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3520 Training loss: 1.4004 0.0875 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3520 Training loss: 1.4005 0.0860 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3520 Training loss: 1.4006 0.0867 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3520 Training loss: 1.4008 0.0874 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3520 Training loss: 1.4006 0.0985 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3520 Training loss: 1.4005 0.0854 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3520 Training loss: 1.4004 0.0851 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3520 Training loss: 1.4002 0.0889 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3520 Training loss: 1.4005 0.0862 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3520 Training loss: 1.4004 0.0916 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3520 Training loss: 1.4004 0.0873 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3520 Training loss: 1.4003 0.0861 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3520 Training loss: 1.4003 0.0855 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3520 Training loss: 1.4002 0.0859 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3520 Training loss: 1.4003 0.0870 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3520 Training loss: 1.4002 0.0857 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3520 Training loss: 1.3999 0.0876 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3520 Training loss: 1.3997 0.0876 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3520 Training loss: 1.3997 0.0888 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3520 Training loss: 1.3996 0.0864 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3520 Training loss: 1.3998 0.0880 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3520 Training loss: 1.3998 0.0878 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3520 Training loss: 1.4000 0.0878 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3520 Training loss: 1.4000 0.0843 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3520 Training loss: 1.4002 0.0942 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3520 Training loss: 1.4003 0.0847 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3520 Training loss: 1.4002 0.0862 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3520 Training loss: 1.4004 0.0856 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3520 Training loss: 1.4003 0.0885 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3520 Training loss: 1.4004 0.0881 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3520 Training loss: 1.4005 0.0863 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3520 Training loss: 1.4005 0.0864 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3520 Training loss: 1.4004 0.0859 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3520 Training loss: 1.4000 0.0881 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3520 Training loss: 1.4000 0.0873 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3520 Training loss: 1.4001 0.0886 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3520 Training loss: 1.4001 0.0887 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3520 Training loss: 1.4000 0.0871 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3520 Training loss: 1.4001 0.0880 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3520 Training loss: 1.4000 0.0842 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3520 Training loss: 1.4002 0.0883 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3520 Training loss: 1.4001 0.0847 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3520 Training loss: 1.4000 0.0851 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3520 Training loss: 1.3999 0.0846 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3520 Training loss: 1.3998 0.0944 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3520 Training loss: 1.3997 0.0873 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3520 Training loss: 1.3997 0.0934 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3520 Training loss: 1.3996 0.0817 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3520 Training loss: 1.3996 0.0870 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3520 Training loss: 1.3997 0.0952 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3520 Training loss: 1.3998 0.0920 sec/batch\n",
      "Validation loss: 1.30885 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 2801/3520 Training loss: 1.4003 0.0876 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3520 Training loss: 1.4003 0.0848 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3520 Training loss: 1.4004 0.0903 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3520 Training loss: 1.4002 0.0865 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3520 Training loss: 1.3999 0.0854 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3520 Training loss: 1.3997 0.0876 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3520 Training loss: 1.3994 0.0865 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3520 Training loss: 1.3994 0.0875 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3520 Training loss: 1.3996 0.0875 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3520 Training loss: 1.3995 0.0880 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3520 Training loss: 1.3996 0.0865 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3520 Training loss: 1.3995 0.0937 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3520 Training loss: 1.3995 0.0846 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3520 Training loss: 1.3996 0.0876 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3520 Training loss: 1.3996 0.0943 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3520 Training loss: 1.3998 0.0863 sec/batch\n",
      "Epoch 17/20  Iteration 2817/3520 Training loss: 1.4322 0.0871 sec/batch\n",
      "Epoch 17/20  Iteration 2818/3520 Training loss: 1.3873 0.0879 sec/batch\n",
      "Epoch 17/20  Iteration 2819/3520 Training loss: 1.3892 0.0858 sec/batch\n",
      "Epoch 17/20  Iteration 2820/3520 Training loss: 1.3904 0.0870 sec/batch\n",
      "Epoch 17/20  Iteration 2821/3520 Training loss: 1.3904 0.0844 sec/batch\n",
      "Epoch 17/20  Iteration 2822/3520 Training loss: 1.3923 0.0849 sec/batch\n",
      "Epoch 17/20  Iteration 2823/3520 Training loss: 1.3902 0.0901 sec/batch\n",
      "Epoch 17/20  Iteration 2824/3520 Training loss: 1.3923 0.0862 sec/batch\n",
      "Epoch 17/20  Iteration 2825/3520 Training loss: 1.3901 0.0859 sec/batch\n",
      "Epoch 17/20  Iteration 2826/3520 Training loss: 1.3900 0.0890 sec/batch\n",
      "Epoch 17/20  Iteration 2827/3520 Training loss: 1.3898 0.0860 sec/batch\n",
      "Epoch 17/20  Iteration 2828/3520 Training loss: 1.3905 0.0870 sec/batch\n",
      "Epoch 17/20  Iteration 2829/3520 Training loss: 1.3898 0.0892 sec/batch\n",
      "Epoch 17/20  Iteration 2830/3520 Training loss: 1.3914 0.0880 sec/batch\n",
      "Epoch 17/20  Iteration 2831/3520 Training loss: 1.3902 0.0861 sec/batch\n",
      "Epoch 17/20  Iteration 2832/3520 Training loss: 1.3901 0.0846 sec/batch\n",
      "Epoch 17/20  Iteration 2833/3520 Training loss: 1.3900 0.0887 sec/batch\n",
      "Epoch 17/20  Iteration 2834/3520 Training loss: 1.3907 0.0873 sec/batch\n",
      "Epoch 17/20  Iteration 2835/3520 Training loss: 1.3895 0.0866 sec/batch\n",
      "Epoch 17/20  Iteration 2836/3520 Training loss: 1.3907 0.0868 sec/batch\n",
      "Epoch 17/20  Iteration 2837/3520 Training loss: 1.3916 0.0865 sec/batch\n",
      "Epoch 17/20  Iteration 2838/3520 Training loss: 1.3910 0.0879 sec/batch\n",
      "Epoch 17/20  Iteration 2839/3520 Training loss: 1.3919 0.0906 sec/batch\n",
      "Epoch 17/20  Iteration 2840/3520 Training loss: 1.3926 0.0864 sec/batch\n",
      "Epoch 17/20  Iteration 2841/3520 Training loss: 1.3934 0.0873 sec/batch\n",
      "Epoch 17/20  Iteration 2842/3520 Training loss: 1.3928 0.0843 sec/batch\n",
      "Epoch 17/20  Iteration 2843/3520 Training loss: 1.3918 0.0848 sec/batch\n",
      "Epoch 17/20  Iteration 2844/3520 Training loss: 1.3918 0.0870 sec/batch\n",
      "Epoch 17/20  Iteration 2845/3520 Training loss: 1.3910 0.0946 sec/batch\n",
      "Epoch 17/20  Iteration 2846/3520 Training loss: 1.3912 0.0856 sec/batch\n",
      "Epoch 17/20  Iteration 2847/3520 Training loss: 1.3907 0.0859 sec/batch\n",
      "Epoch 17/20  Iteration 2848/3520 Training loss: 1.3907 0.0877 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3520 Training loss: 1.3917 0.0878 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3520 Training loss: 1.3913 0.0844 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3520 Training loss: 1.3910 0.0859 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3520 Training loss: 1.3915 0.0845 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3520 Training loss: 1.3901 0.0867 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3520 Training loss: 1.3908 0.0866 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3520 Training loss: 1.3917 0.0887 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3520 Training loss: 1.3920 0.0860 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3520 Training loss: 1.3917 0.0883 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3520 Training loss: 1.3910 0.0833 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3520 Training loss: 1.3913 0.0879 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3520 Training loss: 1.3906 0.0864 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3520 Training loss: 1.3904 0.0871 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3520 Training loss: 1.3901 0.0878 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3520 Training loss: 1.3901 0.0851 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3520 Training loss: 1.3902 0.0879 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3520 Training loss: 1.3902 0.0869 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3520 Training loss: 1.3902 0.0857 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3520 Training loss: 1.3904 0.0954 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3520 Training loss: 1.3904 0.0787 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3520 Training loss: 1.3905 0.0947 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3520 Training loss: 1.3904 0.0885 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3520 Training loss: 1.3902 0.0855 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3520 Training loss: 1.3901 0.0884 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3520 Training loss: 1.3900 0.0845 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3520 Training loss: 1.3899 0.0866 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3520 Training loss: 1.3894 0.0864 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3520 Training loss: 1.3898 0.0864 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3520 Training loss: 1.3897 0.0869 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3520 Training loss: 1.3897 0.0866 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3520 Training loss: 1.3898 0.0855 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3520 Training loss: 1.3898 0.0866 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3520 Training loss: 1.3893 0.0943 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3520 Training loss: 1.3894 0.0874 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3520 Training loss: 1.3895 0.0841 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3520 Training loss: 1.3889 0.0890 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3520 Training loss: 1.3888 0.0920 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3520 Training loss: 1.3888 0.0936 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3520 Training loss: 1.3887 0.0886 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3520 Training loss: 1.3885 0.0874 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3520 Training loss: 1.3882 0.0851 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3520 Training loss: 1.3877 0.0869 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3520 Training loss: 1.3875 0.0875 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3520 Training loss: 1.3878 0.0953 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3520 Training loss: 1.3880 0.0856 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3520 Training loss: 1.3886 0.0845 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3520 Training loss: 1.3885 0.0849 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3520 Training loss: 1.3883 0.0873 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3520 Training loss: 1.3879 0.0871 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3520 Training loss: 1.3882 0.0871 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3520 Training loss: 1.3880 0.0872 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3520 Training loss: 1.3881 0.0859 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3520 Training loss: 1.3879 0.0863 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3520 Training loss: 1.3877 0.0881 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3520 Training loss: 1.3882 0.0870 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3520 Training loss: 1.3882 0.0889 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3520 Training loss: 1.3883 0.0931 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3520 Training loss: 1.3882 0.0869 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3520 Training loss: 1.3881 0.0840 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3520 Training loss: 1.3882 0.0859 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3520 Training loss: 1.3880 0.0847 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3520 Training loss: 1.3879 0.0843 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3520 Training loss: 1.3877 0.0835 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3520 Training loss: 1.3875 0.0845 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3520 Training loss: 1.3877 0.0892 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3520 Training loss: 1.3877 0.0880 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3520 Training loss: 1.3878 0.0874 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3520 Training loss: 1.3875 0.0877 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3520 Training loss: 1.3873 0.0877 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3520 Training loss: 1.3875 0.0954 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3520 Training loss: 1.3873 0.0958 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3520 Training loss: 1.3867 0.0849 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3520 Training loss: 1.3864 0.0883 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3520 Training loss: 1.3864 0.0868 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3520 Training loss: 1.3864 0.0857 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3520 Training loss: 1.3862 0.0834 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3520 Training loss: 1.3863 0.0895 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3520 Training loss: 1.3861 0.0954 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3520 Training loss: 1.3861 0.0915 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3520 Training loss: 1.3862 0.0873 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3520 Training loss: 1.3864 0.0886 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3520 Training loss: 1.3863 0.0849 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3520 Training loss: 1.3861 0.0859 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3520 Training loss: 1.3861 0.0858 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3520 Training loss: 1.3858 0.0859 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3520 Training loss: 1.3861 0.0858 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3520 Training loss: 1.3861 0.0870 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3520 Training loss: 1.3861 0.0860 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3520 Training loss: 1.3860 0.0831 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3520 Training loss: 1.3860 0.0939 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3520 Training loss: 1.3860 0.0857 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3520 Training loss: 1.3860 0.0849 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3520 Training loss: 1.3861 0.0869 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3520 Training loss: 1.3858 0.0888 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3520 Training loss: 1.3856 0.0878 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3520 Training loss: 1.3857 0.0864 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3520 Training loss: 1.3856 0.0857 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3520 Training loss: 1.3858 0.0862 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3520 Training loss: 1.3859 0.0952 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3520 Training loss: 1.3862 0.0886 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3520 Training loss: 1.3861 0.0870 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3520 Training loss: 1.3862 0.0955 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3520 Training loss: 1.3863 0.0877 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3520 Training loss: 1.3863 0.0877 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3520 Training loss: 1.3863 0.0870 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3520 Training loss: 1.3861 0.0879 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3520 Training loss: 1.3863 0.0856 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3520 Training loss: 1.3864 0.0922 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3520 Training loss: 1.3864 0.0877 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3520 Training loss: 1.3863 0.0882 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3520 Training loss: 1.3861 0.0881 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3520 Training loss: 1.3861 0.0849 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3520 Training loss: 1.3863 0.0959 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3520 Training loss: 1.3863 0.0861 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3520 Training loss: 1.3862 0.0858 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3520 Training loss: 1.3862 0.0829 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3520 Training loss: 1.3862 0.0848 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3520 Training loss: 1.3864 0.0834 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3520 Training loss: 1.3863 0.0875 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3520 Training loss: 1.3862 0.0948 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3520 Training loss: 1.3861 0.0860 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3520 Training loss: 1.3858 0.0907 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3520 Training loss: 1.3858 0.0868 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3520 Training loss: 1.3858 0.0857 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3520 Training loss: 1.3856 0.0879 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3520 Training loss: 1.3857 0.0920 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3520 Training loss: 1.3857 0.0877 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3520 Training loss: 1.3859 0.0853 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3520 Training loss: 1.3857 0.0879 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3520 Training loss: 1.3858 0.0875 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3520 Training loss: 1.3859 0.0873 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3520 Training loss: 1.3857 0.0869 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3520 Training loss: 1.3853 0.0961 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3520 Training loss: 1.3852 0.0890 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3520 Training loss: 1.3850 0.0858 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3520 Training loss: 1.3850 0.0852 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3520 Training loss: 1.3852 0.0853 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3520 Training loss: 1.3851 0.0876 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3520 Training loss: 1.3852 0.0862 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3520 Training loss: 1.3852 0.0969 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3520 Training loss: 1.3852 0.0859 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3520 Training loss: 1.3852 0.0851 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3520 Training loss: 1.3852 0.0878 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3520 Training loss: 1.3855 0.0883 sec/batch\n",
      "Epoch 18/20  Iteration 2993/3520 Training loss: 1.4182 0.0873 sec/batch\n",
      "Epoch 18/20  Iteration 2994/3520 Training loss: 1.3837 0.0844 sec/batch\n",
      "Epoch 18/20  Iteration 2995/3520 Training loss: 1.3814 0.0875 sec/batch\n",
      "Epoch 18/20  Iteration 2996/3520 Training loss: 1.3838 0.0850 sec/batch\n",
      "Epoch 18/20  Iteration 2997/3520 Training loss: 1.3859 0.0857 sec/batch\n",
      "Epoch 18/20  Iteration 2998/3520 Training loss: 1.3859 0.0875 sec/batch\n",
      "Epoch 18/20  Iteration 2999/3520 Training loss: 1.3835 0.0855 sec/batch\n",
      "Epoch 18/20  Iteration 3000/3520 Training loss: 1.3851 0.0868 sec/batch\n",
      "Validation loss: 1.2957 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 3001/3520 Training loss: 1.3950 0.0874 sec/batch\n",
      "Epoch 18/20  Iteration 3002/3520 Training loss: 1.3931 0.0871 sec/batch\n",
      "Epoch 18/20  Iteration 3003/3520 Training loss: 1.3928 0.0876 sec/batch\n",
      "Epoch 18/20  Iteration 3004/3520 Training loss: 1.3928 0.0847 sec/batch\n",
      "Epoch 18/20  Iteration 3005/3520 Training loss: 1.3911 0.0863 sec/batch\n",
      "Epoch 18/20  Iteration 3006/3520 Training loss: 1.3921 0.0854 sec/batch\n",
      "Epoch 18/20  Iteration 3007/3520 Training loss: 1.3909 0.0865 sec/batch\n",
      "Epoch 18/20  Iteration 3008/3520 Training loss: 1.3891 0.0837 sec/batch\n",
      "Epoch 18/20  Iteration 3009/3520 Training loss: 1.3879 0.0855 sec/batch\n",
      "Epoch 18/20  Iteration 3010/3520 Training loss: 1.3880 0.0881 sec/batch\n",
      "Epoch 18/20  Iteration 3011/3520 Training loss: 1.3866 0.0951 sec/batch\n",
      "Epoch 18/20  Iteration 3012/3520 Training loss: 1.3870 0.0877 sec/batch\n",
      "Epoch 18/20  Iteration 3013/3520 Training loss: 1.3876 0.0863 sec/batch\n",
      "Epoch 18/20  Iteration 3014/3520 Training loss: 1.3864 0.0848 sec/batch\n",
      "Epoch 18/20  Iteration 3015/3520 Training loss: 1.3873 0.0867 sec/batch\n",
      "Epoch 18/20  Iteration 3016/3520 Training loss: 1.3876 0.0875 sec/batch\n",
      "Epoch 18/20  Iteration 3017/3520 Training loss: 1.3881 0.0860 sec/batch\n",
      "Epoch 18/20  Iteration 3018/3520 Training loss: 1.3875 0.0865 sec/batch\n",
      "Epoch 18/20  Iteration 3019/3520 Training loss: 1.3867 0.0861 sec/batch\n",
      "Epoch 18/20  Iteration 3020/3520 Training loss: 1.3866 0.0852 sec/batch\n",
      "Epoch 18/20  Iteration 3021/3520 Training loss: 1.3855 0.0880 sec/batch\n",
      "Epoch 18/20  Iteration 3022/3520 Training loss: 1.3857 0.0854 sec/batch\n",
      "Epoch 18/20  Iteration 3023/3520 Training loss: 1.3853 0.0872 sec/batch\n",
      "Epoch 18/20  Iteration 3024/3520 Training loss: 1.3849 0.0873 sec/batch\n",
      "Epoch 18/20  Iteration 3025/3520 Training loss: 1.3857 0.0851 sec/batch\n",
      "Epoch 18/20  Iteration 3026/3520 Training loss: 1.3853 0.0877 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3520 Training loss: 1.3853 0.0862 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3520 Training loss: 1.3856 0.0852 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3520 Training loss: 1.3843 0.0876 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3520 Training loss: 1.3849 0.0855 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3520 Training loss: 1.3855 0.0872 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3520 Training loss: 1.3854 0.0969 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3520 Training loss: 1.3850 0.0878 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3520 Training loss: 1.3842 0.0853 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3520 Training loss: 1.3841 0.0854 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3520 Training loss: 1.3833 0.0826 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3520 Training loss: 1.3830 0.0884 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3520 Training loss: 1.3826 0.0869 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3520 Training loss: 1.3826 0.0886 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3520 Training loss: 1.3828 0.0962 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3520 Training loss: 1.3827 0.0866 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3520 Training loss: 1.3827 0.0872 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3520 Training loss: 1.3824 0.0853 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3520 Training loss: 1.3824 0.0853 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3520 Training loss: 1.3823 0.0831 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3520 Training loss: 1.3821 0.0870 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3520 Training loss: 1.3817 0.0845 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3520 Training loss: 1.3816 0.0853 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3520 Training loss: 1.3815 0.0873 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3520 Training loss: 1.3815 0.0866 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3520 Training loss: 1.3811 0.0846 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3520 Training loss: 1.3814 0.0861 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3520 Training loss: 1.3811 0.0868 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3520 Training loss: 1.3810 0.0846 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3520 Training loss: 1.3810 0.0854 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3520 Training loss: 1.3809 0.0862 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3520 Training loss: 1.3803 0.0958 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3520 Training loss: 1.3804 0.0881 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3520 Training loss: 1.3804 0.0870 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3520 Training loss: 1.3800 0.0883 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3520 Training loss: 1.3798 0.0856 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3520 Training loss: 1.3799 0.0849 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3520 Training loss: 1.3798 0.0962 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3520 Training loss: 1.3797 0.0863 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3520 Training loss: 1.3794 0.0859 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3520 Training loss: 1.3790 0.0896 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3520 Training loss: 1.3789 0.0875 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3520 Training loss: 1.3790 0.0856 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3520 Training loss: 1.3793 0.0882 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3520 Training loss: 1.3797 0.0844 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3520 Training loss: 1.3796 0.0851 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3520 Training loss: 1.3795 0.0849 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3520 Training loss: 1.3791 0.0878 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3520 Training loss: 1.3795 0.0852 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3520 Training loss: 1.3793 0.0867 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3520 Training loss: 1.3796 0.0859 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3520 Training loss: 1.3793 0.0856 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3520 Training loss: 1.3788 0.0896 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3520 Training loss: 1.3792 0.0861 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3520 Training loss: 1.3792 0.0860 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3520 Training loss: 1.3793 0.0856 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3520 Training loss: 1.3793 0.0855 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3520 Training loss: 1.3792 0.0873 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3520 Training loss: 1.3793 0.0913 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3520 Training loss: 1.3791 0.0867 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3520 Training loss: 1.3789 0.0890 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3520 Training loss: 1.3787 0.0954 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3520 Training loss: 1.3784 0.0852 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3520 Training loss: 1.3786 0.0936 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3520 Training loss: 1.3786 0.0853 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3520 Training loss: 1.3787 0.0946 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3520 Training loss: 1.3785 0.0832 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3520 Training loss: 1.3783 0.0924 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3520 Training loss: 1.3785 0.0852 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3520 Training loss: 1.3781 0.0837 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3520 Training loss: 1.3776 0.0880 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3520 Training loss: 1.3773 0.0864 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3520 Training loss: 1.3774 0.0873 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3520 Training loss: 1.3774 0.0862 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3520 Training loss: 1.3772 0.0868 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3520 Training loss: 1.3773 0.0941 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3520 Training loss: 1.3772 0.0860 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3520 Training loss: 1.3772 0.0859 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3520 Training loss: 1.3772 0.0852 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3520 Training loss: 1.3773 0.0862 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3520 Training loss: 1.3772 0.0855 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3520 Training loss: 1.3770 0.0934 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3520 Training loss: 1.3769 0.0882 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3520 Training loss: 1.3768 0.0853 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3520 Training loss: 1.3771 0.0841 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3520 Training loss: 1.3772 0.0872 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3520 Training loss: 1.3771 0.0874 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3520 Training loss: 1.3770 0.0874 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3520 Training loss: 1.3769 0.0873 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3520 Training loss: 1.3769 0.0856 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3520 Training loss: 1.3769 0.0882 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3520 Training loss: 1.3768 0.0879 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3520 Training loss: 1.3764 0.0825 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3520 Training loss: 1.3762 0.0868 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3520 Training loss: 1.3763 0.0959 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3520 Training loss: 1.3763 0.0868 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3520 Training loss: 1.3764 0.0881 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3520 Training loss: 1.3764 0.0882 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3520 Training loss: 1.3767 0.0851 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3520 Training loss: 1.3767 0.0854 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3520 Training loss: 1.3769 0.0826 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3520 Training loss: 1.3771 0.0944 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3520 Training loss: 1.3770 0.0800 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3520 Training loss: 1.3770 0.0888 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3520 Training loss: 1.3768 0.0849 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3520 Training loss: 1.3770 0.0889 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3520 Training loss: 1.3772 0.0851 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3520 Training loss: 1.3772 0.0945 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3520 Training loss: 1.3770 0.0820 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3520 Training loss: 1.3767 0.0850 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3520 Training loss: 1.3767 0.0884 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3520 Training loss: 1.3768 0.0871 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3520 Training loss: 1.3768 0.0943 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3520 Training loss: 1.3767 0.0835 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3520 Training loss: 1.3766 0.0868 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3520 Training loss: 1.3766 0.0844 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3520 Training loss: 1.3767 0.0852 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3520 Training loss: 1.3767 0.0881 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3520 Training loss: 1.3766 0.0861 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3520 Training loss: 1.3765 0.0949 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3520 Training loss: 1.3762 0.0851 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3520 Training loss: 1.3761 0.0876 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3520 Training loss: 1.3761 0.0886 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3520 Training loss: 1.3759 0.0938 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3520 Training loss: 1.3759 0.0864 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3520 Training loss: 1.3760 0.0943 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3520 Training loss: 1.3761 0.0863 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3520 Training loss: 1.3758 0.0918 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3520 Training loss: 1.3759 0.0879 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3520 Training loss: 1.3760 0.0865 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3520 Training loss: 1.3758 0.0848 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3520 Training loss: 1.3754 0.0878 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3520 Training loss: 1.3753 0.0850 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3520 Training loss: 1.3750 0.0859 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3520 Training loss: 1.3751 0.0875 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3520 Training loss: 1.3752 0.0947 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3520 Training loss: 1.3751 0.0862 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3520 Training loss: 1.3751 0.0846 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3520 Training loss: 1.3751 0.0871 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3520 Training loss: 1.3751 0.0846 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3520 Training loss: 1.3751 0.0852 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3520 Training loss: 1.3751 0.0866 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3520 Training loss: 1.3754 0.0853 sec/batch\n",
      "Epoch 19/20  Iteration 3169/3520 Training loss: 1.4224 0.0876 sec/batch\n",
      "Epoch 19/20  Iteration 3170/3520 Training loss: 1.3803 0.0887 sec/batch\n",
      "Epoch 19/20  Iteration 3171/3520 Training loss: 1.3786 0.0870 sec/batch\n",
      "Epoch 19/20  Iteration 3172/3520 Training loss: 1.3755 0.0882 sec/batch\n",
      "Epoch 19/20  Iteration 3173/3520 Training loss: 1.3753 0.0935 sec/batch\n",
      "Epoch 19/20  Iteration 3174/3520 Training loss: 1.3745 0.0873 sec/batch\n",
      "Epoch 19/20  Iteration 3175/3520 Training loss: 1.3724 0.0862 sec/batch\n",
      "Epoch 19/20  Iteration 3176/3520 Training loss: 1.3727 0.0876 sec/batch\n",
      "Epoch 19/20  Iteration 3177/3520 Training loss: 1.3695 0.0856 sec/batch\n",
      "Epoch 19/20  Iteration 3178/3520 Training loss: 1.3682 0.0827 sec/batch\n",
      "Epoch 19/20  Iteration 3179/3520 Training loss: 1.3697 0.0857 sec/batch\n",
      "Epoch 19/20  Iteration 3180/3520 Training loss: 1.3701 0.0867 sec/batch\n",
      "Epoch 19/20  Iteration 3181/3520 Training loss: 1.3686 0.0847 sec/batch\n",
      "Epoch 19/20  Iteration 3182/3520 Training loss: 1.3700 0.0848 sec/batch\n",
      "Epoch 19/20  Iteration 3183/3520 Training loss: 1.3693 0.0859 sec/batch\n",
      "Epoch 19/20  Iteration 3184/3520 Training loss: 1.3684 0.0863 sec/batch\n",
      "Epoch 19/20  Iteration 3185/3520 Training loss: 1.3677 0.0874 sec/batch\n",
      "Epoch 19/20  Iteration 3186/3520 Training loss: 1.3688 0.0883 sec/batch\n",
      "Epoch 19/20  Iteration 3187/3520 Training loss: 1.3680 0.0862 sec/batch\n",
      "Epoch 19/20  Iteration 3188/3520 Training loss: 1.3695 0.0872 sec/batch\n",
      "Epoch 19/20  Iteration 3189/3520 Training loss: 1.3702 0.0855 sec/batch\n",
      "Epoch 19/20  Iteration 3190/3520 Training loss: 1.3693 0.0924 sec/batch\n",
      "Epoch 19/20  Iteration 3191/3520 Training loss: 1.3701 0.0861 sec/batch\n",
      "Epoch 19/20  Iteration 3192/3520 Training loss: 1.3707 0.0867 sec/batch\n",
      "Epoch 19/20  Iteration 3193/3520 Training loss: 1.3710 0.0784 sec/batch\n",
      "Epoch 19/20  Iteration 3194/3520 Training loss: 1.3705 0.0856 sec/batch\n",
      "Epoch 19/20  Iteration 3195/3520 Training loss: 1.3696 0.0883 sec/batch\n",
      "Epoch 19/20  Iteration 3196/3520 Training loss: 1.3695 0.0859 sec/batch\n",
      "Epoch 19/20  Iteration 3197/3520 Training loss: 1.3688 0.0870 sec/batch\n",
      "Epoch 19/20  Iteration 3198/3520 Training loss: 1.3693 0.0953 sec/batch\n",
      "Epoch 19/20  Iteration 3199/3520 Training loss: 1.3689 0.0877 sec/batch\n",
      "Epoch 19/20  Iteration 3200/3520 Training loss: 1.3690 0.0877 sec/batch\n",
      "Validation loss: 1.28561 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 3201/3520 Training loss: 1.3732 0.0857 sec/batch\n",
      "Epoch 19/20  Iteration 3202/3520 Training loss: 1.3729 0.0867 sec/batch\n",
      "Epoch 19/20  Iteration 3203/3520 Training loss: 1.3728 0.0873 sec/batch\n",
      "Epoch 19/20  Iteration 3204/3520 Training loss: 1.3735 0.0880 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3520 Training loss: 1.3724 0.0870 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3520 Training loss: 1.3732 0.0851 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3520 Training loss: 1.3738 0.0852 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3520 Training loss: 1.3739 0.0851 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3520 Training loss: 1.3737 0.0870 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3520 Training loss: 1.3730 0.0954 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3520 Training loss: 1.3729 0.0871 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3520 Training loss: 1.3724 0.0850 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3520 Training loss: 1.3722 0.0850 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3520 Training loss: 1.3719 0.0852 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3520 Training loss: 1.3717 0.0865 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3520 Training loss: 1.3719 0.0881 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3520 Training loss: 1.3718 0.0975 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3520 Training loss: 1.3720 0.0904 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3520 Training loss: 1.3719 0.0790 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3520 Training loss: 1.3718 0.0862 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3520 Training loss: 1.3720 0.0861 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3520 Training loss: 1.3718 0.0874 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3520 Training loss: 1.3715 0.0862 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3520 Training loss: 1.3713 0.0881 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3520 Training loss: 1.3711 0.0887 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3520 Training loss: 1.3713 0.0871 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3520 Training loss: 1.3709 0.0958 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3520 Training loss: 1.3712 0.0868 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3520 Training loss: 1.3711 0.0888 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3520 Training loss: 1.3711 0.0876 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3520 Training loss: 1.3713 0.0853 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3520 Training loss: 1.3712 0.0853 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3520 Training loss: 1.3706 0.0864 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3520 Training loss: 1.3705 0.0867 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3520 Training loss: 1.3703 0.0858 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3520 Training loss: 1.3698 0.0870 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3520 Training loss: 1.3697 0.0950 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3520 Training loss: 1.3697 0.0944 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3520 Training loss: 1.3696 0.0879 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3520 Training loss: 1.3693 0.0849 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3520 Training loss: 1.3691 0.0854 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3520 Training loss: 1.3686 0.0936 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3520 Training loss: 1.3686 0.0866 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3520 Training loss: 1.3689 0.0875 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3520 Training loss: 1.3690 0.0856 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3520 Training loss: 1.3694 0.0878 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3520 Training loss: 1.3692 0.0868 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3520 Training loss: 1.3691 0.0877 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3520 Training loss: 1.3687 0.0869 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3520 Training loss: 1.3689 0.0876 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3520 Training loss: 1.3688 0.0869 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3520 Training loss: 1.3690 0.0844 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3520 Training loss: 1.3688 0.0875 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3520 Training loss: 1.3684 0.0863 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3520 Training loss: 1.3686 0.0853 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3520 Training loss: 1.3685 0.0864 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3520 Training loss: 1.3687 0.0877 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3520 Training loss: 1.3686 0.0864 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3520 Training loss: 1.3684 0.0881 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3520 Training loss: 1.3685 0.0869 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3520 Training loss: 1.3684 0.0863 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3520 Training loss: 1.3681 0.0843 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3520 Training loss: 1.3680 0.0880 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3520 Training loss: 1.3677 0.0876 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3520 Training loss: 1.3679 0.0844 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3520 Training loss: 1.3679 0.0848 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3520 Training loss: 1.3680 0.0846 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3520 Training loss: 1.3679 0.0879 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3520 Training loss: 1.3677 0.0884 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3520 Training loss: 1.3677 0.0852 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3520 Training loss: 1.3674 0.0877 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3520 Training loss: 1.3668 0.0873 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3520 Training loss: 1.3665 0.0868 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3520 Training loss: 1.3666 0.0854 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3520 Training loss: 1.3666 0.0865 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3520 Training loss: 1.3663 0.0874 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3520 Training loss: 1.3664 0.0887 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3520 Training loss: 1.3662 0.0884 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3520 Training loss: 1.3663 0.0858 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3520 Training loss: 1.3664 0.0895 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3520 Training loss: 1.3665 0.0871 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3520 Training loss: 1.3663 0.0846 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3520 Training loss: 1.3662 0.0851 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3520 Training loss: 1.3661 0.0856 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3520 Training loss: 1.3659 0.0853 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3520 Training loss: 1.3661 0.0927 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3520 Training loss: 1.3661 0.0882 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3520 Training loss: 1.3660 0.0880 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3520 Training loss: 1.3660 0.0841 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3520 Training loss: 1.3659 0.0870 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3520 Training loss: 1.3659 0.0853 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3520 Training loss: 1.3659 0.0894 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3520 Training loss: 1.3658 0.0866 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3520 Training loss: 1.3654 0.0862 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3520 Training loss: 1.3653 0.0886 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3520 Training loss: 1.3653 0.0859 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3520 Training loss: 1.3653 0.0937 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3520 Training loss: 1.3654 0.0874 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3520 Training loss: 1.3654 0.0877 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3520 Training loss: 1.3656 0.0878 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3520 Training loss: 1.3656 0.0873 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3520 Training loss: 1.3658 0.0854 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3520 Training loss: 1.3659 0.0934 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3520 Training loss: 1.3658 0.0853 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3520 Training loss: 1.3659 0.0866 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3520 Training loss: 1.3657 0.0883 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3520 Training loss: 1.3659 0.0879 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3520 Training loss: 1.3660 0.0863 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3520 Training loss: 1.3660 0.0856 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3520 Training loss: 1.3658 0.0863 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3520 Training loss: 1.3656 0.0924 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3520 Training loss: 1.3655 0.0868 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3520 Training loss: 1.3656 0.0933 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3520 Training loss: 1.3656 0.0844 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3520 Training loss: 1.3655 0.0873 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3520 Training loss: 1.3655 0.0859 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3520 Training loss: 1.3655 0.0877 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3520 Training loss: 1.3658 0.0843 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3520 Training loss: 1.3656 0.0845 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3520 Training loss: 1.3655 0.0888 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3520 Training loss: 1.3654 0.0872 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3520 Training loss: 1.3652 0.0872 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3520 Training loss: 1.3652 0.0861 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3520 Training loss: 1.3652 0.0880 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3520 Training loss: 1.3650 0.0895 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3520 Training loss: 1.3651 0.0868 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3520 Training loss: 1.3652 0.0869 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3520 Training loss: 1.3654 0.0884 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3520 Training loss: 1.3652 0.0884 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3520 Training loss: 1.3652 0.0858 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3520 Training loss: 1.3653 0.0877 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3520 Training loss: 1.3651 0.0860 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3520 Training loss: 1.3648 0.0863 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3520 Training loss: 1.3647 0.0877 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3520 Training loss: 1.3644 0.0860 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3520 Training loss: 1.3645 0.0969 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3520 Training loss: 1.3646 0.0875 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3520 Training loss: 1.3646 0.0874 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3520 Training loss: 1.3647 0.0778 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3520 Training loss: 1.3647 0.0964 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3520 Training loss: 1.3646 0.0870 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3520 Training loss: 1.3647 0.0871 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3520 Training loss: 1.3647 0.0850 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3520 Training loss: 1.3649 0.0872 sec/batch\n",
      "Epoch 20/20  Iteration 3345/3520 Training loss: 1.4091 0.0936 sec/batch\n",
      "Epoch 20/20  Iteration 3346/3520 Training loss: 1.3684 0.0921 sec/batch\n",
      "Epoch 20/20  Iteration 3347/3520 Training loss: 1.3640 0.0879 sec/batch\n",
      "Epoch 20/20  Iteration 3348/3520 Training loss: 1.3626 0.0877 sec/batch\n",
      "Epoch 20/20  Iteration 3349/3520 Training loss: 1.3624 0.0854 sec/batch\n",
      "Epoch 20/20  Iteration 3350/3520 Training loss: 1.3622 0.0845 sec/batch\n",
      "Epoch 20/20  Iteration 3351/3520 Training loss: 1.3601 0.0841 sec/batch\n",
      "Epoch 20/20  Iteration 3352/3520 Training loss: 1.3616 0.0938 sec/batch\n",
      "Epoch 20/20  Iteration 3353/3520 Training loss: 1.3591 0.0861 sec/batch\n",
      "Epoch 20/20  Iteration 3354/3520 Training loss: 1.3580 0.0842 sec/batch\n",
      "Epoch 20/20  Iteration 3355/3520 Training loss: 1.3581 0.0883 sec/batch\n",
      "Epoch 20/20  Iteration 3356/3520 Training loss: 1.3583 0.0929 sec/batch\n",
      "Epoch 20/20  Iteration 3357/3520 Training loss: 1.3569 0.0865 sec/batch\n",
      "Epoch 20/20  Iteration 3358/3520 Training loss: 1.3585 0.0850 sec/batch\n",
      "Epoch 20/20  Iteration 3359/3520 Training loss: 1.3579 0.0875 sec/batch\n",
      "Epoch 20/20  Iteration 3360/3520 Training loss: 1.3572 0.0867 sec/batch\n",
      "Epoch 20/20  Iteration 3361/3520 Training loss: 1.3569 0.0858 sec/batch\n",
      "Epoch 20/20  Iteration 3362/3520 Training loss: 1.3576 0.0925 sec/batch\n",
      "Epoch 20/20  Iteration 3363/3520 Training loss: 1.3567 0.0937 sec/batch\n",
      "Epoch 20/20  Iteration 3364/3520 Training loss: 1.3578 0.0924 sec/batch\n",
      "Epoch 20/20  Iteration 3365/3520 Training loss: 1.3584 0.0879 sec/batch\n",
      "Epoch 20/20  Iteration 3366/3520 Training loss: 1.3577 0.0841 sec/batch\n",
      "Epoch 20/20  Iteration 3367/3520 Training loss: 1.3586 0.0852 sec/batch\n",
      "Epoch 20/20  Iteration 3368/3520 Training loss: 1.3596 0.0809 sec/batch\n",
      "Epoch 20/20  Iteration 3369/3520 Training loss: 1.3603 0.0852 sec/batch\n",
      "Epoch 20/20  Iteration 3370/3520 Training loss: 1.3594 0.0941 sec/batch\n",
      "Epoch 20/20  Iteration 3371/3520 Training loss: 1.3582 0.0956 sec/batch\n",
      "Epoch 20/20  Iteration 3372/3520 Training loss: 1.3581 0.0780 sec/batch\n",
      "Epoch 20/20  Iteration 3373/3520 Training loss: 1.3574 0.0971 sec/batch\n",
      "Epoch 20/20  Iteration 3374/3520 Training loss: 1.3580 0.0964 sec/batch\n",
      "Epoch 20/20  Iteration 3375/3520 Training loss: 1.3575 0.0896 sec/batch\n",
      "Epoch 20/20  Iteration 3376/3520 Training loss: 1.3576 0.0960 sec/batch\n",
      "Epoch 20/20  Iteration 3377/3520 Training loss: 1.3586 0.0882 sec/batch\n",
      "Epoch 20/20  Iteration 3378/3520 Training loss: 1.3586 0.0883 sec/batch\n",
      "Epoch 20/20  Iteration 3379/3520 Training loss: 1.3586 0.0860 sec/batch\n",
      "Epoch 20/20  Iteration 3380/3520 Training loss: 1.3591 0.0873 sec/batch\n",
      "Epoch 20/20  Iteration 3381/3520 Training loss: 1.3580 0.0938 sec/batch\n",
      "Epoch 20/20  Iteration 3382/3520 Training loss: 1.3588 0.0866 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3520 Training loss: 1.3597 0.0853 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3520 Training loss: 1.3598 0.0933 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3520 Training loss: 1.3597 0.0870 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3520 Training loss: 1.3589 0.0855 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3520 Training loss: 1.3590 0.0876 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3520 Training loss: 1.3584 0.0881 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3520 Training loss: 1.3581 0.0837 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3520 Training loss: 1.3578 0.0949 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3520 Training loss: 1.3578 0.0851 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3520 Training loss: 1.3580 0.0875 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3520 Training loss: 1.3580 0.0949 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3520 Training loss: 1.3582 0.0870 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3520 Training loss: 1.3580 0.0845 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3520 Training loss: 1.3578 0.0869 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3520 Training loss: 1.3580 0.0952 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3520 Training loss: 1.3579 0.0858 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3520 Training loss: 1.3576 0.0845 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3520 Training loss: 1.3576 0.0862 sec/batch\n",
      "Validation loss: 1.28069 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 3401/3520 Training loss: 1.3595 0.0865 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3520 Training loss: 1.3596 0.0868 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3520 Training loss: 1.3591 0.0950 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3520 Training loss: 1.3594 0.0882 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3520 Training loss: 1.3592 0.0858 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3520 Training loss: 1.3593 0.0873 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3520 Training loss: 1.3595 0.0842 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3520 Training loss: 1.3595 0.0849 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3520 Training loss: 1.3590 0.0884 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3520 Training loss: 1.3592 0.0920 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3520 Training loss: 1.3591 0.0873 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3520 Training loss: 1.3586 0.0877 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3520 Training loss: 1.3584 0.0887 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3520 Training loss: 1.3585 0.0878 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3520 Training loss: 1.3584 0.0880 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3520 Training loss: 1.3582 0.0856 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3520 Training loss: 1.3579 0.0857 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3520 Training loss: 1.3575 0.0940 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3520 Training loss: 1.3573 0.0847 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3520 Training loss: 1.3575 0.0882 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3520 Training loss: 1.3577 0.0879 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3520 Training loss: 1.3581 0.0853 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3520 Training loss: 1.3579 0.0850 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3520 Training loss: 1.3577 0.0847 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3520 Training loss: 1.3574 0.0857 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3520 Training loss: 1.3577 0.0870 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3520 Training loss: 1.3576 0.0858 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3520 Training loss: 1.3578 0.0854 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3520 Training loss: 1.3576 0.0869 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3520 Training loss: 1.3572 0.0857 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3520 Training loss: 1.3577 0.0957 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3520 Training loss: 1.3576 0.0869 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3520 Training loss: 1.3578 0.0953 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3520 Training loss: 1.3577 0.0860 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3520 Training loss: 1.3575 0.0881 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3520 Training loss: 1.3578 0.0853 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3520 Training loss: 1.3576 0.0924 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3520 Training loss: 1.3573 0.0873 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3520 Training loss: 1.3572 0.0860 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3520 Training loss: 1.3570 0.0839 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3520 Training loss: 1.3572 0.0844 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3520 Training loss: 1.3573 0.0871 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3520 Training loss: 1.3574 0.0864 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3520 Training loss: 1.3572 0.0851 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3520 Training loss: 1.3571 0.0873 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3520 Training loss: 1.3572 0.0950 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3520 Training loss: 1.3569 0.0887 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3520 Training loss: 1.3563 0.0918 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3520 Training loss: 1.3561 0.0909 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3520 Training loss: 1.3562 0.0881 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3520 Training loss: 1.3563 0.0831 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3520 Training loss: 1.3561 0.0850 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3520 Training loss: 1.3561 0.0987 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3520 Training loss: 1.3559 0.0837 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3520 Training loss: 1.3560 0.0877 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3520 Training loss: 1.3562 0.0956 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3520 Training loss: 1.3564 0.0851 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3520 Training loss: 1.3563 0.0941 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3520 Training loss: 1.3561 0.0851 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3520 Training loss: 1.3560 0.0839 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3520 Training loss: 1.3559 0.0854 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3520 Training loss: 1.3562 0.0874 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3520 Training loss: 1.3563 0.0875 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3520 Training loss: 1.3562 0.0841 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3520 Training loss: 1.3562 0.0937 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3520 Training loss: 1.3563 0.0871 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3520 Training loss: 1.3562 0.0855 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3520 Training loss: 1.3562 0.0839 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3520 Training loss: 1.3561 0.0923 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3520 Training loss: 1.3558 0.0884 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3520 Training loss: 1.3557 0.0862 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3520 Training loss: 1.3557 0.0861 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3520 Training loss: 1.3557 0.0853 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3520 Training loss: 1.3559 0.0846 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3520 Training loss: 1.3559 0.0933 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3520 Training loss: 1.3562 0.0908 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3520 Training loss: 1.3561 0.0952 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3520 Training loss: 1.3563 0.0875 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3520 Training loss: 1.3564 0.0858 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3520 Training loss: 1.3564 0.0859 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3520 Training loss: 1.3564 0.0875 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3520 Training loss: 1.3563 0.0863 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3520 Training loss: 1.3565 0.0827 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3520 Training loss: 1.3565 0.0858 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3520 Training loss: 1.3565 0.0872 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3520 Training loss: 1.3563 0.0848 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3520 Training loss: 1.3561 0.0919 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3520 Training loss: 1.3561 0.0868 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3520 Training loss: 1.3562 0.0873 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3520 Training loss: 1.3562 0.0870 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3520 Training loss: 1.3561 0.0873 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3520 Training loss: 1.3561 0.0859 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3520 Training loss: 1.3561 0.0945 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3520 Training loss: 1.3562 0.0884 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3520 Training loss: 1.3561 0.0861 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3520 Training loss: 1.3560 0.0861 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3520 Training loss: 1.3560 0.0850 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3520 Training loss: 1.3558 0.0860 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3520 Training loss: 1.3557 0.0848 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3520 Training loss: 1.3556 0.0872 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3520 Training loss: 1.3554 0.0863 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3520 Training loss: 1.3556 0.0867 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3520 Training loss: 1.3557 0.0942 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3520 Training loss: 1.3559 0.0867 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3520 Training loss: 1.3556 0.0876 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3520 Training loss: 1.3556 0.0947 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3520 Training loss: 1.3557 0.0933 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3520 Training loss: 1.3555 0.0826 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3520 Training loss: 1.3552 0.0940 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3520 Training loss: 1.3551 0.0844 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3520 Training loss: 1.3549 0.0855 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3520 Training loss: 1.3549 0.0868 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3520 Training loss: 1.3551 0.0907 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3520 Training loss: 1.3550 0.0849 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3520 Training loss: 1.3550 0.0879 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3520 Training loss: 1.3550 0.0844 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3520 Training loss: 1.3550 0.0855 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3520 Training loss: 1.3550 0.0878 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3520 Training loss: 1.3550 0.0862 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3520 Training loss: 1.3552 0.0851 sec/batch\n",
      "Validation loss: 1.27512 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 1/3520 Training loss: 4.3425 1.4961 sec/batch\n",
      "Epoch 1/20  Iteration 2/3520 Training loss: 4.3058 0.2169 sec/batch\n",
      "Epoch 1/20  Iteration 3/3520 Training loss: 4.1396 0.1988 sec/batch\n",
      "Epoch 1/20  Iteration 4/3520 Training loss: 4.0162 0.1884 sec/batch\n",
      "Epoch 1/20  Iteration 5/3520 Training loss: 3.9123 0.1683 sec/batch\n",
      "Epoch 1/20  Iteration 6/3520 Training loss: 3.8405 0.1720 sec/batch\n",
      "Epoch 1/20  Iteration 7/3520 Training loss: 3.7774 0.1721 sec/batch\n",
      "Epoch 1/20  Iteration 8/3520 Training loss: 3.7251 0.1689 sec/batch\n",
      "Epoch 1/20  Iteration 9/3520 Training loss: 3.6843 0.1765 sec/batch\n",
      "Epoch 1/20  Iteration 10/3520 Training loss: 3.6507 0.1660 sec/batch\n",
      "Epoch 1/20  Iteration 11/3520 Training loss: 3.6180 0.1675 sec/batch\n",
      "Epoch 1/20  Iteration 12/3520 Training loss: 3.5889 0.1722 sec/batch\n",
      "Epoch 1/20  Iteration 13/3520 Training loss: 3.5654 0.1634 sec/batch\n",
      "Epoch 1/20  Iteration 14/3520 Training loss: 3.5419 0.1753 sec/batch\n",
      "Epoch 1/20  Iteration 15/3520 Training loss: 3.5208 0.1725 sec/batch\n",
      "Epoch 1/20  Iteration 16/3520 Training loss: 3.5020 0.1647 sec/batch\n",
      "Epoch 1/20  Iteration 17/3520 Training loss: 3.4854 0.1664 sec/batch\n",
      "Epoch 1/20  Iteration 18/3520 Training loss: 3.4707 0.1653 sec/batch\n",
      "Epoch 1/20  Iteration 19/3520 Training loss: 3.4557 0.1683 sec/batch\n",
      "Epoch 1/20  Iteration 20/3520 Training loss: 3.4432 0.1670 sec/batch\n",
      "Epoch 1/20  Iteration 21/3520 Training loss: 3.4316 0.1706 sec/batch\n",
      "Epoch 1/20  Iteration 22/3520 Training loss: 3.4215 0.1644 sec/batch\n",
      "Epoch 1/20  Iteration 23/3520 Training loss: 3.4114 0.1637 sec/batch\n",
      "Epoch 1/20  Iteration 24/3520 Training loss: 3.4023 0.1683 sec/batch\n",
      "Epoch 1/20  Iteration 25/3520 Training loss: 3.3936 0.1718 sec/batch\n",
      "Epoch 1/20  Iteration 26/3520 Training loss: 3.3849 0.1680 sec/batch\n",
      "Epoch 1/20  Iteration 27/3520 Training loss: 3.3771 0.1637 sec/batch\n",
      "Epoch 1/20  Iteration 28/3520 Training loss: 3.3693 0.1749 sec/batch\n",
      "Epoch 1/20  Iteration 29/3520 Training loss: 3.3624 0.1732 sec/batch\n",
      "Epoch 1/20  Iteration 30/3520 Training loss: 3.3557 0.1732 sec/batch\n",
      "Epoch 1/20  Iteration 31/3520 Training loss: 3.3491 0.1729 sec/batch\n",
      "Epoch 1/20  Iteration 32/3520 Training loss: 3.3428 0.1723 sec/batch\n",
      "Epoch 1/20  Iteration 33/3520 Training loss: 3.3374 0.1681 sec/batch\n",
      "Epoch 1/20  Iteration 34/3520 Training loss: 3.3325 0.1727 sec/batch\n",
      "Epoch 1/20  Iteration 35/3520 Training loss: 3.3270 0.1673 sec/batch\n",
      "Epoch 1/20  Iteration 36/3520 Training loss: 3.3221 0.1721 sec/batch\n",
      "Epoch 1/20  Iteration 37/3520 Training loss: 3.3177 0.1721 sec/batch\n",
      "Epoch 1/20  Iteration 38/3520 Training loss: 3.3133 0.1724 sec/batch\n",
      "Epoch 1/20  Iteration 39/3520 Training loss: 3.3094 0.1689 sec/batch\n",
      "Epoch 1/20  Iteration 40/3520 Training loss: 3.3053 0.1671 sec/batch\n",
      "Epoch 1/20  Iteration 41/3520 Training loss: 3.3018 0.1683 sec/batch\n",
      "Epoch 1/20  Iteration 42/3520 Training loss: 3.2981 0.1715 sec/batch\n",
      "Epoch 1/20  Iteration 43/3520 Training loss: 3.2946 0.1759 sec/batch\n",
      "Epoch 1/20  Iteration 44/3520 Training loss: 3.2908 0.1722 sec/batch\n",
      "Epoch 1/20  Iteration 45/3520 Training loss: 3.2875 0.1670 sec/batch\n",
      "Epoch 1/20  Iteration 46/3520 Training loss: 3.2842 0.1660 sec/batch\n",
      "Epoch 1/20  Iteration 47/3520 Training loss: 3.2811 0.1685 sec/batch\n",
      "Epoch 1/20  Iteration 48/3520 Training loss: 3.2782 0.1744 sec/batch\n",
      "Epoch 1/20  Iteration 49/3520 Training loss: 3.2755 0.1676 sec/batch\n",
      "Epoch 1/20  Iteration 50/3520 Training loss: 3.2731 0.1656 sec/batch\n",
      "Epoch 1/20  Iteration 51/3520 Training loss: 3.2707 0.1676 sec/batch\n",
      "Epoch 1/20  Iteration 52/3520 Training loss: 3.2685 0.1723 sec/batch\n",
      "Epoch 1/20  Iteration 53/3520 Training loss: 3.2660 0.1662 sec/batch\n",
      "Epoch 1/20  Iteration 54/3520 Training loss: 3.2639 0.1729 sec/batch\n",
      "Epoch 1/20  Iteration 55/3520 Training loss: 3.2615 0.1654 sec/batch\n",
      "Epoch 1/20  Iteration 56/3520 Training loss: 3.2594 0.1706 sec/batch\n",
      "Epoch 1/20  Iteration 57/3520 Training loss: 3.2571 0.1729 sec/batch\n",
      "Epoch 1/20  Iteration 58/3520 Training loss: 3.2548 0.1725 sec/batch\n",
      "Epoch 1/20  Iteration 59/3520 Training loss: 3.2527 0.1725 sec/batch\n",
      "Epoch 1/20  Iteration 60/3520 Training loss: 3.2506 0.1727 sec/batch\n",
      "Epoch 1/20  Iteration 61/3520 Training loss: 3.2485 0.1681 sec/batch\n",
      "Epoch 1/20  Iteration 62/3520 Training loss: 3.2466 0.1658 sec/batch\n",
      "Epoch 1/20  Iteration 63/3520 Training loss: 3.2448 0.1731 sec/batch\n",
      "Epoch 1/20  Iteration 64/3520 Training loss: 3.2425 0.1733 sec/batch\n",
      "Epoch 1/20  Iteration 65/3520 Training loss: 3.2408 0.1671 sec/batch\n",
      "Epoch 1/20  Iteration 66/3520 Training loss: 3.2387 0.1730 sec/batch\n",
      "Epoch 1/20  Iteration 67/3520 Training loss: 3.2371 0.1729 sec/batch\n",
      "Epoch 1/20  Iteration 68/3520 Training loss: 3.2353 0.1727 sec/batch\n",
      "Epoch 1/20  Iteration 69/3520 Training loss: 3.2335 0.1721 sec/batch\n",
      "Epoch 1/20  Iteration 70/3520 Training loss: 3.2319 0.1816 sec/batch\n",
      "Epoch 1/20  Iteration 71/3520 Training loss: 3.2303 0.1679 sec/batch\n",
      "Epoch 1/20  Iteration 72/3520 Training loss: 3.2288 0.1709 sec/batch\n",
      "Epoch 1/20  Iteration 73/3520 Training loss: 3.2273 0.1841 sec/batch\n",
      "Epoch 1/20  Iteration 74/3520 Training loss: 3.2255 0.1720 sec/batch\n",
      "Epoch 1/20  Iteration 75/3520 Training loss: 3.2238 0.1724 sec/batch\n",
      "Epoch 1/20  Iteration 76/3520 Training loss: 3.2220 0.1726 sec/batch\n",
      "Epoch 1/20  Iteration 77/3520 Training loss: 3.2203 0.1685 sec/batch\n",
      "Epoch 1/20  Iteration 78/3520 Training loss: 3.2185 0.1688 sec/batch\n",
      "Epoch 1/20  Iteration 79/3520 Training loss: 3.2169 0.1672 sec/batch\n",
      "Epoch 1/20  Iteration 80/3520 Training loss: 3.2151 0.1747 sec/batch\n",
      "Epoch 1/20  Iteration 81/3520 Training loss: 3.2135 0.1729 sec/batch\n",
      "Epoch 1/20  Iteration 82/3520 Training loss: 3.2116 0.1667 sec/batch\n",
      "Epoch 1/20  Iteration 83/3520 Training loss: 3.2100 0.1662 sec/batch\n",
      "Epoch 1/20  Iteration 84/3520 Training loss: 3.2086 0.1723 sec/batch\n",
      "Epoch 1/20  Iteration 85/3520 Training loss: 3.2069 0.1684 sec/batch\n",
      "Epoch 1/20  Iteration 86/3520 Training loss: 3.2053 0.1724 sec/batch\n",
      "Epoch 1/20  Iteration 87/3520 Training loss: 3.2037 0.1723 sec/batch\n",
      "Epoch 1/20  Iteration 88/3520 Training loss: 3.2020 0.1682 sec/batch\n",
      "Epoch 1/20  Iteration 89/3520 Training loss: 3.2003 0.1683 sec/batch\n",
      "Epoch 1/20  Iteration 90/3520 Training loss: 3.1987 0.1682 sec/batch\n",
      "Epoch 1/20  Iteration 91/3520 Training loss: 3.1971 0.1732 sec/batch\n",
      "Epoch 1/20  Iteration 92/3520 Training loss: 3.1954 0.1731 sec/batch\n",
      "Epoch 1/20  Iteration 93/3520 Training loss: 3.1936 0.1755 sec/batch\n",
      "Epoch 1/20  Iteration 94/3520 Training loss: 3.1919 0.1729 sec/batch\n",
      "Epoch 1/20  Iteration 95/3520 Training loss: 3.1899 0.1719 sec/batch\n",
      "Epoch 1/20  Iteration 96/3520 Training loss: 3.1882 0.1765 sec/batch\n",
      "Epoch 1/20  Iteration 97/3520 Training loss: 3.1867 0.1677 sec/batch\n",
      "Epoch 1/20  Iteration 98/3520 Training loss: 3.1849 0.1653 sec/batch\n",
      "Epoch 1/20  Iteration 99/3520 Training loss: 3.1831 0.1673 sec/batch\n",
      "Epoch 1/20  Iteration 100/3520 Training loss: 3.1809 0.1685 sec/batch\n",
      "Epoch 1/20  Iteration 101/3520 Training loss: 3.1788 0.1733 sec/batch\n",
      "Epoch 1/20  Iteration 102/3520 Training loss: 3.1768 0.1727 sec/batch\n",
      "Epoch 1/20  Iteration 103/3520 Training loss: 3.1748 0.1672 sec/batch\n",
      "Epoch 1/20  Iteration 104/3520 Training loss: 3.1726 0.1675 sec/batch\n",
      "Epoch 1/20  Iteration 105/3520 Training loss: 3.1707 0.1681 sec/batch\n",
      "Epoch 1/20  Iteration 106/3520 Training loss: 3.1687 0.1757 sec/batch\n",
      "Epoch 1/20  Iteration 107/3520 Training loss: 3.1661 0.1660 sec/batch\n",
      "Epoch 1/20  Iteration 108/3520 Training loss: 3.1636 0.1728 sec/batch\n",
      "Epoch 1/20  Iteration 109/3520 Training loss: 3.1613 0.1726 sec/batch\n",
      "Epoch 1/20  Iteration 110/3520 Training loss: 3.1588 0.1734 sec/batch\n",
      "Epoch 1/20  Iteration 111/3520 Training loss: 3.1562 0.1690 sec/batch\n",
      "Epoch 1/20  Iteration 112/3520 Training loss: 3.1535 0.1723 sec/batch\n",
      "Epoch 1/20  Iteration 113/3520 Training loss: 3.1510 0.1730 sec/batch\n",
      "Epoch 1/20  Iteration 114/3520 Training loss: 3.1481 0.1727 sec/batch\n",
      "Epoch 1/20  Iteration 115/3520 Training loss: 3.1453 0.1729 sec/batch\n",
      "Epoch 1/20  Iteration 116/3520 Training loss: 3.1426 0.1732 sec/batch\n",
      "Epoch 1/20  Iteration 117/3520 Training loss: 3.1398 0.1731 sec/batch\n",
      "Epoch 1/20  Iteration 118/3520 Training loss: 3.1371 0.1735 sec/batch\n",
      "Epoch 1/20  Iteration 119/3520 Training loss: 3.1343 0.1730 sec/batch\n",
      "Epoch 1/20  Iteration 120/3520 Training loss: 3.1314 0.1755 sec/batch\n",
      "Epoch 1/20  Iteration 121/3520 Training loss: 3.1287 0.1644 sec/batch\n",
      "Epoch 1/20  Iteration 122/3520 Training loss: 3.1260 0.1693 sec/batch\n",
      "Epoch 1/20  Iteration 123/3520 Training loss: 3.1228 0.1727 sec/batch\n",
      "Epoch 1/20  Iteration 124/3520 Training loss: 3.1199 0.1625 sec/batch\n",
      "Epoch 1/20  Iteration 125/3520 Training loss: 3.1170 0.1614 sec/batch\n",
      "Epoch 1/20  Iteration 126/3520 Training loss: 3.1140 0.1730 sec/batch\n",
      "Epoch 1/20  Iteration 127/3520 Training loss: 3.1109 0.1725 sec/batch\n",
      "Epoch 1/20  Iteration 128/3520 Training loss: 3.1078 0.1666 sec/batch\n",
      "Epoch 1/20  Iteration 129/3520 Training loss: 3.1044 0.1667 sec/batch\n",
      "Epoch 1/20  Iteration 130/3520 Training loss: 3.1013 0.1680 sec/batch\n",
      "Epoch 1/20  Iteration 131/3520 Training loss: 3.0983 0.1636 sec/batch\n",
      "Epoch 1/20  Iteration 132/3520 Training loss: 3.0951 0.1652 sec/batch\n",
      "Epoch 1/20  Iteration 133/3520 Training loss: 3.0921 0.1724 sec/batch\n",
      "Epoch 1/20  Iteration 134/3520 Training loss: 3.0889 0.1686 sec/batch\n",
      "Epoch 1/20  Iteration 135/3520 Training loss: 3.0860 0.1727 sec/batch\n",
      "Epoch 1/20  Iteration 136/3520 Training loss: 3.0830 0.1727 sec/batch\n",
      "Epoch 1/20  Iteration 137/3520 Training loss: 3.0801 0.1687 sec/batch\n",
      "Epoch 1/20  Iteration 138/3520 Training loss: 3.0769 0.1668 sec/batch\n",
      "Epoch 1/20  Iteration 139/3520 Training loss: 3.0738 0.1667 sec/batch\n",
      "Epoch 1/20  Iteration 140/3520 Training loss: 3.0708 0.1711 sec/batch\n",
      "Epoch 1/20  Iteration 141/3520 Training loss: 3.0678 0.1727 sec/batch\n",
      "Epoch 1/20  Iteration 142/3520 Training loss: 3.0647 0.1665 sec/batch\n",
      "Epoch 1/20  Iteration 143/3520 Training loss: 3.0616 0.1667 sec/batch\n",
      "Epoch 1/20  Iteration 144/3520 Training loss: 3.0586 0.1683 sec/batch\n",
      "Epoch 1/20  Iteration 145/3520 Training loss: 3.0555 0.1687 sec/batch\n",
      "Epoch 1/20  Iteration 146/3520 Training loss: 3.0525 0.1726 sec/batch\n",
      "Epoch 1/20  Iteration 147/3520 Training loss: 3.0494 0.1681 sec/batch\n",
      "Epoch 1/20  Iteration 148/3520 Training loss: 3.0465 0.1676 sec/batch\n",
      "Epoch 1/20  Iteration 149/3520 Training loss: 3.0436 0.1752 sec/batch\n",
      "Epoch 1/20  Iteration 150/3520 Training loss: 3.0406 0.1712 sec/batch\n",
      "Epoch 1/20  Iteration 151/3520 Training loss: 3.0377 0.1708 sec/batch\n",
      "Epoch 1/20  Iteration 152/3520 Training loss: 3.0347 0.1683 sec/batch\n",
      "Epoch 1/20  Iteration 153/3520 Training loss: 3.0318 0.1727 sec/batch\n",
      "Epoch 1/20  Iteration 154/3520 Training loss: 3.0289 0.1676 sec/batch\n",
      "Epoch 1/20  Iteration 155/3520 Training loss: 3.0259 0.1690 sec/batch\n",
      "Epoch 1/20  Iteration 156/3520 Training loss: 3.0228 0.1710 sec/batch\n",
      "Epoch 1/20  Iteration 157/3520 Training loss: 3.0198 0.1666 sec/batch\n",
      "Epoch 1/20  Iteration 158/3520 Training loss: 3.0168 0.1726 sec/batch\n",
      "Epoch 1/20  Iteration 159/3520 Training loss: 3.0137 0.1713 sec/batch\n",
      "Epoch 1/20  Iteration 160/3520 Training loss: 3.0107 0.1721 sec/batch\n",
      "Epoch 1/20  Iteration 161/3520 Training loss: 3.0079 0.1677 sec/batch\n",
      "Epoch 1/20  Iteration 162/3520 Training loss: 3.0048 0.1723 sec/batch\n",
      "Epoch 1/20  Iteration 163/3520 Training loss: 3.0018 0.1722 sec/batch\n",
      "Epoch 1/20  Iteration 164/3520 Training loss: 2.9987 0.1723 sec/batch\n",
      "Epoch 1/20  Iteration 165/3520 Training loss: 2.9958 0.1724 sec/batch\n",
      "Epoch 1/20  Iteration 166/3520 Training loss: 2.9928 0.1675 sec/batch\n",
      "Epoch 1/20  Iteration 167/3520 Training loss: 2.9899 0.1723 sec/batch\n",
      "Epoch 1/20  Iteration 168/3520 Training loss: 2.9872 0.1726 sec/batch\n",
      "Epoch 1/20  Iteration 169/3520 Training loss: 2.9844 0.1688 sec/batch\n",
      "Epoch 1/20  Iteration 170/3520 Training loss: 2.9815 0.1728 sec/batch\n",
      "Epoch 1/20  Iteration 171/3520 Training loss: 2.9786 0.1705 sec/batch\n",
      "Epoch 1/20  Iteration 172/3520 Training loss: 2.9758 0.1661 sec/batch\n",
      "Epoch 1/20  Iteration 173/3520 Training loss: 2.9729 0.1732 sec/batch\n",
      "Epoch 1/20  Iteration 174/3520 Training loss: 2.9702 0.1716 sec/batch\n",
      "Epoch 1/20  Iteration 175/3520 Training loss: 2.9673 0.1686 sec/batch\n",
      "Epoch 1/20  Iteration 176/3520 Training loss: 2.9646 0.1735 sec/batch\n",
      "Epoch 2/20  Iteration 177/3520 Training loss: 2.5297 0.1671 sec/batch\n",
      "Epoch 2/20  Iteration 178/3520 Training loss: 2.4896 0.1653 sec/batch\n",
      "Epoch 2/20  Iteration 179/3520 Training loss: 2.4899 0.1685 sec/batch\n",
      "Epoch 2/20  Iteration 180/3520 Training loss: 2.4881 0.1676 sec/batch\n",
      "Epoch 2/20  Iteration 181/3520 Training loss: 2.4869 0.1677 sec/batch\n",
      "Epoch 2/20  Iteration 182/3520 Training loss: 2.4877 0.1666 sec/batch\n",
      "Epoch 2/20  Iteration 183/3520 Training loss: 2.4848 0.1663 sec/batch\n",
      "Epoch 2/20  Iteration 184/3520 Training loss: 2.4874 0.1679 sec/batch\n",
      "Epoch 2/20  Iteration 185/3520 Training loss: 2.4854 0.1738 sec/batch\n",
      "Epoch 2/20  Iteration 186/3520 Training loss: 2.4858 0.1729 sec/batch\n",
      "Epoch 2/20  Iteration 187/3520 Training loss: 2.4827 0.1673 sec/batch\n",
      "Epoch 2/20  Iteration 188/3520 Training loss: 2.4814 0.1681 sec/batch\n",
      "Epoch 2/20  Iteration 189/3520 Training loss: 2.4793 0.1641 sec/batch\n",
      "Epoch 2/20  Iteration 190/3520 Training loss: 2.4777 0.1689 sec/batch\n",
      "Epoch 2/20  Iteration 191/3520 Training loss: 2.4737 0.1640 sec/batch\n",
      "Epoch 2/20  Iteration 192/3520 Training loss: 2.4715 0.1686 sec/batch\n",
      "Epoch 2/20  Iteration 193/3520 Training loss: 2.4693 0.1658 sec/batch\n",
      "Epoch 2/20  Iteration 194/3520 Training loss: 2.4689 0.1661 sec/batch\n",
      "Epoch 2/20  Iteration 195/3520 Training loss: 2.4651 0.1723 sec/batch\n",
      "Epoch 2/20  Iteration 196/3520 Training loss: 2.4639 0.1661 sec/batch\n",
      "Epoch 2/20  Iteration 197/3520 Training loss: 2.4620 0.1688 sec/batch\n",
      "Epoch 2/20  Iteration 198/3520 Training loss: 2.4611 0.1673 sec/batch\n",
      "Epoch 2/20  Iteration 199/3520 Training loss: 2.4607 0.1681 sec/batch\n",
      "Epoch 2/20  Iteration 200/3520 Training loss: 2.4596 0.1728 sec/batch\n",
      "Validation loss: 2.30957 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 201/3520 Training loss: 2.4586 0.1937 sec/batch\n",
      "Epoch 2/20  Iteration 202/3520 Training loss: 2.4558 0.1675 sec/batch\n",
      "Epoch 2/20  Iteration 203/3520 Training loss: 2.4536 0.1658 sec/batch\n",
      "Epoch 2/20  Iteration 204/3520 Training loss: 2.4518 0.1690 sec/batch\n",
      "Epoch 2/20  Iteration 205/3520 Training loss: 2.4498 0.1718 sec/batch\n",
      "Epoch 2/20  Iteration 206/3520 Training loss: 2.4481 0.1658 sec/batch\n",
      "Epoch 2/20  Iteration 207/3520 Training loss: 2.4462 0.1724 sec/batch\n",
      "Epoch 2/20  Iteration 208/3520 Training loss: 2.4443 0.1722 sec/batch\n",
      "Epoch 2/20  Iteration 209/3520 Training loss: 2.4425 0.1683 sec/batch\n",
      "Epoch 2/20  Iteration 210/3520 Training loss: 2.4425 0.1696 sec/batch\n",
      "Epoch 2/20  Iteration 211/3520 Training loss: 2.4407 0.1673 sec/batch\n",
      "Epoch 2/20  Iteration 212/3520 Training loss: 2.4392 0.1641 sec/batch\n",
      "Epoch 2/20  Iteration 213/3520 Training loss: 2.4379 0.1659 sec/batch\n",
      "Epoch 2/20  Iteration 214/3520 Training loss: 2.4369 0.1721 sec/batch\n",
      "Epoch 2/20  Iteration 215/3520 Training loss: 2.4361 0.1832 sec/batch\n",
      "Epoch 2/20  Iteration 216/3520 Training loss: 2.4348 0.1714 sec/batch\n",
      "Epoch 2/20  Iteration 217/3520 Training loss: 2.4334 0.1749 sec/batch\n",
      "Epoch 2/20  Iteration 218/3520 Training loss: 2.4312 0.1699 sec/batch\n",
      "Epoch 2/20  Iteration 219/3520 Training loss: 2.4299 0.1669 sec/batch\n",
      "Epoch 2/20  Iteration 220/3520 Training loss: 2.4279 0.1718 sec/batch\n",
      "Epoch 2/20  Iteration 221/3520 Training loss: 2.4266 0.1724 sec/batch\n",
      "Epoch 2/20  Iteration 222/3520 Training loss: 2.4244 0.1692 sec/batch\n",
      "Epoch 2/20  Iteration 223/3520 Training loss: 2.4226 0.1674 sec/batch\n",
      "Epoch 2/20  Iteration 224/3520 Training loss: 2.4211 0.1729 sec/batch\n",
      "Epoch 2/20  Iteration 225/3520 Training loss: 2.4199 0.1661 sec/batch\n",
      "Epoch 2/20  Iteration 226/3520 Training loss: 2.4190 0.1690 sec/batch\n",
      "Epoch 2/20  Iteration 227/3520 Training loss: 2.4179 0.1679 sec/batch\n",
      "Epoch 2/20  Iteration 228/3520 Training loss: 2.4169 0.1770 sec/batch\n",
      "Epoch 2/20  Iteration 229/3520 Training loss: 2.4156 0.1644 sec/batch\n",
      "Epoch 2/20  Iteration 230/3520 Training loss: 2.4149 0.1733 sec/batch\n",
      "Epoch 2/20  Iteration 231/3520 Training loss: 2.4132 0.1725 sec/batch\n",
      "Epoch 2/20  Iteration 232/3520 Training loss: 2.4116 0.1685 sec/batch\n",
      "Epoch 2/20  Iteration 233/3520 Training loss: 2.4097 0.1724 sec/batch\n",
      "Epoch 2/20  Iteration 234/3520 Training loss: 2.4082 0.1684 sec/batch\n",
      "Epoch 2/20  Iteration 235/3520 Training loss: 2.4068 0.1727 sec/batch\n",
      "Epoch 2/20  Iteration 236/3520 Training loss: 2.4054 0.1669 sec/batch\n",
      "Epoch 2/20  Iteration 237/3520 Training loss: 2.4041 0.1724 sec/batch\n",
      "Epoch 2/20  Iteration 238/3520 Training loss: 2.4033 0.1723 sec/batch\n",
      "Epoch 2/20  Iteration 239/3520 Training loss: 2.4018 0.1735 sec/batch\n",
      "Epoch 2/20  Iteration 240/3520 Training loss: 2.4002 0.1727 sec/batch\n",
      "Epoch 2/20  Iteration 241/3520 Training loss: 2.3987 0.1663 sec/batch\n",
      "Epoch 2/20  Iteration 242/3520 Training loss: 2.3972 0.1726 sec/batch\n",
      "Epoch 2/20  Iteration 243/3520 Training loss: 2.3959 0.1666 sec/batch\n",
      "Epoch 2/20  Iteration 244/3520 Training loss: 2.3945 0.1693 sec/batch\n",
      "Epoch 2/20  Iteration 245/3520 Training loss: 2.3930 0.1689 sec/batch\n",
      "Epoch 2/20  Iteration 246/3520 Training loss: 2.3920 0.1642 sec/batch\n",
      "Epoch 2/20  Iteration 247/3520 Training loss: 2.3906 0.1654 sec/batch\n",
      "Epoch 2/20  Iteration 248/3520 Training loss: 2.3893 0.1720 sec/batch\n",
      "Epoch 2/20  Iteration 249/3520 Training loss: 2.3881 0.1723 sec/batch\n",
      "Epoch 2/20  Iteration 250/3520 Training loss: 2.3867 0.1654 sec/batch\n",
      "Epoch 2/20  Iteration 251/3520 Training loss: 2.3851 0.1715 sec/batch\n",
      "Epoch 2/20  Iteration 252/3520 Training loss: 2.3838 0.1690 sec/batch\n",
      "Epoch 2/20  Iteration 253/3520 Training loss: 2.3826 0.1732 sec/batch\n",
      "Epoch 2/20  Iteration 254/3520 Training loss: 2.3811 0.1726 sec/batch\n",
      "Epoch 2/20  Iteration 255/3520 Training loss: 2.3797 0.1657 sec/batch\n",
      "Epoch 2/20  Iteration 256/3520 Training loss: 2.3785 0.1724 sec/batch\n",
      "Epoch 2/20  Iteration 257/3520 Training loss: 2.3768 0.1657 sec/batch\n",
      "Epoch 2/20  Iteration 258/3520 Training loss: 2.3758 0.1639 sec/batch\n",
      "Epoch 2/20  Iteration 259/3520 Training loss: 2.3743 0.1731 sec/batch\n",
      "Epoch 2/20  Iteration 260/3520 Training loss: 2.3733 0.1738 sec/batch\n",
      "Epoch 2/20  Iteration 261/3520 Training loss: 2.3720 0.1682 sec/batch\n",
      "Epoch 2/20  Iteration 262/3520 Training loss: 2.3707 0.1641 sec/batch\n",
      "Epoch 2/20  Iteration 263/3520 Training loss: 2.3697 0.1727 sec/batch\n",
      "Epoch 2/20  Iteration 264/3520 Training loss: 2.3686 0.1729 sec/batch\n",
      "Epoch 2/20  Iteration 265/3520 Training loss: 2.3674 0.1704 sec/batch\n",
      "Epoch 2/20  Iteration 266/3520 Training loss: 2.3663 0.1725 sec/batch\n",
      "Epoch 2/20  Iteration 267/3520 Training loss: 2.3652 0.1726 sec/batch\n",
      "Epoch 2/20  Iteration 268/3520 Training loss: 2.3643 0.1674 sec/batch\n",
      "Epoch 2/20  Iteration 269/3520 Training loss: 2.3634 0.1723 sec/batch\n",
      "Epoch 2/20  Iteration 270/3520 Training loss: 2.3622 0.1684 sec/batch\n",
      "Epoch 2/20  Iteration 271/3520 Training loss: 2.3608 0.1719 sec/batch\n",
      "Epoch 2/20  Iteration 272/3520 Training loss: 2.3597 0.1725 sec/batch\n",
      "Epoch 2/20  Iteration 273/3520 Training loss: 2.3589 0.1657 sec/batch\n",
      "Epoch 2/20  Iteration 274/3520 Training loss: 2.3579 0.1694 sec/batch\n",
      "Epoch 2/20  Iteration 275/3520 Training loss: 2.3568 0.1675 sec/batch\n",
      "Epoch 2/20  Iteration 276/3520 Training loss: 2.3554 0.1727 sec/batch\n",
      "Epoch 2/20  Iteration 277/3520 Training loss: 2.3544 0.1612 sec/batch\n",
      "Epoch 2/20  Iteration 278/3520 Training loss: 2.3535 0.1720 sec/batch\n",
      "Epoch 2/20  Iteration 279/3520 Training loss: 2.3524 0.1755 sec/batch\n",
      "Epoch 2/20  Iteration 280/3520 Training loss: 2.3510 0.1723 sec/batch\n",
      "Epoch 2/20  Iteration 281/3520 Training loss: 2.3499 0.1700 sec/batch\n",
      "Epoch 2/20  Iteration 282/3520 Training loss: 2.3489 0.1688 sec/batch\n",
      "Epoch 2/20  Iteration 283/3520 Training loss: 2.3476 0.1702 sec/batch\n",
      "Epoch 2/20  Iteration 284/3520 Training loss: 2.3461 0.1681 sec/batch\n",
      "Epoch 2/20  Iteration 285/3520 Training loss: 2.3450 0.1726 sec/batch\n",
      "Epoch 2/20  Iteration 286/3520 Training loss: 2.3438 0.1731 sec/batch\n",
      "Epoch 2/20  Iteration 287/3520 Training loss: 2.3424 0.1676 sec/batch\n",
      "Epoch 2/20  Iteration 288/3520 Training loss: 2.3413 0.1730 sec/batch\n",
      "Epoch 2/20  Iteration 289/3520 Training loss: 2.3402 0.1764 sec/batch\n",
      "Epoch 2/20  Iteration 290/3520 Training loss: 2.3390 0.1731 sec/batch\n",
      "Epoch 2/20  Iteration 291/3520 Training loss: 2.3378 0.1694 sec/batch\n",
      "Epoch 2/20  Iteration 292/3520 Training loss: 2.3367 0.1728 sec/batch\n",
      "Epoch 2/20  Iteration 293/3520 Training loss: 2.3356 0.1730 sec/batch\n",
      "Epoch 2/20  Iteration 294/3520 Training loss: 2.3345 0.1669 sec/batch\n",
      "Epoch 2/20  Iteration 295/3520 Training loss: 2.3333 0.1723 sec/batch\n",
      "Epoch 2/20  Iteration 296/3520 Training loss: 2.3321 0.1652 sec/batch\n",
      "Epoch 2/20  Iteration 297/3520 Training loss: 2.3311 0.1801 sec/batch\n",
      "Epoch 2/20  Iteration 298/3520 Training loss: 2.3300 0.1733 sec/batch\n",
      "Epoch 2/20  Iteration 299/3520 Training loss: 2.3288 0.1688 sec/batch\n",
      "Epoch 2/20  Iteration 300/3520 Training loss: 2.3278 0.1727 sec/batch\n",
      "Epoch 2/20  Iteration 301/3520 Training loss: 2.3268 0.1730 sec/batch\n",
      "Epoch 2/20  Iteration 302/3520 Training loss: 2.3257 0.1680 sec/batch\n",
      "Epoch 2/20  Iteration 303/3520 Training loss: 2.3244 0.1672 sec/batch\n",
      "Epoch 2/20  Iteration 304/3520 Training loss: 2.3233 0.1738 sec/batch\n",
      "Epoch 2/20  Iteration 305/3520 Training loss: 2.3220 0.1661 sec/batch\n",
      "Epoch 2/20  Iteration 306/3520 Training loss: 2.3209 0.1729 sec/batch\n",
      "Epoch 2/20  Iteration 307/3520 Training loss: 2.3200 0.1732 sec/batch\n",
      "Epoch 2/20  Iteration 308/3520 Training loss: 2.3189 0.1733 sec/batch\n",
      "Epoch 2/20  Iteration 309/3520 Training loss: 2.3180 0.1838 sec/batch\n",
      "Epoch 2/20  Iteration 310/3520 Training loss: 2.3170 0.1730 sec/batch\n",
      "Epoch 2/20  Iteration 311/3520 Training loss: 2.3161 0.1654 sec/batch\n",
      "Epoch 2/20  Iteration 312/3520 Training loss: 2.3152 0.1702 sec/batch\n",
      "Epoch 2/20  Iteration 313/3520 Training loss: 2.3144 0.1728 sec/batch\n",
      "Epoch 2/20  Iteration 314/3520 Training loss: 2.3132 0.1730 sec/batch\n",
      "Epoch 2/20  Iteration 315/3520 Training loss: 2.3122 0.1698 sec/batch\n",
      "Epoch 2/20  Iteration 316/3520 Training loss: 2.3113 0.1649 sec/batch\n",
      "Epoch 2/20  Iteration 317/3520 Training loss: 2.3105 0.1726 sec/batch\n",
      "Epoch 2/20  Iteration 318/3520 Training loss: 2.3095 0.1733 sec/batch\n",
      "Epoch 2/20  Iteration 319/3520 Training loss: 2.3085 0.1677 sec/batch\n",
      "Epoch 2/20  Iteration 320/3520 Training loss: 2.3075 0.1726 sec/batch\n",
      "Epoch 2/20  Iteration 321/3520 Training loss: 2.3067 0.1654 sec/batch\n",
      "Epoch 2/20  Iteration 322/3520 Training loss: 2.3057 0.1729 sec/batch\n",
      "Epoch 2/20  Iteration 323/3520 Training loss: 2.3047 0.1728 sec/batch\n",
      "Epoch 2/20  Iteration 324/3520 Training loss: 2.3039 0.1673 sec/batch\n",
      "Epoch 2/20  Iteration 325/3520 Training loss: 2.3030 0.1657 sec/batch\n",
      "Epoch 2/20  Iteration 326/3520 Training loss: 2.3023 0.1689 sec/batch\n",
      "Epoch 2/20  Iteration 327/3520 Training loss: 2.3014 0.1677 sec/batch\n",
      "Epoch 2/20  Iteration 328/3520 Training loss: 2.3005 0.1727 sec/batch\n",
      "Epoch 2/20  Iteration 329/3520 Training loss: 2.2995 0.1732 sec/batch\n",
      "Epoch 2/20  Iteration 330/3520 Training loss: 2.2986 0.1723 sec/batch\n",
      "Epoch 2/20  Iteration 331/3520 Training loss: 2.2976 0.1667 sec/batch\n",
      "Epoch 2/20  Iteration 332/3520 Training loss: 2.2964 0.1727 sec/batch\n",
      "Epoch 2/20  Iteration 333/3520 Training loss: 2.2954 0.1714 sec/batch\n",
      "Epoch 2/20  Iteration 334/3520 Training loss: 2.2944 0.1664 sec/batch\n",
      "Epoch 2/20  Iteration 335/3520 Training loss: 2.2934 0.1681 sec/batch\n",
      "Epoch 2/20  Iteration 336/3520 Training loss: 2.2925 0.1654 sec/batch\n",
      "Epoch 2/20  Iteration 337/3520 Training loss: 2.2915 0.1756 sec/batch\n",
      "Epoch 2/20  Iteration 338/3520 Training loss: 2.2905 0.1644 sec/batch\n",
      "Epoch 2/20  Iteration 339/3520 Training loss: 2.2896 0.1732 sec/batch\n",
      "Epoch 2/20  Iteration 340/3520 Training loss: 2.2885 0.1726 sec/batch\n",
      "Epoch 2/20  Iteration 341/3520 Training loss: 2.2874 0.1728 sec/batch\n",
      "Epoch 2/20  Iteration 342/3520 Training loss: 2.2864 0.1740 sec/batch\n",
      "Epoch 2/20  Iteration 343/3520 Training loss: 2.2854 0.1737 sec/batch\n",
      "Epoch 2/20  Iteration 344/3520 Training loss: 2.2845 0.1682 sec/batch\n",
      "Epoch 2/20  Iteration 345/3520 Training loss: 2.2836 0.1671 sec/batch\n",
      "Epoch 2/20  Iteration 346/3520 Training loss: 2.2825 0.1725 sec/batch\n",
      "Epoch 2/20  Iteration 347/3520 Training loss: 2.2816 0.1722 sec/batch\n",
      "Epoch 2/20  Iteration 348/3520 Training loss: 2.2806 0.1698 sec/batch\n",
      "Epoch 2/20  Iteration 349/3520 Training loss: 2.2796 0.1659 sec/batch\n",
      "Epoch 2/20  Iteration 350/3520 Training loss: 2.2787 0.1731 sec/batch\n",
      "Epoch 2/20  Iteration 351/3520 Training loss: 2.2776 0.1731 sec/batch\n",
      "Epoch 2/20  Iteration 352/3520 Training loss: 2.2768 0.1768 sec/batch\n",
      "Epoch 3/20  Iteration 353/3520 Training loss: 2.1502 0.1728 sec/batch\n",
      "Epoch 3/20  Iteration 354/3520 Training loss: 2.1082 0.1657 sec/batch\n",
      "Epoch 3/20  Iteration 355/3520 Training loss: 2.1135 0.1652 sec/batch\n",
      "Epoch 3/20  Iteration 356/3520 Training loss: 2.1130 0.1677 sec/batch\n",
      "Epoch 3/20  Iteration 357/3520 Training loss: 2.1133 0.1716 sec/batch\n",
      "Epoch 3/20  Iteration 358/3520 Training loss: 2.1128 0.1811 sec/batch\n",
      "Epoch 3/20  Iteration 359/3520 Training loss: 2.1096 0.1708 sec/batch\n",
      "Epoch 3/20  Iteration 360/3520 Training loss: 2.1130 0.1722 sec/batch\n",
      "Epoch 3/20  Iteration 361/3520 Training loss: 2.1116 0.1695 sec/batch\n",
      "Epoch 3/20  Iteration 362/3520 Training loss: 2.1119 0.1685 sec/batch\n",
      "Epoch 3/20  Iteration 363/3520 Training loss: 2.1106 0.1742 sec/batch\n",
      "Epoch 3/20  Iteration 364/3520 Training loss: 2.1108 0.1731 sec/batch\n",
      "Epoch 3/20  Iteration 365/3520 Training loss: 2.1079 0.1733 sec/batch\n",
      "Epoch 3/20  Iteration 366/3520 Training loss: 2.1078 0.1695 sec/batch\n",
      "Epoch 3/20  Iteration 367/3520 Training loss: 2.1046 0.1727 sec/batch\n",
      "Epoch 3/20  Iteration 368/3520 Training loss: 2.1036 0.1653 sec/batch\n",
      "Epoch 3/20  Iteration 369/3520 Training loss: 2.1027 0.1673 sec/batch\n",
      "Epoch 3/20  Iteration 370/3520 Training loss: 2.1024 0.1707 sec/batch\n",
      "Epoch 3/20  Iteration 371/3520 Training loss: 2.1000 0.1721 sec/batch\n",
      "Epoch 3/20  Iteration 372/3520 Training loss: 2.0996 0.1722 sec/batch\n",
      "Epoch 3/20  Iteration 373/3520 Training loss: 2.0992 0.1734 sec/batch\n",
      "Epoch 3/20  Iteration 374/3520 Training loss: 2.0985 0.1722 sec/batch\n",
      "Epoch 3/20  Iteration 375/3520 Training loss: 2.0982 0.1725 sec/batch\n",
      "Epoch 3/20  Iteration 376/3520 Training loss: 2.0970 0.1678 sec/batch\n",
      "Epoch 3/20  Iteration 377/3520 Training loss: 2.0966 0.1671 sec/batch\n",
      "Epoch 3/20  Iteration 378/3520 Training loss: 2.0942 0.1695 sec/batch\n",
      "Epoch 3/20  Iteration 379/3520 Training loss: 2.0922 0.1708 sec/batch\n",
      "Epoch 3/20  Iteration 380/3520 Training loss: 2.0911 0.1709 sec/batch\n",
      "Epoch 3/20  Iteration 381/3520 Training loss: 2.0896 0.1666 sec/batch\n",
      "Epoch 3/20  Iteration 382/3520 Training loss: 2.0887 0.1730 sec/batch\n",
      "Epoch 3/20  Iteration 383/3520 Training loss: 2.0883 0.1730 sec/batch\n",
      "Epoch 3/20  Iteration 384/3520 Training loss: 2.0871 0.1652 sec/batch\n",
      "Epoch 3/20  Iteration 385/3520 Training loss: 2.0862 0.1712 sec/batch\n",
      "Epoch 3/20  Iteration 386/3520 Training loss: 2.0868 0.1728 sec/batch\n",
      "Epoch 3/20  Iteration 387/3520 Training loss: 2.0880 0.1681 sec/batch\n",
      "Epoch 3/20  Iteration 388/3520 Training loss: 2.0891 0.1691 sec/batch\n",
      "Epoch 3/20  Iteration 389/3520 Training loss: 2.0895 0.1729 sec/batch\n",
      "Epoch 3/20  Iteration 390/3520 Training loss: 2.0904 0.1732 sec/batch\n",
      "Epoch 3/20  Iteration 391/3520 Training loss: 2.0913 0.1726 sec/batch\n",
      "Epoch 3/20  Iteration 392/3520 Training loss: 2.0912 0.1723 sec/batch\n",
      "Epoch 3/20  Iteration 393/3520 Training loss: 2.0912 0.1725 sec/batch\n",
      "Epoch 3/20  Iteration 394/3520 Training loss: 2.0897 0.1694 sec/batch\n",
      "Epoch 3/20  Iteration 395/3520 Training loss: 2.0898 0.1726 sec/batch\n",
      "Epoch 3/20  Iteration 396/3520 Training loss: 2.0888 0.1736 sec/batch\n",
      "Epoch 3/20  Iteration 397/3520 Training loss: 2.0883 0.1733 sec/batch\n",
      "Epoch 3/20  Iteration 398/3520 Training loss: 2.0873 0.1689 sec/batch\n",
      "Epoch 3/20  Iteration 399/3520 Training loss: 2.0864 0.1688 sec/batch\n",
      "Epoch 3/20  Iteration 400/3520 Training loss: 2.0858 0.1717 sec/batch\n",
      "Validation loss: 1.9025 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 401/3520 Training loss: 2.0858 0.1875 sec/batch\n",
      "Epoch 3/20  Iteration 402/3520 Training loss: 2.0852 0.1749 sec/batch\n",
      "Epoch 3/20  Iteration 403/3520 Training loss: 2.0844 0.1672 sec/batch\n",
      "Epoch 3/20  Iteration 404/3520 Training loss: 2.0838 0.1692 sec/batch\n",
      "Epoch 3/20  Iteration 405/3520 Training loss: 2.0833 0.1732 sec/batch\n",
      "Epoch 3/20  Iteration 406/3520 Training loss: 2.0824 0.1678 sec/batch\n",
      "Epoch 3/20  Iteration 407/3520 Training loss: 2.0813 0.1737 sec/batch\n",
      "Epoch 3/20  Iteration 408/3520 Training loss: 2.0799 0.1733 sec/batch\n",
      "Epoch 3/20  Iteration 409/3520 Training loss: 2.0784 0.1673 sec/batch\n",
      "Epoch 3/20  Iteration 410/3520 Training loss: 2.0774 0.1735 sec/batch\n",
      "Epoch 3/20  Iteration 411/3520 Training loss: 2.0762 0.1726 sec/batch\n",
      "Epoch 3/20  Iteration 412/3520 Training loss: 2.0755 0.1667 sec/batch\n",
      "Epoch 3/20  Iteration 413/3520 Training loss: 2.0746 0.1736 sec/batch\n",
      "Epoch 3/20  Iteration 414/3520 Training loss: 2.0740 0.1734 sec/batch\n",
      "Epoch 3/20  Iteration 415/3520 Training loss: 2.0730 0.1724 sec/batch\n",
      "Epoch 3/20  Iteration 416/3520 Training loss: 2.0721 0.1699 sec/batch\n",
      "Epoch 3/20  Iteration 417/3520 Training loss: 2.0711 0.1729 sec/batch\n",
      "Epoch 3/20  Iteration 418/3520 Training loss: 2.0700 0.1673 sec/batch\n",
      "Epoch 3/20  Iteration 419/3520 Training loss: 2.0690 0.1729 sec/batch\n",
      "Epoch 3/20  Iteration 420/3520 Training loss: 2.0682 0.1687 sec/batch\n",
      "Epoch 3/20  Iteration 421/3520 Training loss: 2.0671 0.1725 sec/batch\n",
      "Epoch 3/20  Iteration 422/3520 Training loss: 2.0667 0.1728 sec/batch\n",
      "Epoch 3/20  Iteration 423/3520 Training loss: 2.0658 0.1636 sec/batch\n",
      "Epoch 3/20  Iteration 424/3520 Training loss: 2.0651 0.1729 sec/batch\n",
      "Epoch 3/20  Iteration 425/3520 Training loss: 2.0642 0.1725 sec/batch\n",
      "Epoch 3/20  Iteration 426/3520 Training loss: 2.0630 0.1684 sec/batch\n",
      "Epoch 3/20  Iteration 427/3520 Training loss: 2.0622 0.1738 sec/batch\n",
      "Epoch 3/20  Iteration 428/3520 Training loss: 2.0616 0.1648 sec/batch\n",
      "Epoch 3/20  Iteration 429/3520 Training loss: 2.0608 0.1650 sec/batch\n",
      "Epoch 3/20  Iteration 430/3520 Training loss: 2.0601 0.1672 sec/batch\n",
      "Epoch 3/20  Iteration 431/3520 Training loss: 2.0591 0.1717 sec/batch\n",
      "Epoch 3/20  Iteration 432/3520 Training loss: 2.0583 0.1686 sec/batch\n",
      "Epoch 3/20  Iteration 433/3520 Training loss: 2.0571 0.1644 sec/batch\n",
      "Epoch 3/20  Iteration 434/3520 Training loss: 2.0567 0.1689 sec/batch\n",
      "Epoch 3/20  Iteration 435/3520 Training loss: 2.0556 0.1732 sec/batch\n",
      "Epoch 3/20  Iteration 436/3520 Training loss: 2.0550 0.1688 sec/batch\n",
      "Epoch 3/20  Iteration 437/3520 Training loss: 2.0542 0.1727 sec/batch\n",
      "Epoch 3/20  Iteration 438/3520 Training loss: 2.0533 0.1728 sec/batch\n",
      "Epoch 3/20  Iteration 439/3520 Training loss: 2.0529 0.1728 sec/batch\n",
      "Epoch 3/20  Iteration 440/3520 Training loss: 2.0522 0.1696 sec/batch\n",
      "Epoch 3/20  Iteration 441/3520 Training loss: 2.0516 0.1722 sec/batch\n",
      "Epoch 3/20  Iteration 442/3520 Training loss: 2.0509 0.1639 sec/batch\n",
      "Epoch 3/20  Iteration 443/3520 Training loss: 2.0503 0.1682 sec/batch\n",
      "Epoch 3/20  Iteration 444/3520 Training loss: 2.0499 0.1741 sec/batch\n",
      "Epoch 3/20  Iteration 445/3520 Training loss: 2.0492 0.1720 sec/batch\n",
      "Epoch 3/20  Iteration 446/3520 Training loss: 2.0484 0.1725 sec/batch\n",
      "Epoch 3/20  Iteration 447/3520 Training loss: 2.0475 0.1671 sec/batch\n",
      "Epoch 3/20  Iteration 448/3520 Training loss: 2.0465 0.1658 sec/batch\n",
      "Epoch 3/20  Iteration 449/3520 Training loss: 2.0462 0.1767 sec/batch\n",
      "Epoch 3/20  Iteration 450/3520 Training loss: 2.0455 0.1756 sec/batch\n",
      "Epoch 3/20  Iteration 451/3520 Training loss: 2.0450 0.1642 sec/batch\n",
      "Epoch 3/20  Iteration 452/3520 Training loss: 2.0440 0.1743 sec/batch\n",
      "Epoch 3/20  Iteration 453/3520 Training loss: 2.0433 0.1723 sec/batch\n",
      "Epoch 3/20  Iteration 454/3520 Training loss: 2.0428 0.1729 sec/batch\n",
      "Epoch 3/20  Iteration 455/3520 Training loss: 2.0419 0.1690 sec/batch\n",
      "Epoch 3/20  Iteration 456/3520 Training loss: 2.0410 0.1674 sec/batch\n",
      "Epoch 3/20  Iteration 457/3520 Training loss: 2.0403 0.1751 sec/batch\n",
      "Epoch 3/20  Iteration 458/3520 Training loss: 2.0397 0.1677 sec/batch\n",
      "Epoch 3/20  Iteration 459/3520 Training loss: 2.0388 0.1722 sec/batch\n",
      "Epoch 3/20  Iteration 460/3520 Training loss: 2.0377 0.1674 sec/batch\n",
      "Epoch 3/20  Iteration 461/3520 Training loss: 2.0370 0.1691 sec/batch\n",
      "Epoch 3/20  Iteration 462/3520 Training loss: 2.0362 0.1753 sec/batch\n",
      "Epoch 3/20  Iteration 463/3520 Training loss: 2.0353 0.1683 sec/batch\n",
      "Epoch 3/20  Iteration 464/3520 Training loss: 2.0347 0.1689 sec/batch\n",
      "Epoch 3/20  Iteration 465/3520 Training loss: 2.0342 0.1671 sec/batch\n",
      "Epoch 3/20  Iteration 466/3520 Training loss: 2.0335 0.1702 sec/batch\n",
      "Epoch 3/20  Iteration 467/3520 Training loss: 2.0327 0.1726 sec/batch\n",
      "Epoch 3/20  Iteration 468/3520 Training loss: 2.0322 0.1766 sec/batch\n",
      "Epoch 3/20  Iteration 469/3520 Training loss: 2.0315 0.1723 sec/batch\n",
      "Epoch 3/20  Iteration 470/3520 Training loss: 2.0309 0.1729 sec/batch\n",
      "Epoch 3/20  Iteration 471/3520 Training loss: 2.0302 0.1729 sec/batch\n",
      "Epoch 3/20  Iteration 472/3520 Training loss: 2.0296 0.1723 sec/batch\n",
      "Epoch 3/20  Iteration 473/3520 Training loss: 2.0290 0.1742 sec/batch\n",
      "Epoch 3/20  Iteration 474/3520 Training loss: 2.0284 0.1702 sec/batch\n",
      "Epoch 3/20  Iteration 475/3520 Training loss: 2.0277 0.1652 sec/batch\n",
      "Epoch 3/20  Iteration 476/3520 Training loss: 2.0271 0.1679 sec/batch\n",
      "Epoch 3/20  Iteration 477/3520 Training loss: 2.0266 0.1680 sec/batch\n",
      "Epoch 3/20  Iteration 478/3520 Training loss: 2.0258 0.1724 sec/batch\n",
      "Epoch 3/20  Iteration 479/3520 Training loss: 2.0248 0.1664 sec/batch\n",
      "Epoch 3/20  Iteration 480/3520 Training loss: 2.0241 0.1723 sec/batch\n",
      "Epoch 3/20  Iteration 481/3520 Training loss: 2.0233 0.1801 sec/batch\n",
      "Epoch 3/20  Iteration 482/3520 Training loss: 2.0228 0.1724 sec/batch\n",
      "Epoch 3/20  Iteration 483/3520 Training loss: 2.0223 0.1676 sec/batch\n",
      "Epoch 3/20  Iteration 484/3520 Training loss: 2.0218 0.1683 sec/batch\n",
      "Epoch 3/20  Iteration 485/3520 Training loss: 2.0214 0.1717 sec/batch\n",
      "Epoch 3/20  Iteration 486/3520 Training loss: 2.0208 0.1660 sec/batch\n",
      "Epoch 3/20  Iteration 487/3520 Training loss: 2.0203 0.1727 sec/batch\n",
      "Epoch 3/20  Iteration 488/3520 Training loss: 2.0199 0.1656 sec/batch\n",
      "Epoch 3/20  Iteration 489/3520 Training loss: 2.0193 0.1746 sec/batch\n",
      "Epoch 3/20  Iteration 490/3520 Training loss: 2.0186 0.1716 sec/batch\n",
      "Epoch 3/20  Iteration 491/3520 Training loss: 2.0181 0.1726 sec/batch\n",
      "Epoch 3/20  Iteration 492/3520 Training loss: 2.0177 0.1682 sec/batch\n",
      "Epoch 3/20  Iteration 493/3520 Training loss: 2.0172 0.1721 sec/batch\n",
      "Epoch 3/20  Iteration 494/3520 Training loss: 2.0165 0.1685 sec/batch\n",
      "Epoch 3/20  Iteration 495/3520 Training loss: 2.0158 0.1724 sec/batch\n",
      "Epoch 3/20  Iteration 496/3520 Training loss: 2.0152 0.1730 sec/batch\n",
      "Epoch 3/20  Iteration 497/3520 Training loss: 2.0147 0.1724 sec/batch\n",
      "Epoch 3/20  Iteration 498/3520 Training loss: 2.0142 0.1743 sec/batch\n",
      "Epoch 3/20  Iteration 499/3520 Training loss: 2.0135 0.1708 sec/batch\n",
      "Epoch 3/20  Iteration 500/3520 Training loss: 2.0131 0.1659 sec/batch\n",
      "Epoch 3/20  Iteration 501/3520 Training loss: 2.0126 0.1668 sec/batch\n",
      "Epoch 3/20  Iteration 502/3520 Training loss: 2.0123 0.1697 sec/batch\n",
      "Epoch 3/20  Iteration 503/3520 Training loss: 2.0117 0.1725 sec/batch\n",
      "Epoch 3/20  Iteration 504/3520 Training loss: 2.0111 0.1678 sec/batch\n",
      "Epoch 3/20  Iteration 505/3520 Training loss: 2.0104 0.1691 sec/batch\n",
      "Epoch 3/20  Iteration 506/3520 Training loss: 2.0098 0.1682 sec/batch\n",
      "Epoch 3/20  Iteration 507/3520 Training loss: 2.0092 0.1665 sec/batch\n",
      "Epoch 3/20  Iteration 508/3520 Training loss: 2.0085 0.1689 sec/batch\n",
      "Epoch 3/20  Iteration 509/3520 Training loss: 2.0079 0.1749 sec/batch\n",
      "Epoch 3/20  Iteration 510/3520 Training loss: 2.0074 0.1721 sec/batch\n",
      "Epoch 3/20  Iteration 511/3520 Training loss: 2.0068 0.1703 sec/batch\n",
      "Epoch 3/20  Iteration 512/3520 Training loss: 2.0064 0.1660 sec/batch\n",
      "Epoch 3/20  Iteration 513/3520 Training loss: 2.0058 0.1655 sec/batch\n",
      "Epoch 3/20  Iteration 514/3520 Training loss: 2.0053 0.1702 sec/batch\n",
      "Epoch 3/20  Iteration 515/3520 Training loss: 2.0049 0.1730 sec/batch\n",
      "Epoch 3/20  Iteration 516/3520 Training loss: 2.0041 0.1727 sec/batch\n",
      "Epoch 3/20  Iteration 517/3520 Training loss: 2.0034 0.1664 sec/batch\n",
      "Epoch 3/20  Iteration 518/3520 Training loss: 2.0028 0.1689 sec/batch\n",
      "Epoch 3/20  Iteration 519/3520 Training loss: 2.0020 0.1696 sec/batch\n",
      "Epoch 3/20  Iteration 520/3520 Training loss: 2.0015 0.1725 sec/batch\n",
      "Epoch 3/20  Iteration 521/3520 Training loss: 2.0010 0.1801 sec/batch\n",
      "Epoch 3/20  Iteration 522/3520 Training loss: 2.0005 0.1672 sec/batch\n",
      "Epoch 3/20  Iteration 523/3520 Training loss: 2.0000 0.1628 sec/batch\n",
      "Epoch 3/20  Iteration 524/3520 Training loss: 1.9994 0.1685 sec/batch\n",
      "Epoch 3/20  Iteration 525/3520 Training loss: 1.9988 0.1678 sec/batch\n",
      "Epoch 3/20  Iteration 526/3520 Training loss: 1.9982 0.1722 sec/batch\n",
      "Epoch 3/20  Iteration 527/3520 Training loss: 1.9977 0.1683 sec/batch\n",
      "Epoch 3/20  Iteration 528/3520 Training loss: 1.9973 0.1666 sec/batch\n",
      "Epoch 4/20  Iteration 529/3520 Training loss: 1.9323 0.1692 sec/batch\n",
      "Epoch 4/20  Iteration 530/3520 Training loss: 1.8929 0.1662 sec/batch\n",
      "Epoch 4/20  Iteration 531/3520 Training loss: 1.8964 0.1659 sec/batch\n",
      "Epoch 4/20  Iteration 532/3520 Training loss: 1.8979 0.1741 sec/batch\n",
      "Epoch 4/20  Iteration 533/3520 Training loss: 1.8981 0.1659 sec/batch\n",
      "Epoch 4/20  Iteration 534/3520 Training loss: 1.8982 0.1689 sec/batch\n",
      "Epoch 4/20  Iteration 535/3520 Training loss: 1.8971 0.1715 sec/batch\n",
      "Epoch 4/20  Iteration 536/3520 Training loss: 1.9001 0.1642 sec/batch\n",
      "Epoch 4/20  Iteration 537/3520 Training loss: 1.8983 0.1770 sec/batch\n",
      "Epoch 4/20  Iteration 538/3520 Training loss: 1.8983 0.1652 sec/batch\n",
      "Epoch 4/20  Iteration 539/3520 Training loss: 1.8986 0.1688 sec/batch\n",
      "Epoch 4/20  Iteration 540/3520 Training loss: 1.8994 0.1713 sec/batch\n",
      "Epoch 4/20  Iteration 541/3520 Training loss: 1.8963 0.1728 sec/batch\n",
      "Epoch 4/20  Iteration 542/3520 Training loss: 1.8965 0.1723 sec/batch\n",
      "Epoch 4/20  Iteration 543/3520 Training loss: 1.8947 0.1726 sec/batch\n",
      "Epoch 4/20  Iteration 544/3520 Training loss: 1.8942 0.1657 sec/batch\n",
      "Epoch 4/20  Iteration 545/3520 Training loss: 1.8936 0.1682 sec/batch\n",
      "Epoch 4/20  Iteration 546/3520 Training loss: 1.8945 0.1637 sec/batch\n",
      "Epoch 4/20  Iteration 547/3520 Training loss: 1.8929 0.1705 sec/batch\n",
      "Epoch 4/20  Iteration 548/3520 Training loss: 1.8931 0.1663 sec/batch\n",
      "Epoch 4/20  Iteration 549/3520 Training loss: 1.8937 0.1656 sec/batch\n",
      "Epoch 4/20  Iteration 550/3520 Training loss: 1.8936 0.1696 sec/batch\n",
      "Epoch 4/20  Iteration 551/3520 Training loss: 1.8942 0.1714 sec/batch\n",
      "Epoch 4/20  Iteration 552/3520 Training loss: 1.8935 0.1679 sec/batch\n",
      "Epoch 4/20  Iteration 553/3520 Training loss: 1.8935 0.1728 sec/batch\n",
      "Epoch 4/20  Iteration 554/3520 Training loss: 1.8920 0.1645 sec/batch\n",
      "Epoch 4/20  Iteration 555/3520 Training loss: 1.8902 0.1698 sec/batch\n",
      "Epoch 4/20  Iteration 556/3520 Training loss: 1.8894 0.1676 sec/batch\n",
      "Epoch 4/20  Iteration 557/3520 Training loss: 1.8884 0.1663 sec/batch\n",
      "Epoch 4/20  Iteration 558/3520 Training loss: 1.8881 0.1673 sec/batch\n",
      "Epoch 4/20  Iteration 559/3520 Training loss: 1.8879 0.1674 sec/batch\n",
      "Epoch 4/20  Iteration 560/3520 Training loss: 1.8873 0.1666 sec/batch\n",
      "Epoch 4/20  Iteration 561/3520 Training loss: 1.8871 0.1744 sec/batch\n",
      "Epoch 4/20  Iteration 562/3520 Training loss: 1.8870 0.1735 sec/batch\n",
      "Epoch 4/20  Iteration 563/3520 Training loss: 1.8866 0.1646 sec/batch\n",
      "Epoch 4/20  Iteration 564/3520 Training loss: 1.8864 0.1666 sec/batch\n",
      "Epoch 4/20  Iteration 565/3520 Training loss: 1.8854 0.1728 sec/batch\n",
      "Epoch 4/20  Iteration 566/3520 Training loss: 1.8858 0.1640 sec/batch\n",
      "Epoch 4/20  Iteration 567/3520 Training loss: 1.8868 0.1719 sec/batch\n",
      "Epoch 4/20  Iteration 568/3520 Training loss: 1.8867 0.1725 sec/batch\n",
      "Epoch 4/20  Iteration 569/3520 Training loss: 1.8864 0.1686 sec/batch\n",
      "Epoch 4/20  Iteration 570/3520 Training loss: 1.8849 0.1732 sec/batch\n",
      "Epoch 4/20  Iteration 571/3520 Training loss: 1.8848 0.1703 sec/batch\n",
      "Epoch 4/20  Iteration 572/3520 Training loss: 1.8838 0.1717 sec/batch\n",
      "Epoch 4/20  Iteration 573/3520 Training loss: 1.8834 0.1732 sec/batch\n",
      "Epoch 4/20  Iteration 574/3520 Training loss: 1.8823 0.1722 sec/batch\n",
      "Epoch 4/20  Iteration 575/3520 Training loss: 1.8819 0.1646 sec/batch\n",
      "Epoch 4/20  Iteration 576/3520 Training loss: 1.8816 0.1721 sec/batch\n",
      "Epoch 4/20  Iteration 577/3520 Training loss: 1.8814 0.1737 sec/batch\n",
      "Epoch 4/20  Iteration 578/3520 Training loss: 1.8813 0.1645 sec/batch\n",
      "Epoch 4/20  Iteration 579/3520 Training loss: 1.8810 0.1731 sec/batch\n",
      "Epoch 4/20  Iteration 580/3520 Training loss: 1.8808 0.1700 sec/batch\n",
      "Epoch 4/20  Iteration 581/3520 Training loss: 1.8811 0.1649 sec/batch\n",
      "Epoch 4/20  Iteration 582/3520 Training loss: 1.8809 0.1728 sec/batch\n",
      "Epoch 4/20  Iteration 583/3520 Training loss: 1.8802 0.1710 sec/batch\n",
      "Epoch 4/20  Iteration 584/3520 Training loss: 1.8794 0.1651 sec/batch\n",
      "Epoch 4/20  Iteration 585/3520 Training loss: 1.8786 0.1729 sec/batch\n",
      "Epoch 4/20  Iteration 586/3520 Training loss: 1.8782 0.1733 sec/batch\n",
      "Epoch 4/20  Iteration 587/3520 Training loss: 1.8775 0.1663 sec/batch\n",
      "Epoch 4/20  Iteration 588/3520 Training loss: 1.8778 0.1756 sec/batch\n",
      "Epoch 4/20  Iteration 589/3520 Training loss: 1.8774 0.1729 sec/batch\n",
      "Epoch 4/20  Iteration 590/3520 Training loss: 1.8772 0.1682 sec/batch\n",
      "Epoch 4/20  Iteration 591/3520 Training loss: 1.8767 0.1663 sec/batch\n",
      "Epoch 4/20  Iteration 592/3520 Training loss: 1.8765 0.1677 sec/batch\n",
      "Epoch 4/20  Iteration 593/3520 Training loss: 1.8753 0.1714 sec/batch\n",
      "Epoch 4/20  Iteration 594/3520 Training loss: 1.8749 0.1724 sec/batch\n",
      "Epoch 4/20  Iteration 595/3520 Training loss: 1.8743 0.1680 sec/batch\n",
      "Epoch 4/20  Iteration 596/3520 Training loss: 1.8736 0.1670 sec/batch\n",
      "Epoch 4/20  Iteration 597/3520 Training loss: 1.8730 0.1732 sec/batch\n",
      "Epoch 4/20  Iteration 598/3520 Training loss: 1.8730 0.1648 sec/batch\n",
      "Epoch 4/20  Iteration 599/3520 Training loss: 1.8725 0.1665 sec/batch\n",
      "Epoch 4/20  Iteration 600/3520 Training loss: 1.8720 0.1729 sec/batch\n",
      "Validation loss: 1.69632 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 601/3520 Training loss: 1.8721 0.1930 sec/batch\n",
      "Epoch 4/20  Iteration 602/3520 Training loss: 1.8713 0.1856 sec/batch\n",
      "Epoch 4/20  Iteration 603/3520 Training loss: 1.8707 0.1733 sec/batch\n",
      "Epoch 4/20  Iteration 604/3520 Training loss: 1.8706 0.1729 sec/batch\n",
      "Epoch 4/20  Iteration 605/3520 Training loss: 1.8703 0.1724 sec/batch\n",
      "Epoch 4/20  Iteration 606/3520 Training loss: 1.8701 0.1681 sec/batch\n",
      "Epoch 4/20  Iteration 607/3520 Training loss: 1.8694 0.1652 sec/batch\n",
      "Epoch 4/20  Iteration 608/3520 Training loss: 1.8690 0.1669 sec/batch\n",
      "Epoch 4/20  Iteration 609/3520 Training loss: 1.8681 0.1715 sec/batch\n",
      "Epoch 4/20  Iteration 610/3520 Training loss: 1.8679 0.1722 sec/batch\n",
      "Epoch 4/20  Iteration 611/3520 Training loss: 1.8672 0.1680 sec/batch\n",
      "Epoch 4/20  Iteration 612/3520 Training loss: 1.8671 0.1684 sec/batch\n",
      "Epoch 4/20  Iteration 613/3520 Training loss: 1.8666 0.1696 sec/batch\n",
      "Epoch 4/20  Iteration 614/3520 Training loss: 1.8659 0.1729 sec/batch\n",
      "Epoch 4/20  Iteration 615/3520 Training loss: 1.8660 0.1723 sec/batch\n",
      "Epoch 4/20  Iteration 616/3520 Training loss: 1.8657 0.1662 sec/batch\n",
      "Epoch 4/20  Iteration 617/3520 Training loss: 1.8654 0.1689 sec/batch\n",
      "Epoch 4/20  Iteration 618/3520 Training loss: 1.8649 0.1683 sec/batch\n",
      "Epoch 4/20  Iteration 619/3520 Training loss: 1.8647 0.1670 sec/batch\n",
      "Epoch 4/20  Iteration 620/3520 Training loss: 1.8646 0.1683 sec/batch\n",
      "Epoch 4/20  Iteration 621/3520 Training loss: 1.8643 0.1678 sec/batch\n",
      "Epoch 4/20  Iteration 622/3520 Training loss: 1.8638 0.1700 sec/batch\n",
      "Epoch 4/20  Iteration 623/3520 Training loss: 1.8634 0.1677 sec/batch\n",
      "Epoch 4/20  Iteration 624/3520 Training loss: 1.8627 0.1722 sec/batch\n",
      "Epoch 4/20  Iteration 625/3520 Training loss: 1.8627 0.1723 sec/batch\n",
      "Epoch 4/20  Iteration 626/3520 Training loss: 1.8624 0.1723 sec/batch\n",
      "Epoch 4/20  Iteration 627/3520 Training loss: 1.8623 0.1726 sec/batch\n",
      "Epoch 4/20  Iteration 628/3520 Training loss: 1.8616 0.1723 sec/batch\n",
      "Epoch 4/20  Iteration 629/3520 Training loss: 1.8612 0.1657 sec/batch\n",
      "Epoch 4/20  Iteration 630/3520 Training loss: 1.8610 0.1678 sec/batch\n",
      "Epoch 4/20  Iteration 631/3520 Training loss: 1.8605 0.1702 sec/batch\n",
      "Epoch 4/20  Iteration 632/3520 Training loss: 1.8598 0.1736 sec/batch\n",
      "Epoch 4/20  Iteration 633/3520 Training loss: 1.8593 0.1684 sec/batch\n",
      "Epoch 4/20  Iteration 634/3520 Training loss: 1.8590 0.1680 sec/batch\n",
      "Epoch 4/20  Iteration 635/3520 Training loss: 1.8585 0.1667 sec/batch\n",
      "Epoch 4/20  Iteration 636/3520 Training loss: 1.8578 0.1730 sec/batch\n",
      "Epoch 4/20  Iteration 637/3520 Training loss: 1.8575 0.1672 sec/batch\n",
      "Epoch 4/20  Iteration 638/3520 Training loss: 1.8571 0.1651 sec/batch\n",
      "Epoch 4/20  Iteration 639/3520 Training loss: 1.8566 0.1664 sec/batch\n",
      "Epoch 4/20  Iteration 640/3520 Training loss: 1.8563 0.1663 sec/batch\n",
      "Epoch 4/20  Iteration 641/3520 Training loss: 1.8560 0.1721 sec/batch\n",
      "Epoch 4/20  Iteration 642/3520 Training loss: 1.8555 0.1685 sec/batch\n",
      "Epoch 4/20  Iteration 643/3520 Training loss: 1.8550 0.1757 sec/batch\n",
      "Epoch 4/20  Iteration 644/3520 Training loss: 1.8546 0.1719 sec/batch\n",
      "Epoch 4/20  Iteration 645/3520 Training loss: 1.8542 0.1748 sec/batch\n",
      "Epoch 4/20  Iteration 646/3520 Training loss: 1.8539 0.1725 sec/batch\n",
      "Epoch 4/20  Iteration 647/3520 Training loss: 1.8534 0.1697 sec/batch\n",
      "Epoch 4/20  Iteration 648/3520 Training loss: 1.8532 0.1618 sec/batch\n",
      "Epoch 4/20  Iteration 649/3520 Training loss: 1.8529 0.1669 sec/batch\n",
      "Epoch 4/20  Iteration 650/3520 Training loss: 1.8525 0.1731 sec/batch\n",
      "Epoch 4/20  Iteration 651/3520 Training loss: 1.8520 0.1659 sec/batch\n",
      "Epoch 4/20  Iteration 652/3520 Training loss: 1.8516 0.1729 sec/batch\n",
      "Epoch 4/20  Iteration 653/3520 Training loss: 1.8513 0.1725 sec/batch\n",
      "Epoch 4/20  Iteration 654/3520 Training loss: 1.8508 0.1674 sec/batch\n",
      "Epoch 4/20  Iteration 655/3520 Training loss: 1.8501 0.1646 sec/batch\n",
      "Epoch 4/20  Iteration 656/3520 Training loss: 1.8497 0.1676 sec/batch\n",
      "Epoch 4/20  Iteration 657/3520 Training loss: 1.8493 0.1690 sec/batch\n",
      "Epoch 4/20  Iteration 658/3520 Training loss: 1.8490 0.1723 sec/batch\n",
      "Epoch 4/20  Iteration 659/3520 Training loss: 1.8487 0.1713 sec/batch\n",
      "Epoch 4/20  Iteration 660/3520 Training loss: 1.8485 0.1689 sec/batch\n",
      "Epoch 4/20  Iteration 661/3520 Training loss: 1.8483 0.1721 sec/batch\n",
      "Epoch 4/20  Iteration 662/3520 Training loss: 1.8479 0.1691 sec/batch\n",
      "Epoch 4/20  Iteration 663/3520 Training loss: 1.8477 0.1683 sec/batch\n",
      "Epoch 4/20  Iteration 664/3520 Training loss: 1.8474 0.1726 sec/batch\n",
      "Epoch 4/20  Iteration 665/3520 Training loss: 1.8472 0.1711 sec/batch\n",
      "Epoch 4/20  Iteration 666/3520 Training loss: 1.8467 0.1680 sec/batch\n",
      "Epoch 4/20  Iteration 667/3520 Training loss: 1.8465 0.1691 sec/batch\n",
      "Epoch 4/20  Iteration 668/3520 Training loss: 1.8463 0.1734 sec/batch\n",
      "Epoch 4/20  Iteration 669/3520 Training loss: 1.8460 0.1730 sec/batch\n",
      "Epoch 4/20  Iteration 670/3520 Training loss: 1.8456 0.1677 sec/batch\n",
      "Epoch 4/20  Iteration 671/3520 Training loss: 1.8451 0.1721 sec/batch\n",
      "Epoch 4/20  Iteration 672/3520 Training loss: 1.8447 0.1790 sec/batch\n",
      "Epoch 4/20  Iteration 673/3520 Training loss: 1.8445 0.1667 sec/batch\n",
      "Epoch 4/20  Iteration 674/3520 Training loss: 1.8442 0.1721 sec/batch\n",
      "Epoch 4/20  Iteration 675/3520 Training loss: 1.8437 0.1655 sec/batch\n",
      "Epoch 4/20  Iteration 676/3520 Training loss: 1.8435 0.1715 sec/batch\n",
      "Epoch 4/20  Iteration 677/3520 Training loss: 1.8432 0.1669 sec/batch\n",
      "Epoch 4/20  Iteration 678/3520 Training loss: 1.8432 0.1664 sec/batch\n",
      "Epoch 4/20  Iteration 679/3520 Training loss: 1.8429 0.1728 sec/batch\n",
      "Epoch 4/20  Iteration 680/3520 Training loss: 1.8425 0.1755 sec/batch\n",
      "Epoch 4/20  Iteration 681/3520 Training loss: 1.8421 0.1683 sec/batch\n",
      "Epoch 4/20  Iteration 682/3520 Training loss: 1.8417 0.1644 sec/batch\n",
      "Epoch 4/20  Iteration 683/3520 Training loss: 1.8413 0.1685 sec/batch\n",
      "Epoch 4/20  Iteration 684/3520 Training loss: 1.8409 0.1723 sec/batch\n",
      "Epoch 4/20  Iteration 685/3520 Training loss: 1.8405 0.1650 sec/batch\n",
      "Epoch 4/20  Iteration 686/3520 Training loss: 1.8401 0.1666 sec/batch\n",
      "Epoch 4/20  Iteration 687/3520 Training loss: 1.8398 0.1722 sec/batch\n",
      "Epoch 4/20  Iteration 688/3520 Training loss: 1.8396 0.1785 sec/batch\n",
      "Epoch 4/20  Iteration 689/3520 Training loss: 1.8392 0.1779 sec/batch\n",
      "Epoch 4/20  Iteration 690/3520 Training loss: 1.8389 0.1668 sec/batch\n",
      "Epoch 4/20  Iteration 691/3520 Training loss: 1.8387 0.1680 sec/batch\n",
      "Epoch 4/20  Iteration 692/3520 Training loss: 1.8382 0.1707 sec/batch\n",
      "Epoch 4/20  Iteration 693/3520 Training loss: 1.8377 0.1761 sec/batch\n",
      "Epoch 4/20  Iteration 694/3520 Training loss: 1.8372 0.1664 sec/batch\n",
      "Epoch 4/20  Iteration 695/3520 Training loss: 1.8368 0.1726 sec/batch\n",
      "Epoch 4/20  Iteration 696/3520 Training loss: 1.8365 0.1690 sec/batch\n",
      "Epoch 4/20  Iteration 697/3520 Training loss: 1.8363 0.1664 sec/batch\n",
      "Epoch 4/20  Iteration 698/3520 Training loss: 1.8359 0.1680 sec/batch\n",
      "Epoch 4/20  Iteration 699/3520 Training loss: 1.8356 0.1730 sec/batch\n",
      "Epoch 4/20  Iteration 700/3520 Training loss: 1.8352 0.1729 sec/batch\n",
      "Epoch 4/20  Iteration 701/3520 Training loss: 1.8348 0.1726 sec/batch\n",
      "Epoch 4/20  Iteration 702/3520 Training loss: 1.8345 0.1727 sec/batch\n",
      "Epoch 4/20  Iteration 703/3520 Training loss: 1.8341 0.1636 sec/batch\n",
      "Epoch 4/20  Iteration 704/3520 Training loss: 1.8340 0.1709 sec/batch\n",
      "Epoch 5/20  Iteration 705/3520 Training loss: 1.7962 0.1721 sec/batch\n",
      "Epoch 5/20  Iteration 706/3520 Training loss: 1.7586 0.1727 sec/batch\n",
      "Epoch 5/20  Iteration 707/3520 Training loss: 1.7650 0.1705 sec/batch\n",
      "Epoch 5/20  Iteration 708/3520 Training loss: 1.7668 0.1671 sec/batch\n",
      "Epoch 5/20  Iteration 709/3520 Training loss: 1.7690 0.1727 sec/batch\n",
      "Epoch 5/20  Iteration 710/3520 Training loss: 1.7703 0.1675 sec/batch\n",
      "Epoch 5/20  Iteration 711/3520 Training loss: 1.7690 0.1685 sec/batch\n",
      "Epoch 5/20  Iteration 712/3520 Training loss: 1.7708 0.1688 sec/batch\n",
      "Epoch 5/20  Iteration 713/3520 Training loss: 1.7690 0.1690 sec/batch\n",
      "Epoch 5/20  Iteration 714/3520 Training loss: 1.7701 0.1678 sec/batch\n",
      "Epoch 5/20  Iteration 715/3520 Training loss: 1.7702 0.1637 sec/batch\n",
      "Epoch 5/20  Iteration 716/3520 Training loss: 1.7713 0.1723 sec/batch\n",
      "Epoch 5/20  Iteration 717/3520 Training loss: 1.7696 0.1696 sec/batch\n",
      "Epoch 5/20  Iteration 718/3520 Training loss: 1.7702 0.1684 sec/batch\n",
      "Epoch 5/20  Iteration 719/3520 Training loss: 1.7690 0.1683 sec/batch\n",
      "Epoch 5/20  Iteration 720/3520 Training loss: 1.7683 0.1678 sec/batch\n",
      "Epoch 5/20  Iteration 721/3520 Training loss: 1.7684 0.1683 sec/batch\n",
      "Epoch 5/20  Iteration 722/3520 Training loss: 1.7689 0.1725 sec/batch\n",
      "Epoch 5/20  Iteration 723/3520 Training loss: 1.7670 0.1732 sec/batch\n",
      "Epoch 5/20  Iteration 724/3520 Training loss: 1.7683 0.1675 sec/batch\n",
      "Epoch 5/20  Iteration 725/3520 Training loss: 1.7691 0.1730 sec/batch\n",
      "Epoch 5/20  Iteration 726/3520 Training loss: 1.7684 0.1671 sec/batch\n",
      "Epoch 5/20  Iteration 727/3520 Training loss: 1.7697 0.1693 sec/batch\n",
      "Epoch 5/20  Iteration 728/3520 Training loss: 1.7694 0.1665 sec/batch\n",
      "Epoch 5/20  Iteration 729/3520 Training loss: 1.7697 0.1666 sec/batch\n",
      "Epoch 5/20  Iteration 730/3520 Training loss: 1.7687 0.1685 sec/batch\n",
      "Epoch 5/20  Iteration 731/3520 Training loss: 1.7673 0.1723 sec/batch\n",
      "Epoch 5/20  Iteration 732/3520 Training loss: 1.7670 0.1660 sec/batch\n",
      "Epoch 5/20  Iteration 733/3520 Training loss: 1.7659 0.1723 sec/batch\n",
      "Epoch 5/20  Iteration 734/3520 Training loss: 1.7657 0.1648 sec/batch\n",
      "Epoch 5/20  Iteration 735/3520 Training loss: 1.7656 0.1675 sec/batch\n",
      "Epoch 5/20  Iteration 736/3520 Training loss: 1.7654 0.1696 sec/batch\n",
      "Epoch 5/20  Iteration 737/3520 Training loss: 1.7655 0.1720 sec/batch\n",
      "Epoch 5/20  Iteration 738/3520 Training loss: 1.7654 0.1743 sec/batch\n",
      "Epoch 5/20  Iteration 739/3520 Training loss: 1.7650 0.1675 sec/batch\n",
      "Epoch 5/20  Iteration 740/3520 Training loss: 1.7651 0.1683 sec/batch\n",
      "Epoch 5/20  Iteration 741/3520 Training loss: 1.7642 0.1695 sec/batch\n",
      "Epoch 5/20  Iteration 742/3520 Training loss: 1.7653 0.1634 sec/batch\n",
      "Epoch 5/20  Iteration 743/3520 Training loss: 1.7663 0.1676 sec/batch\n",
      "Epoch 5/20  Iteration 744/3520 Training loss: 1.7663 0.1728 sec/batch\n",
      "Epoch 5/20  Iteration 745/3520 Training loss: 1.7662 0.1726 sec/batch\n",
      "Epoch 5/20  Iteration 746/3520 Training loss: 1.7649 0.1657 sec/batch\n",
      "Epoch 5/20  Iteration 747/3520 Training loss: 1.7651 0.1722 sec/batch\n",
      "Epoch 5/20  Iteration 748/3520 Training loss: 1.7641 0.1667 sec/batch\n",
      "Epoch 5/20  Iteration 749/3520 Training loss: 1.7639 0.1680 sec/batch\n",
      "Epoch 5/20  Iteration 750/3520 Training loss: 1.7634 0.1723 sec/batch\n",
      "Epoch 5/20  Iteration 751/3520 Training loss: 1.7630 0.1725 sec/batch\n",
      "Epoch 5/20  Iteration 752/3520 Training loss: 1.7631 0.1726 sec/batch\n",
      "Epoch 5/20  Iteration 753/3520 Training loss: 1.7632 0.1746 sec/batch\n",
      "Epoch 5/20  Iteration 754/3520 Training loss: 1.7630 0.1726 sec/batch\n",
      "Epoch 5/20  Iteration 755/3520 Training loss: 1.7629 0.1726 sec/batch\n",
      "Epoch 5/20  Iteration 756/3520 Training loss: 1.7628 0.1741 sec/batch\n",
      "Epoch 5/20  Iteration 757/3520 Training loss: 1.7630 0.1724 sec/batch\n",
      "Epoch 5/20  Iteration 758/3520 Training loss: 1.7630 0.1727 sec/batch\n",
      "Epoch 5/20  Iteration 759/3520 Training loss: 1.7624 0.1674 sec/batch\n",
      "Epoch 5/20  Iteration 760/3520 Training loss: 1.7618 0.1690 sec/batch\n",
      "Epoch 5/20  Iteration 761/3520 Training loss: 1.7612 0.1671 sec/batch\n",
      "Epoch 5/20  Iteration 762/3520 Training loss: 1.7610 0.1656 sec/batch\n",
      "Epoch 5/20  Iteration 763/3520 Training loss: 1.7604 0.1744 sec/batch\n",
      "Epoch 5/20  Iteration 764/3520 Training loss: 1.7607 0.1645 sec/batch\n",
      "Epoch 5/20  Iteration 765/3520 Training loss: 1.7604 0.1725 sec/batch\n",
      "Epoch 5/20  Iteration 766/3520 Training loss: 1.7604 0.1694 sec/batch\n",
      "Epoch 5/20  Iteration 767/3520 Training loss: 1.7601 0.1721 sec/batch\n",
      "Epoch 5/20  Iteration 768/3520 Training loss: 1.7598 0.1718 sec/batch\n",
      "Epoch 5/20  Iteration 769/3520 Training loss: 1.7591 0.1715 sec/batch\n",
      "Epoch 5/20  Iteration 770/3520 Training loss: 1.7589 0.1686 sec/batch\n",
      "Epoch 5/20  Iteration 771/3520 Training loss: 1.7586 0.1709 sec/batch\n",
      "Epoch 5/20  Iteration 772/3520 Training loss: 1.7580 0.1673 sec/batch\n",
      "Epoch 5/20  Iteration 773/3520 Training loss: 1.7578 0.1674 sec/batch\n",
      "Epoch 5/20  Iteration 774/3520 Training loss: 1.7579 0.1726 sec/batch\n",
      "Epoch 5/20  Iteration 775/3520 Training loss: 1.7574 0.1674 sec/batch\n",
      "Epoch 5/20  Iteration 776/3520 Training loss: 1.7571 0.1677 sec/batch\n",
      "Epoch 5/20  Iteration 777/3520 Training loss: 1.7568 0.1674 sec/batch\n",
      "Epoch 5/20  Iteration 778/3520 Training loss: 1.7560 0.1795 sec/batch\n",
      "Epoch 5/20  Iteration 779/3520 Training loss: 1.7557 0.1675 sec/batch\n",
      "Epoch 5/20  Iteration 780/3520 Training loss: 1.7557 0.1722 sec/batch\n",
      "Epoch 5/20  Iteration 781/3520 Training loss: 1.7556 0.1722 sec/batch\n",
      "Epoch 5/20  Iteration 782/3520 Training loss: 1.7556 0.1727 sec/batch\n",
      "Epoch 5/20  Iteration 783/3520 Training loss: 1.7551 0.1678 sec/batch\n",
      "Epoch 5/20  Iteration 784/3520 Training loss: 1.7548 0.1684 sec/batch\n",
      "Epoch 5/20  Iteration 785/3520 Training loss: 1.7541 0.1680 sec/batch\n",
      "Epoch 5/20  Iteration 786/3520 Training loss: 1.7541 0.1728 sec/batch\n",
      "Epoch 5/20  Iteration 787/3520 Training loss: 1.7537 0.1705 sec/batch\n",
      "Epoch 5/20  Iteration 788/3520 Training loss: 1.7537 0.1728 sec/batch\n",
      "Epoch 5/20  Iteration 789/3520 Training loss: 1.7535 0.1651 sec/batch\n",
      "Epoch 5/20  Iteration 790/3520 Training loss: 1.7529 0.1697 sec/batch\n",
      "Epoch 5/20  Iteration 791/3520 Training loss: 1.7530 0.1725 sec/batch\n",
      "Epoch 5/20  Iteration 792/3520 Training loss: 1.7528 0.1740 sec/batch\n",
      "Epoch 5/20  Iteration 793/3520 Training loss: 1.7528 0.1730 sec/batch\n",
      "Epoch 5/20  Iteration 794/3520 Training loss: 1.7525 0.1719 sec/batch\n",
      "Epoch 5/20  Iteration 795/3520 Training loss: 1.7523 0.1676 sec/batch\n",
      "Epoch 5/20  Iteration 796/3520 Training loss: 1.7525 0.1724 sec/batch\n",
      "Epoch 5/20  Iteration 797/3520 Training loss: 1.7522 0.1723 sec/batch\n",
      "Epoch 5/20  Iteration 798/3520 Training loss: 1.7518 0.1649 sec/batch\n",
      "Epoch 5/20  Iteration 799/3520 Training loss: 1.7516 0.1727 sec/batch\n",
      "Epoch 5/20  Iteration 800/3520 Training loss: 1.7512 0.1721 sec/batch\n",
      "Validation loss: 1.57006 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 801/3520 Training loss: 1.7518 0.1732 sec/batch\n",
      "Epoch 5/20  Iteration 802/3520 Training loss: 1.7517 0.1785 sec/batch\n",
      "Epoch 5/20  Iteration 803/3520 Training loss: 1.7517 0.1725 sec/batch\n",
      "Epoch 5/20  Iteration 804/3520 Training loss: 1.7512 0.1689 sec/batch\n",
      "Epoch 5/20  Iteration 805/3520 Training loss: 1.7510 0.1724 sec/batch\n",
      "Epoch 5/20  Iteration 806/3520 Training loss: 1.7510 0.1723 sec/batch\n",
      "Epoch 5/20  Iteration 807/3520 Training loss: 1.7505 0.1754 sec/batch\n",
      "Epoch 5/20  Iteration 808/3520 Training loss: 1.7499 0.1718 sec/batch\n",
      "Epoch 5/20  Iteration 809/3520 Training loss: 1.7496 0.1721 sec/batch\n",
      "Epoch 5/20  Iteration 810/3520 Training loss: 1.7496 0.1729 sec/batch\n",
      "Epoch 5/20  Iteration 811/3520 Training loss: 1.7493 0.1728 sec/batch\n",
      "Epoch 5/20  Iteration 812/3520 Training loss: 1.7486 0.1727 sec/batch\n",
      "Epoch 5/20  Iteration 813/3520 Training loss: 1.7484 0.1729 sec/batch\n",
      "Epoch 5/20  Iteration 814/3520 Training loss: 1.7481 0.1729 sec/batch\n",
      "Epoch 5/20  Iteration 815/3520 Training loss: 1.7477 0.1738 sec/batch\n",
      "Epoch 5/20  Iteration 816/3520 Training loss: 1.7474 0.1685 sec/batch\n",
      "Epoch 5/20  Iteration 817/3520 Training loss: 1.7474 0.1726 sec/batch\n",
      "Epoch 5/20  Iteration 818/3520 Training loss: 1.7471 0.1699 sec/batch\n",
      "Epoch 5/20  Iteration 819/3520 Training loss: 1.7467 0.1647 sec/batch\n",
      "Epoch 5/20  Iteration 820/3520 Training loss: 1.7465 0.1723 sec/batch\n",
      "Epoch 5/20  Iteration 821/3520 Training loss: 1.7462 0.1726 sec/batch\n",
      "Epoch 5/20  Iteration 822/3520 Training loss: 1.7461 0.1727 sec/batch\n",
      "Epoch 5/20  Iteration 823/3520 Training loss: 1.7458 0.1680 sec/batch\n",
      "Epoch 5/20  Iteration 824/3520 Training loss: 1.7456 0.1678 sec/batch\n",
      "Epoch 5/20  Iteration 825/3520 Training loss: 1.7454 0.1727 sec/batch\n",
      "Epoch 5/20  Iteration 826/3520 Training loss: 1.7452 0.1673 sec/batch\n",
      "Epoch 5/20  Iteration 827/3520 Training loss: 1.7448 0.1731 sec/batch\n",
      "Epoch 5/20  Iteration 828/3520 Training loss: 1.7446 0.1725 sec/batch\n",
      "Epoch 5/20  Iteration 829/3520 Training loss: 1.7444 0.1624 sec/batch\n",
      "Epoch 5/20  Iteration 830/3520 Training loss: 1.7439 0.1728 sec/batch\n",
      "Epoch 5/20  Iteration 831/3520 Training loss: 1.7434 0.1730 sec/batch\n",
      "Epoch 5/20  Iteration 832/3520 Training loss: 1.7431 0.1632 sec/batch\n",
      "Epoch 5/20  Iteration 833/3520 Training loss: 1.7428 0.1727 sec/batch\n",
      "Epoch 5/20  Iteration 834/3520 Training loss: 1.7428 0.1684 sec/batch\n",
      "Epoch 5/20  Iteration 835/3520 Training loss: 1.7426 0.1720 sec/batch\n",
      "Epoch 5/20  Iteration 836/3520 Training loss: 1.7427 0.1719 sec/batch\n",
      "Epoch 5/20  Iteration 837/3520 Training loss: 1.7425 0.1664 sec/batch\n",
      "Epoch 5/20  Iteration 838/3520 Training loss: 1.7424 0.1741 sec/batch\n",
      "Epoch 5/20  Iteration 839/3520 Training loss: 1.7422 0.1670 sec/batch\n",
      "Epoch 5/20  Iteration 840/3520 Training loss: 1.7422 0.1714 sec/batch\n",
      "Epoch 5/20  Iteration 841/3520 Training loss: 1.7421 0.1766 sec/batch\n",
      "Epoch 5/20  Iteration 842/3520 Training loss: 1.7418 0.1691 sec/batch\n",
      "Epoch 5/20  Iteration 843/3520 Training loss: 1.7416 0.1723 sec/batch\n",
      "Epoch 5/20  Iteration 844/3520 Training loss: 1.7415 0.1726 sec/batch\n",
      "Epoch 5/20  Iteration 845/3520 Training loss: 1.7412 0.1650 sec/batch\n",
      "Epoch 5/20  Iteration 846/3520 Training loss: 1.7409 0.1713 sec/batch\n",
      "Epoch 5/20  Iteration 847/3520 Training loss: 1.7404 0.1677 sec/batch\n",
      "Epoch 5/20  Iteration 848/3520 Training loss: 1.7402 0.1737 sec/batch\n",
      "Epoch 5/20  Iteration 849/3520 Training loss: 1.7401 0.1723 sec/batch\n",
      "Epoch 5/20  Iteration 850/3520 Training loss: 1.7399 0.1673 sec/batch\n",
      "Epoch 5/20  Iteration 851/3520 Training loss: 1.7395 0.1726 sec/batch\n",
      "Epoch 5/20  Iteration 852/3520 Training loss: 1.7393 0.1727 sec/batch\n",
      "Epoch 5/20  Iteration 853/3520 Training loss: 1.7392 0.1733 sec/batch\n",
      "Epoch 5/20  Iteration 854/3520 Training loss: 1.7391 0.1696 sec/batch\n",
      "Epoch 5/20  Iteration 855/3520 Training loss: 1.7388 0.1737 sec/batch\n",
      "Epoch 5/20  Iteration 856/3520 Training loss: 1.7386 0.1687 sec/batch\n",
      "Epoch 5/20  Iteration 857/3520 Training loss: 1.7382 0.1674 sec/batch\n",
      "Epoch 5/20  Iteration 858/3520 Training loss: 1.7378 0.1723 sec/batch\n",
      "Epoch 5/20  Iteration 859/3520 Training loss: 1.7376 0.1698 sec/batch\n",
      "Epoch 5/20  Iteration 860/3520 Training loss: 1.7373 0.1745 sec/batch\n",
      "Epoch 5/20  Iteration 861/3520 Training loss: 1.7370 0.1662 sec/batch\n",
      "Epoch 5/20  Iteration 862/3520 Training loss: 1.7368 0.1647 sec/batch\n",
      "Epoch 5/20  Iteration 863/3520 Training loss: 1.7366 0.1725 sec/batch\n",
      "Epoch 5/20  Iteration 864/3520 Training loss: 1.7366 0.1719 sec/batch\n",
      "Epoch 5/20  Iteration 865/3520 Training loss: 1.7361 0.1671 sec/batch\n",
      "Epoch 5/20  Iteration 866/3520 Training loss: 1.7360 0.1721 sec/batch\n",
      "Epoch 5/20  Iteration 867/3520 Training loss: 1.7359 0.1655 sec/batch\n",
      "Epoch 5/20  Iteration 868/3520 Training loss: 1.7355 0.1686 sec/batch\n",
      "Epoch 5/20  Iteration 869/3520 Training loss: 1.7351 0.1691 sec/batch\n",
      "Epoch 5/20  Iteration 870/3520 Training loss: 1.7347 0.1670 sec/batch\n",
      "Epoch 5/20  Iteration 871/3520 Training loss: 1.7344 0.1781 sec/batch\n",
      "Epoch 5/20  Iteration 872/3520 Training loss: 1.7342 0.1727 sec/batch\n",
      "Epoch 5/20  Iteration 873/3520 Training loss: 1.7342 0.1668 sec/batch\n",
      "Epoch 5/20  Iteration 874/3520 Training loss: 1.7338 0.1694 sec/batch\n",
      "Epoch 5/20  Iteration 875/3520 Training loss: 1.7337 0.1687 sec/batch\n",
      "Epoch 5/20  Iteration 876/3520 Training loss: 1.7335 0.1671 sec/batch\n",
      "Epoch 5/20  Iteration 877/3520 Training loss: 1.7332 0.1703 sec/batch\n",
      "Epoch 5/20  Iteration 878/3520 Training loss: 1.7330 0.1698 sec/batch\n",
      "Epoch 5/20  Iteration 879/3520 Training loss: 1.7328 0.1686 sec/batch\n",
      "Epoch 5/20  Iteration 880/3520 Training loss: 1.7327 0.1663 sec/batch\n",
      "Epoch 6/20  Iteration 881/3520 Training loss: 1.7161 0.1713 sec/batch\n",
      "Epoch 6/20  Iteration 882/3520 Training loss: 1.6761 0.1724 sec/batch\n",
      "Epoch 6/20  Iteration 883/3520 Training loss: 1.6854 0.1722 sec/batch\n",
      "Epoch 6/20  Iteration 884/3520 Training loss: 1.6848 0.1724 sec/batch\n",
      "Epoch 6/20  Iteration 885/3520 Training loss: 1.6868 0.1738 sec/batch\n",
      "Epoch 6/20  Iteration 886/3520 Training loss: 1.6874 0.1726 sec/batch\n",
      "Epoch 6/20  Iteration 887/3520 Training loss: 1.6860 0.1637 sec/batch\n",
      "Epoch 6/20  Iteration 888/3520 Training loss: 1.6881 0.1699 sec/batch\n",
      "Epoch 6/20  Iteration 889/3520 Training loss: 1.6868 0.1634 sec/batch\n",
      "Epoch 6/20  Iteration 890/3520 Training loss: 1.6880 0.1687 sec/batch\n",
      "Epoch 6/20  Iteration 891/3520 Training loss: 1.6892 0.1673 sec/batch\n",
      "Epoch 6/20  Iteration 892/3520 Training loss: 1.6909 0.1677 sec/batch\n",
      "Epoch 6/20  Iteration 893/3520 Training loss: 1.6899 0.1680 sec/batch\n",
      "Epoch 6/20  Iteration 894/3520 Training loss: 1.6909 0.1668 sec/batch\n",
      "Epoch 6/20  Iteration 895/3520 Training loss: 1.6903 0.1644 sec/batch\n",
      "Epoch 6/20  Iteration 896/3520 Training loss: 1.6907 0.1676 sec/batch\n",
      "Epoch 6/20  Iteration 897/3520 Training loss: 1.6906 0.1669 sec/batch\n",
      "Epoch 6/20  Iteration 898/3520 Training loss: 1.6909 0.1654 sec/batch\n",
      "Epoch 6/20  Iteration 899/3520 Training loss: 1.6898 0.1726 sec/batch\n",
      "Epoch 6/20  Iteration 900/3520 Training loss: 1.6916 0.1696 sec/batch\n",
      "Epoch 6/20  Iteration 901/3520 Training loss: 1.6918 0.1786 sec/batch\n",
      "Epoch 6/20  Iteration 902/3520 Training loss: 1.6906 0.1706 sec/batch\n",
      "Epoch 6/20  Iteration 903/3520 Training loss: 1.6912 0.1646 sec/batch\n",
      "Epoch 6/20  Iteration 904/3520 Training loss: 1.6908 0.1701 sec/batch\n",
      "Epoch 6/20  Iteration 905/3520 Training loss: 1.6912 0.1724 sec/batch\n",
      "Epoch 6/20  Iteration 906/3520 Training loss: 1.6905 0.1679 sec/batch\n",
      "Epoch 6/20  Iteration 907/3520 Training loss: 1.6892 0.1726 sec/batch\n",
      "Epoch 6/20  Iteration 908/3520 Training loss: 1.6888 0.1739 sec/batch\n",
      "Epoch 6/20  Iteration 909/3520 Training loss: 1.6880 0.1723 sec/batch\n",
      "Epoch 6/20  Iteration 910/3520 Training loss: 1.6880 0.1674 sec/batch\n",
      "Epoch 6/20  Iteration 911/3520 Training loss: 1.6877 0.1726 sec/batch\n",
      "Epoch 6/20  Iteration 912/3520 Training loss: 1.6873 0.1731 sec/batch\n",
      "Epoch 6/20  Iteration 913/3520 Training loss: 1.6874 0.1709 sec/batch\n",
      "Epoch 6/20  Iteration 914/3520 Training loss: 1.6876 0.1692 sec/batch\n",
      "Epoch 6/20  Iteration 915/3520 Training loss: 1.6872 0.1681 sec/batch\n",
      "Epoch 6/20  Iteration 916/3520 Training loss: 1.6872 0.1722 sec/batch\n",
      "Epoch 6/20  Iteration 917/3520 Training loss: 1.6861 0.1665 sec/batch\n",
      "Epoch 6/20  Iteration 918/3520 Training loss: 1.6871 0.1694 sec/batch\n",
      "Epoch 6/20  Iteration 919/3520 Training loss: 1.6880 0.1723 sec/batch\n",
      "Epoch 6/20  Iteration 920/3520 Training loss: 1.6880 0.1699 sec/batch\n",
      "Epoch 6/20  Iteration 921/3520 Training loss: 1.6877 0.1717 sec/batch\n",
      "Epoch 6/20  Iteration 922/3520 Training loss: 1.6865 0.1681 sec/batch\n",
      "Epoch 6/20  Iteration 923/3520 Training loss: 1.6863 0.1680 sec/batch\n",
      "Epoch 6/20  Iteration 924/3520 Training loss: 1.6857 0.1726 sec/batch\n",
      "Epoch 6/20  Iteration 925/3520 Training loss: 1.6851 0.1678 sec/batch\n",
      "Epoch 6/20  Iteration 926/3520 Training loss: 1.6845 0.1622 sec/batch\n",
      "Epoch 6/20  Iteration 927/3520 Training loss: 1.6842 0.1652 sec/batch\n",
      "Epoch 6/20  Iteration 928/3520 Training loss: 1.6840 0.1678 sec/batch\n",
      "Epoch 6/20  Iteration 929/3520 Training loss: 1.6838 0.1751 sec/batch\n",
      "Epoch 6/20  Iteration 930/3520 Training loss: 1.6834 0.1691 sec/batch\n",
      "Epoch 6/20  Iteration 931/3520 Training loss: 1.6833 0.1726 sec/batch\n",
      "Epoch 6/20  Iteration 932/3520 Training loss: 1.6833 0.1686 sec/batch\n",
      "Epoch 6/20  Iteration 933/3520 Training loss: 1.6835 0.1656 sec/batch\n",
      "Epoch 6/20  Iteration 934/3520 Training loss: 1.6837 0.1742 sec/batch\n",
      "Epoch 6/20  Iteration 935/3520 Training loss: 1.6832 0.1681 sec/batch\n",
      "Epoch 6/20  Iteration 936/3520 Training loss: 1.6827 0.1727 sec/batch\n",
      "Epoch 6/20  Iteration 937/3520 Training loss: 1.6821 0.1762 sec/batch\n",
      "Epoch 6/20  Iteration 938/3520 Training loss: 1.6820 0.1721 sec/batch\n",
      "Epoch 6/20  Iteration 939/3520 Training loss: 1.6811 0.1642 sec/batch\n",
      "Epoch 6/20  Iteration 940/3520 Training loss: 1.6816 0.1710 sec/batch\n",
      "Epoch 6/20  Iteration 941/3520 Training loss: 1.6813 0.1728 sec/batch\n",
      "Epoch 6/20  Iteration 942/3520 Training loss: 1.6811 0.1818 sec/batch\n",
      "Epoch 6/20  Iteration 943/3520 Training loss: 1.6808 0.1729 sec/batch\n",
      "Epoch 6/20  Iteration 944/3520 Training loss: 1.6807 0.1741 sec/batch\n",
      "Epoch 6/20  Iteration 945/3520 Training loss: 1.6800 0.1701 sec/batch\n",
      "Epoch 6/20  Iteration 946/3520 Training loss: 1.6799 0.1736 sec/batch\n",
      "Epoch 6/20  Iteration 947/3520 Training loss: 1.6798 0.1725 sec/batch\n",
      "Epoch 6/20  Iteration 948/3520 Training loss: 1.6793 0.1729 sec/batch\n",
      "Epoch 6/20  Iteration 949/3520 Training loss: 1.6789 0.1690 sec/batch\n",
      "Epoch 6/20  Iteration 950/3520 Training loss: 1.6792 0.1720 sec/batch\n",
      "Epoch 6/20  Iteration 951/3520 Training loss: 1.6789 0.1684 sec/batch\n",
      "Epoch 6/20  Iteration 952/3520 Training loss: 1.6788 0.1729 sec/batch\n",
      "Epoch 6/20  Iteration 953/3520 Training loss: 1.6783 0.1725 sec/batch\n",
      "Epoch 6/20  Iteration 954/3520 Training loss: 1.6776 0.1693 sec/batch\n",
      "Epoch 6/20  Iteration 955/3520 Training loss: 1.6774 0.1734 sec/batch\n",
      "Epoch 6/20  Iteration 956/3520 Training loss: 1.6777 0.1724 sec/batch\n",
      "Epoch 6/20  Iteration 957/3520 Training loss: 1.6777 0.1725 sec/batch\n",
      "Epoch 6/20  Iteration 958/3520 Training loss: 1.6781 0.1728 sec/batch\n",
      "Epoch 6/20  Iteration 959/3520 Training loss: 1.6776 0.1722 sec/batch\n",
      "Epoch 6/20  Iteration 960/3520 Training loss: 1.6773 0.1668 sec/batch\n",
      "Epoch 6/20  Iteration 961/3520 Training loss: 1.6767 0.1632 sec/batch\n",
      "Epoch 6/20  Iteration 962/3520 Training loss: 1.6770 0.1725 sec/batch\n",
      "Epoch 6/20  Iteration 963/3520 Training loss: 1.6766 0.1678 sec/batch\n",
      "Epoch 6/20  Iteration 964/3520 Training loss: 1.6767 0.1694 sec/batch\n",
      "Epoch 6/20  Iteration 965/3520 Training loss: 1.6765 0.1733 sec/batch\n",
      "Epoch 6/20  Iteration 966/3520 Training loss: 1.6760 0.1724 sec/batch\n",
      "Epoch 6/20  Iteration 967/3520 Training loss: 1.6764 0.1668 sec/batch\n",
      "Epoch 6/20  Iteration 968/3520 Training loss: 1.6762 0.1669 sec/batch\n",
      "Epoch 6/20  Iteration 969/3520 Training loss: 1.6762 0.1732 sec/batch\n",
      "Epoch 6/20  Iteration 970/3520 Training loss: 1.6759 0.1737 sec/batch\n",
      "Epoch 6/20  Iteration 971/3520 Training loss: 1.6759 0.1704 sec/batch\n",
      "Epoch 6/20  Iteration 972/3520 Training loss: 1.6762 0.1695 sec/batch\n",
      "Epoch 6/20  Iteration 973/3520 Training loss: 1.6759 0.1721 sec/batch\n",
      "Epoch 6/20  Iteration 974/3520 Training loss: 1.6755 0.1722 sec/batch\n",
      "Epoch 6/20  Iteration 975/3520 Training loss: 1.6752 0.1665 sec/batch\n",
      "Epoch 6/20  Iteration 976/3520 Training loss: 1.6747 0.1683 sec/batch\n",
      "Epoch 6/20  Iteration 977/3520 Training loss: 1.6749 0.1751 sec/batch\n",
      "Epoch 6/20  Iteration 978/3520 Training loss: 1.6748 0.1661 sec/batch\n",
      "Epoch 6/20  Iteration 979/3520 Training loss: 1.6748 0.1729 sec/batch\n",
      "Epoch 6/20  Iteration 980/3520 Training loss: 1.6744 0.1729 sec/batch\n",
      "Epoch 6/20  Iteration 981/3520 Training loss: 1.6742 0.1684 sec/batch\n",
      "Epoch 6/20  Iteration 982/3520 Training loss: 1.6741 0.1673 sec/batch\n",
      "Epoch 6/20  Iteration 983/3520 Training loss: 1.6736 0.1660 sec/batch\n",
      "Epoch 6/20  Iteration 984/3520 Training loss: 1.6729 0.1654 sec/batch\n",
      "Epoch 6/20  Iteration 985/3520 Training loss: 1.6726 0.1626 sec/batch\n",
      "Epoch 6/20  Iteration 986/3520 Training loss: 1.6726 0.1713 sec/batch\n",
      "Epoch 6/20  Iteration 987/3520 Training loss: 1.6724 0.1682 sec/batch\n",
      "Epoch 6/20  Iteration 988/3520 Training loss: 1.6718 0.1703 sec/batch\n",
      "Epoch 6/20  Iteration 989/3520 Training loss: 1.6717 0.1676 sec/batch\n",
      "Epoch 6/20  Iteration 990/3520 Training loss: 1.6714 0.1679 sec/batch\n",
      "Epoch 6/20  Iteration 991/3520 Training loss: 1.6711 0.1736 sec/batch\n",
      "Epoch 6/20  Iteration 992/3520 Training loss: 1.6709 0.1645 sec/batch\n",
      "Epoch 6/20  Iteration 993/3520 Training loss: 1.6709 0.1676 sec/batch\n",
      "Epoch 6/20  Iteration 994/3520 Training loss: 1.6707 0.1725 sec/batch\n",
      "Epoch 6/20  Iteration 995/3520 Training loss: 1.6703 0.1703 sec/batch\n",
      "Epoch 6/20  Iteration 996/3520 Training loss: 1.6702 0.1729 sec/batch\n",
      "Epoch 6/20  Iteration 997/3520 Training loss: 1.6699 0.1670 sec/batch\n",
      "Epoch 6/20  Iteration 998/3520 Training loss: 1.6699 0.1736 sec/batch\n",
      "Epoch 6/20  Iteration 999/3520 Training loss: 1.6696 0.1728 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3520 Training loss: 1.6693 0.1729 sec/batch\n",
      "Validation loss: 1.49501 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1001/3520 Training loss: 1.6698 0.1881 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3520 Training loss: 1.6698 0.1816 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3520 Training loss: 1.6696 0.1737 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3520 Training loss: 1.6694 0.1730 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3520 Training loss: 1.6692 0.1695 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3520 Training loss: 1.6687 0.1731 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3520 Training loss: 1.6683 0.1687 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3520 Training loss: 1.6682 0.1677 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3520 Training loss: 1.6678 0.1728 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3520 Training loss: 1.6679 0.1735 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3520 Training loss: 1.6677 0.1735 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3520 Training loss: 1.6678 0.1726 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3520 Training loss: 1.6678 0.1637 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3520 Training loss: 1.6677 0.1846 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3520 Training loss: 1.6677 0.1724 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3520 Training loss: 1.6677 0.1679 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3520 Training loss: 1.6676 0.1674 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3520 Training loss: 1.6674 0.1668 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3520 Training loss: 1.6674 0.1664 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3520 Training loss: 1.6673 0.1691 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3520 Training loss: 1.6672 0.1730 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3520 Training loss: 1.6670 0.1721 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3520 Training loss: 1.6666 0.1653 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3520 Training loss: 1.6664 0.1735 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3520 Training loss: 1.6664 0.1725 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3520 Training loss: 1.6663 0.1650 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3520 Training loss: 1.6661 0.1731 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3520 Training loss: 1.6660 0.1733 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3520 Training loss: 1.6658 0.1727 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3520 Training loss: 1.6659 0.1744 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3520 Training loss: 1.6657 0.1751 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3520 Training loss: 1.6655 0.1655 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3520 Training loss: 1.6653 0.1656 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3520 Training loss: 1.6650 0.1695 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3520 Training loss: 1.6649 0.1685 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3520 Training loss: 1.6647 0.1735 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3520 Training loss: 1.6644 0.1749 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3520 Training loss: 1.6642 0.1696 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3520 Training loss: 1.6641 0.1744 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3520 Training loss: 1.6642 0.1678 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3520 Training loss: 1.6638 0.1616 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3520 Training loss: 1.6637 0.1813 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3520 Training loss: 1.6637 0.1728 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3520 Training loss: 1.6633 0.1651 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3520 Training loss: 1.6629 0.1730 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3520 Training loss: 1.6627 0.1720 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3520 Training loss: 1.6624 0.1667 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3520 Training loss: 1.6623 0.1669 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3520 Training loss: 1.6623 0.1724 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3520 Training loss: 1.6621 0.1726 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3520 Training loss: 1.6620 0.1725 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3520 Training loss: 1.6618 0.1685 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3520 Training loss: 1.6616 0.1685 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3520 Training loss: 1.6615 0.1692 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3520 Training loss: 1.6613 0.1731 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3520 Training loss: 1.6613 0.1725 sec/batch\n",
      "Epoch 7/20  Iteration 1057/3520 Training loss: 1.6593 0.1711 sec/batch\n",
      "Epoch 7/20  Iteration 1058/3520 Training loss: 1.6166 0.1729 sec/batch\n",
      "Epoch 7/20  Iteration 1059/3520 Training loss: 1.6230 0.1681 sec/batch\n",
      "Epoch 7/20  Iteration 1060/3520 Training loss: 1.6232 0.1728 sec/batch\n",
      "Epoch 7/20  Iteration 1061/3520 Training loss: 1.6260 0.1732 sec/batch\n",
      "Epoch 7/20  Iteration 1062/3520 Training loss: 1.6286 0.1732 sec/batch\n",
      "Epoch 7/20  Iteration 1063/3520 Training loss: 1.6276 0.1759 sec/batch\n",
      "Epoch 7/20  Iteration 1064/3520 Training loss: 1.6309 0.1724 sec/batch\n",
      "Epoch 7/20  Iteration 1065/3520 Training loss: 1.6293 0.1728 sec/batch\n",
      "Epoch 7/20  Iteration 1066/3520 Training loss: 1.6288 0.1653 sec/batch\n",
      "Epoch 7/20  Iteration 1067/3520 Training loss: 1.6289 0.1662 sec/batch\n",
      "Epoch 7/20  Iteration 1068/3520 Training loss: 1.6296 0.1749 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3520 Training loss: 1.6283 0.1657 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3520 Training loss: 1.6293 0.1724 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3520 Training loss: 1.6284 0.1727 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3520 Training loss: 1.6277 0.1719 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3520 Training loss: 1.6278 0.1727 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3520 Training loss: 1.6285 0.1673 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3520 Training loss: 1.6270 0.1731 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3520 Training loss: 1.6283 0.1684 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3520 Training loss: 1.6287 0.1667 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3520 Training loss: 1.6276 0.1697 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3520 Training loss: 1.6281 0.1664 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3520 Training loss: 1.6282 0.1745 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3520 Training loss: 1.6281 0.1671 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3520 Training loss: 1.6274 0.1715 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3520 Training loss: 1.6262 0.1737 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3520 Training loss: 1.6264 0.1667 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3520 Training loss: 1.6256 0.1727 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3520 Training loss: 1.6257 0.1678 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3520 Training loss: 1.6253 0.1730 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3520 Training loss: 1.6247 0.1729 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3520 Training loss: 1.6253 0.1730 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3520 Training loss: 1.6249 0.1668 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3520 Training loss: 1.6246 0.1724 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3520 Training loss: 1.6248 0.1684 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3520 Training loss: 1.6240 0.1757 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3520 Training loss: 1.6247 0.1675 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3520 Training loss: 1.6257 0.1729 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3520 Training loss: 1.6260 0.1727 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3520 Training loss: 1.6259 0.1717 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3520 Training loss: 1.6252 0.1680 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3520 Training loss: 1.6254 0.1712 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3520 Training loss: 1.6246 0.1667 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3520 Training loss: 1.6245 0.1670 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3520 Training loss: 1.6241 0.1673 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3520 Training loss: 1.6238 0.1667 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3520 Training loss: 1.6237 0.1657 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3520 Training loss: 1.6239 0.1743 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3520 Training loss: 1.6237 0.1723 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3520 Training loss: 1.6236 0.1708 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3520 Training loss: 1.6237 0.1723 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3520 Training loss: 1.6241 0.1664 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3520 Training loss: 1.6243 0.1752 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3520 Training loss: 1.6240 0.1732 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3520 Training loss: 1.6235 0.1692 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3520 Training loss: 1.6231 0.1757 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3520 Training loss: 1.6230 0.1668 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3520 Training loss: 1.6226 0.1658 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3520 Training loss: 1.6231 0.1740 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3520 Training loss: 1.6228 0.1685 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3520 Training loss: 1.6230 0.1688 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3520 Training loss: 1.6229 0.1731 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3520 Training loss: 1.6229 0.1682 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3520 Training loss: 1.6221 0.1672 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3520 Training loss: 1.6221 0.1672 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3520 Training loss: 1.6219 0.1727 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3520 Training loss: 1.6214 0.1646 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3520 Training loss: 1.6210 0.1726 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3520 Training loss: 1.6212 0.1727 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3520 Training loss: 1.6209 0.1763 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3520 Training loss: 1.6207 0.1729 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3520 Training loss: 1.6204 0.1693 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3520 Training loss: 1.6197 0.1710 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3520 Training loss: 1.6195 0.1676 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3520 Training loss: 1.6197 0.1732 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3520 Training loss: 1.6199 0.1730 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3520 Training loss: 1.6203 0.1685 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3520 Training loss: 1.6201 0.1742 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3520 Training loss: 1.6199 0.1741 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3520 Training loss: 1.6195 0.1662 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3520 Training loss: 1.6198 0.1656 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3520 Training loss: 1.6194 0.1727 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3520 Training loss: 1.6196 0.1724 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3520 Training loss: 1.6194 0.1673 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3520 Training loss: 1.6190 0.1665 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3520 Training loss: 1.6192 0.1658 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3520 Training loss: 1.6191 0.1742 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3520 Training loss: 1.6191 0.1725 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3520 Training loss: 1.6189 0.1685 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3520 Training loss: 1.6188 0.1724 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3520 Training loss: 1.6189 0.1711 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3520 Training loss: 1.6187 0.1728 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3520 Training loss: 1.6184 0.1676 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3520 Training loss: 1.6183 0.1655 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3520 Training loss: 1.6179 0.1685 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3520 Training loss: 1.6181 0.1722 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3520 Training loss: 1.6182 0.1646 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3520 Training loss: 1.6183 0.1715 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3520 Training loss: 1.6179 0.1728 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3520 Training loss: 1.6177 0.1703 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3520 Training loss: 1.6178 0.1722 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3520 Training loss: 1.6173 0.1686 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3520 Training loss: 1.6169 0.1681 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3520 Training loss: 1.6165 0.1757 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3520 Training loss: 1.6165 0.1728 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3520 Training loss: 1.6161 0.1728 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3520 Training loss: 1.6157 0.1721 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3520 Training loss: 1.6156 0.1717 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3520 Training loss: 1.6154 0.1730 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3520 Training loss: 1.6153 0.1686 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3520 Training loss: 1.6153 0.1675 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3520 Training loss: 1.6154 0.1725 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3520 Training loss: 1.6151 0.1755 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3520 Training loss: 1.6148 0.1705 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3520 Training loss: 1.6147 0.1659 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3520 Training loss: 1.6145 0.1792 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3520 Training loss: 1.6146 0.1759 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3520 Training loss: 1.6144 0.1758 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3520 Training loss: 1.6142 0.1720 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3520 Training loss: 1.6142 0.1672 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3520 Training loss: 1.6141 0.1649 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3520 Training loss: 1.6139 0.1727 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3520 Training loss: 1.6138 0.1727 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3520 Training loss: 1.6137 0.1728 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3520 Training loss: 1.6133 0.1724 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3520 Training loss: 1.6129 0.1726 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3520 Training loss: 1.6127 0.1693 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3520 Training loss: 1.6125 0.1673 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3520 Training loss: 1.6126 0.1674 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3520 Training loss: 1.6125 0.1671 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3520 Training loss: 1.6126 0.1677 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3520 Training loss: 1.6126 0.1720 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3520 Training loss: 1.6125 0.1709 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3520 Training loss: 1.6125 0.1725 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3520 Training loss: 1.6124 0.1679 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3520 Training loss: 1.6123 0.1793 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3520 Training loss: 1.6122 0.1740 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3520 Training loss: 1.6122 0.1664 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3520 Training loss: 1.6122 0.1726 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3520 Training loss: 1.6120 0.1726 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3520 Training loss: 1.6118 0.1728 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3520 Training loss: 1.6114 0.1675 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3520 Training loss: 1.6112 0.1649 sec/batch\n",
      "Validation loss: 1.44074 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1201/3520 Training loss: 1.6118 0.1696 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3520 Training loss: 1.6117 0.1747 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3520 Training loss: 1.6115 0.1727 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3520 Training loss: 1.6114 0.1733 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3520 Training loss: 1.6113 0.1726 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3520 Training loss: 1.6114 0.1653 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3520 Training loss: 1.6113 0.1683 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3520 Training loss: 1.6111 0.1674 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3520 Training loss: 1.6109 0.1692 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3520 Training loss: 1.6107 0.1726 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3520 Training loss: 1.6106 0.1739 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3520 Training loss: 1.6104 0.1666 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3520 Training loss: 1.6101 0.1683 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3520 Training loss: 1.6101 0.1710 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3520 Training loss: 1.6101 0.1677 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3520 Training loss: 1.6101 0.1756 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3520 Training loss: 1.6098 0.1692 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3520 Training loss: 1.6097 0.1740 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3520 Training loss: 1.6097 0.1694 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3520 Training loss: 1.6094 0.1704 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3520 Training loss: 1.6089 0.1658 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3520 Training loss: 1.6086 0.1730 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3520 Training loss: 1.6084 0.1664 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3520 Training loss: 1.6084 0.1669 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3520 Training loss: 1.6084 0.1651 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3520 Training loss: 1.6082 0.1654 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3520 Training loss: 1.6081 0.1646 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3520 Training loss: 1.6079 0.1723 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3520 Training loss: 1.6078 0.1671 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3520 Training loss: 1.6079 0.1726 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3520 Training loss: 1.6077 0.1712 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3520 Training loss: 1.6078 0.1693 sec/batch\n",
      "Epoch 8/20  Iteration 1233/3520 Training loss: 1.6021 0.1724 sec/batch\n",
      "Epoch 8/20  Iteration 1234/3520 Training loss: 1.5618 0.1720 sec/batch\n",
      "Epoch 8/20  Iteration 1235/3520 Training loss: 1.5725 0.1725 sec/batch\n",
      "Epoch 8/20  Iteration 1236/3520 Training loss: 1.5731 0.1729 sec/batch\n",
      "Epoch 8/20  Iteration 1237/3520 Training loss: 1.5775 0.1665 sec/batch\n",
      "Epoch 8/20  Iteration 1238/3520 Training loss: 1.5775 0.1727 sec/batch\n",
      "Epoch 8/20  Iteration 1239/3520 Training loss: 1.5753 0.1643 sec/batch\n",
      "Epoch 8/20  Iteration 1240/3520 Training loss: 1.5783 0.1728 sec/batch\n",
      "Epoch 8/20  Iteration 1241/3520 Training loss: 1.5763 0.1715 sec/batch\n",
      "Epoch 8/20  Iteration 1242/3520 Training loss: 1.5768 0.1728 sec/batch\n",
      "Epoch 8/20  Iteration 1243/3520 Training loss: 1.5782 0.1690 sec/batch\n",
      "Epoch 8/20  Iteration 1244/3520 Training loss: 1.5792 0.1722 sec/batch\n",
      "Epoch 8/20  Iteration 1245/3520 Training loss: 1.5784 0.1691 sec/batch\n",
      "Epoch 8/20  Iteration 1246/3520 Training loss: 1.5804 0.1672 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3520 Training loss: 1.5803 0.1731 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3520 Training loss: 1.5802 0.1730 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3520 Training loss: 1.5803 0.1674 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3520 Training loss: 1.5806 0.1730 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3520 Training loss: 1.5797 0.1728 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3520 Training loss: 1.5812 0.1734 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3520 Training loss: 1.5817 0.1674 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3520 Training loss: 1.5809 0.1734 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3520 Training loss: 1.5815 0.1728 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3520 Training loss: 1.5813 0.1732 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3520 Training loss: 1.5814 0.1663 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3520 Training loss: 1.5806 0.1721 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3520 Training loss: 1.5795 0.1728 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3520 Training loss: 1.5796 0.1681 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3520 Training loss: 1.5786 0.1745 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3520 Training loss: 1.5788 0.1729 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3520 Training loss: 1.5786 0.1730 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3520 Training loss: 1.5781 0.1737 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3520 Training loss: 1.5788 0.1735 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3520 Training loss: 1.5786 0.1730 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3520 Training loss: 1.5783 0.1658 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3520 Training loss: 1.5785 0.1673 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3520 Training loss: 1.5773 0.1637 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3520 Training loss: 1.5780 0.1648 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3520 Training loss: 1.5792 0.1728 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3520 Training loss: 1.5792 0.1727 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3520 Training loss: 1.5791 0.1723 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3520 Training loss: 1.5783 0.1648 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3520 Training loss: 1.5785 0.1761 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3520 Training loss: 1.5777 0.1726 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3520 Training loss: 1.5773 0.1731 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3520 Training loss: 1.5770 0.1684 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3520 Training loss: 1.5768 0.1730 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3520 Training loss: 1.5769 0.1729 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3520 Training loss: 1.5769 0.1644 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3520 Training loss: 1.5768 0.1658 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3520 Training loss: 1.5770 0.1732 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3520 Training loss: 1.5770 0.1730 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3520 Training loss: 1.5772 0.1668 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3520 Training loss: 1.5771 0.1730 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3520 Training loss: 1.5767 0.1731 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3520 Training loss: 1.5763 0.1755 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3520 Training loss: 1.5760 0.1683 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3520 Training loss: 1.5761 0.1721 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3520 Training loss: 1.5756 0.1698 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3520 Training loss: 1.5761 0.1729 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3520 Training loss: 1.5759 0.1663 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3520 Training loss: 1.5760 0.1663 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3520 Training loss: 1.5759 0.1723 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3520 Training loss: 1.5759 0.1627 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3520 Training loss: 1.5753 0.1677 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3520 Training loss: 1.5753 0.1741 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3520 Training loss: 1.5751 0.1656 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3520 Training loss: 1.5746 0.1711 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3520 Training loss: 1.5744 0.1747 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3520 Training loss: 1.5746 0.1719 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3520 Training loss: 1.5744 0.1642 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3520 Training loss: 1.5741 0.1644 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3520 Training loss: 1.5738 0.1696 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3520 Training loss: 1.5731 0.1686 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3520 Training loss: 1.5728 0.1748 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3520 Training loss: 1.5730 0.1669 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3520 Training loss: 1.5731 0.1730 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3520 Training loss: 1.5733 0.1688 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3520 Training loss: 1.5732 0.1672 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3520 Training loss: 1.5731 0.1654 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3520 Training loss: 1.5727 0.1666 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3520 Training loss: 1.5730 0.1672 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3520 Training loss: 1.5728 0.1659 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3520 Training loss: 1.5729 0.1678 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3520 Training loss: 1.5727 0.1738 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3520 Training loss: 1.5722 0.1793 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3520 Training loss: 1.5726 0.1652 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3520 Training loss: 1.5725 0.1724 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3520 Training loss: 1.5727 0.1673 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3520 Training loss: 1.5725 0.1727 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3520 Training loss: 1.5724 0.1668 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3520 Training loss: 1.5726 0.1696 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3520 Training loss: 1.5725 0.1656 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3520 Training loss: 1.5722 0.1755 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3520 Training loss: 1.5721 0.1671 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3520 Training loss: 1.5717 0.1682 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3520 Training loss: 1.5720 0.1671 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3520 Training loss: 1.5720 0.1633 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3520 Training loss: 1.5721 0.1720 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3520 Training loss: 1.5718 0.1673 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3520 Training loss: 1.5716 0.1723 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3520 Training loss: 1.5718 0.1668 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3520 Training loss: 1.5714 0.1722 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3520 Training loss: 1.5708 0.1725 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3520 Training loss: 1.5704 0.1735 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3520 Training loss: 1.5705 0.1685 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3520 Training loss: 1.5704 0.1732 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3520 Training loss: 1.5699 0.1733 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3520 Training loss: 1.5699 0.1731 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3520 Training loss: 1.5697 0.1730 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3520 Training loss: 1.5696 0.1731 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3520 Training loss: 1.5695 0.1642 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3520 Training loss: 1.5697 0.1729 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3520 Training loss: 1.5694 0.1727 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3520 Training loss: 1.5692 0.1671 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3520 Training loss: 1.5692 0.1688 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3520 Training loss: 1.5690 0.1715 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3520 Training loss: 1.5690 0.1727 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3520 Training loss: 1.5688 0.1660 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3520 Training loss: 1.5686 0.1684 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3520 Training loss: 1.5686 0.1727 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3520 Training loss: 1.5686 0.1663 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3520 Training loss: 1.5685 0.1667 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3520 Training loss: 1.5685 0.1724 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3520 Training loss: 1.5684 0.1726 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3520 Training loss: 1.5680 0.1723 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3520 Training loss: 1.5677 0.1738 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3520 Training loss: 1.5676 0.1728 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3520 Training loss: 1.5675 0.1653 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3520 Training loss: 1.5676 0.1684 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3520 Training loss: 1.5675 0.1726 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3520 Training loss: 1.5676 0.1727 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3520 Training loss: 1.5676 0.1672 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3520 Training loss: 1.5677 0.1706 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3520 Training loss: 1.5677 0.1759 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3520 Training loss: 1.5677 0.1681 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3520 Training loss: 1.5677 0.1678 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3520 Training loss: 1.5676 0.1673 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3520 Training loss: 1.5676 0.1675 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3520 Training loss: 1.5676 0.1747 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3520 Training loss: 1.5675 0.1694 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3520 Training loss: 1.5674 0.1732 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3520 Training loss: 1.5670 0.1707 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3520 Training loss: 1.5669 0.1741 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3520 Training loss: 1.5669 0.1680 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3520 Training loss: 1.5668 0.1662 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3520 Training loss: 1.5666 0.1721 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3520 Training loss: 1.5666 0.1724 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3520 Training loss: 1.5665 0.1660 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3520 Training loss: 1.5666 0.1725 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3520 Training loss: 1.5665 0.1744 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3520 Training loss: 1.5663 0.1700 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3520 Training loss: 1.5661 0.1726 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3520 Training loss: 1.5659 0.1751 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3520 Training loss: 1.5659 0.1737 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3520 Training loss: 1.5658 0.1728 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3520 Training loss: 1.5655 0.1687 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3520 Training loss: 1.5655 0.1656 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3520 Training loss: 1.5656 0.1767 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3520 Training loss: 1.5656 0.1730 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3520 Training loss: 1.5654 0.1677 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3520 Training loss: 1.5654 0.1798 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3520 Training loss: 1.5654 0.1682 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3520 Training loss: 1.5652 0.1755 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3520 Training loss: 1.5648 0.1726 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3520 Training loss: 1.5647 0.1729 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3520 Training loss: 1.5645 0.1727 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3520 Training loss: 1.5645 0.1749 sec/batch\n",
      "Validation loss: 1.39655 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 1401/3520 Training loss: 1.5651 0.1974 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3520 Training loss: 1.5650 0.1673 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3520 Training loss: 1.5650 0.1682 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3520 Training loss: 1.5648 0.1721 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3520 Training loss: 1.5647 0.1663 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3520 Training loss: 1.5647 0.1668 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3520 Training loss: 1.5647 0.1756 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3520 Training loss: 1.5648 0.1728 sec/batch\n",
      "Epoch 9/20  Iteration 1409/3520 Training loss: 1.5715 0.1721 sec/batch\n",
      "Epoch 9/20  Iteration 1410/3520 Training loss: 1.5298 0.1726 sec/batch\n",
      "Epoch 9/20  Iteration 1411/3520 Training loss: 1.5379 0.1645 sec/batch\n",
      "Epoch 9/20  Iteration 1412/3520 Training loss: 1.5396 0.1726 sec/batch\n",
      "Epoch 9/20  Iteration 1413/3520 Training loss: 1.5411 0.1690 sec/batch\n",
      "Epoch 9/20  Iteration 1414/3520 Training loss: 1.5438 0.1696 sec/batch\n",
      "Epoch 9/20  Iteration 1415/3520 Training loss: 1.5412 0.1719 sec/batch\n",
      "Epoch 9/20  Iteration 1416/3520 Training loss: 1.5439 0.1680 sec/batch\n",
      "Epoch 9/20  Iteration 1417/3520 Training loss: 1.5422 0.1728 sec/batch\n",
      "Epoch 9/20  Iteration 1418/3520 Training loss: 1.5418 0.1690 sec/batch\n",
      "Epoch 9/20  Iteration 1419/3520 Training loss: 1.5436 0.1694 sec/batch\n",
      "Epoch 9/20  Iteration 1420/3520 Training loss: 1.5442 0.1664 sec/batch\n",
      "Epoch 9/20  Iteration 1421/3520 Training loss: 1.5435 0.1727 sec/batch\n",
      "Epoch 9/20  Iteration 1422/3520 Training loss: 1.5446 0.1711 sec/batch\n",
      "Epoch 9/20  Iteration 1423/3520 Training loss: 1.5448 0.1673 sec/batch\n",
      "Epoch 9/20  Iteration 1424/3520 Training loss: 1.5443 0.1725 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3520 Training loss: 1.5435 0.1730 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3520 Training loss: 1.5444 0.1728 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3520 Training loss: 1.5433 0.1702 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3520 Training loss: 1.5442 0.1663 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3520 Training loss: 1.5443 0.1664 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3520 Training loss: 1.5435 0.1736 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3520 Training loss: 1.5448 0.1698 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3520 Training loss: 1.5447 0.1735 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3520 Training loss: 1.5450 0.1682 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3520 Training loss: 1.5444 0.1652 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3520 Training loss: 1.5434 0.1662 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3520 Training loss: 1.5434 0.1675 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3520 Training loss: 1.5428 0.1725 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3520 Training loss: 1.5429 0.1679 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3520 Training loss: 1.5424 0.1723 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3520 Training loss: 1.5422 0.1693 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3520 Training loss: 1.5428 0.1680 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3520 Training loss: 1.5426 0.1709 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3520 Training loss: 1.5422 0.1719 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3520 Training loss: 1.5425 0.1721 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3520 Training loss: 1.5413 0.1728 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3520 Training loss: 1.5421 0.1710 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3520 Training loss: 1.5432 0.1663 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3520 Training loss: 1.5432 0.1730 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3520 Training loss: 1.5430 0.1727 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3520 Training loss: 1.5421 0.1697 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3520 Training loss: 1.5425 0.1647 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3520 Training loss: 1.5419 0.1665 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3520 Training loss: 1.5415 0.1816 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3520 Training loss: 1.5413 0.1681 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3520 Training loss: 1.5410 0.1726 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3520 Training loss: 1.5409 0.1765 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3520 Training loss: 1.5411 0.1724 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3520 Training loss: 1.5411 0.1796 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3520 Training loss: 1.5409 0.1645 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3520 Training loss: 1.5411 0.1689 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3520 Training loss: 1.5412 0.1673 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3520 Training loss: 1.5414 0.1701 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3520 Training loss: 1.5411 0.1732 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3520 Training loss: 1.5408 0.1749 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3520 Training loss: 1.5407 0.1728 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3520 Training loss: 1.5406 0.1754 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3520 Training loss: 1.5399 0.1678 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3520 Training loss: 1.5400 0.1723 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3520 Training loss: 1.5399 0.1675 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3520 Training loss: 1.5399 0.1679 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3520 Training loss: 1.5398 0.1752 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3520 Training loss: 1.5397 0.1722 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3520 Training loss: 1.5390 0.1671 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3520 Training loss: 1.5389 0.1716 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3520 Training loss: 1.5387 0.1677 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3520 Training loss: 1.5383 0.1651 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3520 Training loss: 1.5378 0.1686 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3520 Training loss: 1.5381 0.1723 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3520 Training loss: 1.5379 0.1675 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3520 Training loss: 1.5377 0.1729 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3520 Training loss: 1.5374 0.1657 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3520 Training loss: 1.5367 0.1752 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3520 Training loss: 1.5367 0.1657 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3520 Training loss: 1.5367 0.1766 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3520 Training loss: 1.5369 0.1655 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3520 Training loss: 1.5373 0.1691 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3520 Training loss: 1.5371 0.1671 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3520 Training loss: 1.5369 0.1686 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3520 Training loss: 1.5366 0.1793 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3520 Training loss: 1.5369 0.1726 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3520 Training loss: 1.5366 0.1716 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3520 Training loss: 1.5367 0.1706 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3520 Training loss: 1.5366 0.1663 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3520 Training loss: 1.5361 0.1674 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3520 Training loss: 1.5366 0.1695 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3520 Training loss: 1.5365 0.1730 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3520 Training loss: 1.5367 0.1653 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3520 Training loss: 1.5364 0.1677 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3520 Training loss: 1.5365 0.1725 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3520 Training loss: 1.5367 0.1684 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3520 Training loss: 1.5365 0.1676 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3520 Training loss: 1.5363 0.1724 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3520 Training loss: 1.5362 0.1663 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3520 Training loss: 1.5358 0.1764 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3520 Training loss: 1.5362 0.1659 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3520 Training loss: 1.5362 0.1732 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3520 Training loss: 1.5364 0.1661 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3520 Training loss: 1.5360 0.1723 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3520 Training loss: 1.5358 0.1689 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3520 Training loss: 1.5359 0.1677 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3520 Training loss: 1.5355 0.1727 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3520 Training loss: 1.5349 0.1729 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3520 Training loss: 1.5346 0.1724 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3520 Training loss: 1.5348 0.1623 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3520 Training loss: 1.5346 0.1724 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3520 Training loss: 1.5342 0.1789 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3520 Training loss: 1.5343 0.1657 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3520 Training loss: 1.5341 0.1727 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3520 Training loss: 1.5340 0.1643 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3520 Training loss: 1.5341 0.1701 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3520 Training loss: 1.5343 0.1665 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3520 Training loss: 1.5340 0.1699 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3520 Training loss: 1.5339 0.1672 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3520 Training loss: 1.5338 0.1649 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3520 Training loss: 1.5336 0.1731 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3520 Training loss: 1.5336 0.1673 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3520 Training loss: 1.5334 0.1753 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3520 Training loss: 1.5333 0.1674 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3520 Training loss: 1.5333 0.1734 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3520 Training loss: 1.5332 0.1731 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3520 Training loss: 1.5330 0.1724 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3520 Training loss: 1.5330 0.1731 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3520 Training loss: 1.5329 0.1728 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3520 Training loss: 1.5325 0.1723 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3520 Training loss: 1.5321 0.1723 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3520 Training loss: 1.5320 0.1726 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3520 Training loss: 1.5319 0.1728 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3520 Training loss: 1.5320 0.1687 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3520 Training loss: 1.5319 0.1771 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3520 Training loss: 1.5322 0.1691 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3520 Training loss: 1.5322 0.1675 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3520 Training loss: 1.5323 0.1724 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3520 Training loss: 1.5323 0.1652 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3520 Training loss: 1.5322 0.1727 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3520 Training loss: 1.5322 0.1680 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3520 Training loss: 1.5321 0.1724 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3520 Training loss: 1.5322 0.1762 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3520 Training loss: 1.5322 0.1697 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3520 Training loss: 1.5321 0.1677 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3520 Training loss: 1.5319 0.1666 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3520 Training loss: 1.5316 0.1681 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3520 Training loss: 1.5314 0.1689 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3520 Training loss: 1.5315 0.1664 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3520 Training loss: 1.5315 0.1735 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3520 Training loss: 1.5313 0.1725 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3520 Training loss: 1.5313 0.1683 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3520 Training loss: 1.5312 0.1840 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3520 Training loss: 1.5314 0.1654 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3520 Training loss: 1.5312 0.1649 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3520 Training loss: 1.5311 0.1676 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3520 Training loss: 1.5309 0.1721 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3520 Training loss: 1.5307 0.1758 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3520 Training loss: 1.5306 0.1723 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3520 Training loss: 1.5305 0.1736 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3520 Training loss: 1.5303 0.1690 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3520 Training loss: 1.5302 0.1695 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3520 Training loss: 1.5303 0.1727 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3520 Training loss: 1.5303 0.1720 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3520 Training loss: 1.5301 0.1739 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3520 Training loss: 1.5301 0.1672 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3520 Training loss: 1.5301 0.1686 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3520 Training loss: 1.5298 0.1669 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3520 Training loss: 1.5295 0.1670 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3520 Training loss: 1.5293 0.1701 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3520 Training loss: 1.5291 0.1737 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3520 Training loss: 1.5291 0.1683 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3520 Training loss: 1.5291 0.1654 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3520 Training loss: 1.5290 0.1676 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3520 Training loss: 1.5289 0.1685 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3520 Training loss: 1.5289 0.1752 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3520 Training loss: 1.5288 0.1723 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3520 Training loss: 1.5288 0.1725 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3520 Training loss: 1.5288 0.1676 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3520 Training loss: 1.5289 0.1684 sec/batch\n",
      "Epoch 10/20  Iteration 1585/3520 Training loss: 1.5454 0.1701 sec/batch\n",
      "Epoch 10/20  Iteration 1586/3520 Training loss: 1.5099 0.1647 sec/batch\n",
      "Epoch 10/20  Iteration 1587/3520 Training loss: 1.5120 0.1731 sec/batch\n",
      "Epoch 10/20  Iteration 1588/3520 Training loss: 1.5122 0.1650 sec/batch\n",
      "Epoch 10/20  Iteration 1589/3520 Training loss: 1.5129 0.1730 sec/batch\n",
      "Epoch 10/20  Iteration 1590/3520 Training loss: 1.5125 0.1729 sec/batch\n",
      "Epoch 10/20  Iteration 1591/3520 Training loss: 1.5110 0.1726 sec/batch\n",
      "Epoch 10/20  Iteration 1592/3520 Training loss: 1.5132 0.1650 sec/batch\n",
      "Epoch 10/20  Iteration 1593/3520 Training loss: 1.5109 0.1676 sec/batch\n",
      "Epoch 10/20  Iteration 1594/3520 Training loss: 1.5105 0.1697 sec/batch\n",
      "Epoch 10/20  Iteration 1595/3520 Training loss: 1.5105 0.1680 sec/batch\n",
      "Epoch 10/20  Iteration 1596/3520 Training loss: 1.5115 0.1686 sec/batch\n",
      "Epoch 10/20  Iteration 1597/3520 Training loss: 1.5105 0.1727 sec/batch\n",
      "Epoch 10/20  Iteration 1598/3520 Training loss: 1.5122 0.1747 sec/batch\n",
      "Epoch 10/20  Iteration 1599/3520 Training loss: 1.5119 0.1692 sec/batch\n",
      "Epoch 10/20  Iteration 1600/3520 Training loss: 1.5110 0.1713 sec/batch\n",
      "Validation loss: 1.36252 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 1601/3520 Training loss: 1.5167 0.1749 sec/batch\n",
      "Epoch 10/20  Iteration 1602/3520 Training loss: 1.5173 0.1749 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3520 Training loss: 1.5158 0.1671 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3520 Training loss: 1.5169 0.1728 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3520 Training loss: 1.5169 0.1671 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3520 Training loss: 1.5164 0.1685 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3520 Training loss: 1.5169 0.1729 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3520 Training loss: 1.5172 0.1692 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3520 Training loss: 1.5173 0.1724 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3520 Training loss: 1.5162 0.1651 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3520 Training loss: 1.5153 0.1661 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3520 Training loss: 1.5152 0.1722 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3520 Training loss: 1.5144 0.1707 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3520 Training loss: 1.5147 0.1680 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3520 Training loss: 1.5141 0.1725 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3520 Training loss: 1.5138 0.1660 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3520 Training loss: 1.5143 0.1673 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3520 Training loss: 1.5141 0.1740 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3520 Training loss: 1.5137 0.1726 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3520 Training loss: 1.5140 0.1695 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3520 Training loss: 1.5130 0.1687 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3520 Training loss: 1.5137 0.1722 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3520 Training loss: 1.5149 0.1692 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3520 Training loss: 1.5149 0.1702 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3520 Training loss: 1.5144 0.1722 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3520 Training loss: 1.5134 0.1688 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3520 Training loss: 1.5133 0.1741 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3520 Training loss: 1.5126 0.1724 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3520 Training loss: 1.5122 0.1636 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3520 Training loss: 1.5117 0.1727 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3520 Training loss: 1.5116 0.1653 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3520 Training loss: 1.5115 0.1726 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3520 Training loss: 1.5116 0.1662 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3520 Training loss: 1.5113 0.1726 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3520 Training loss: 1.5112 0.1761 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3520 Training loss: 1.5112 0.1757 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3520 Training loss: 1.5113 0.1727 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3520 Training loss: 1.5115 0.1713 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3520 Training loss: 1.5113 0.1730 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3520 Training loss: 1.5110 0.1742 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3520 Training loss: 1.5107 0.1657 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3520 Training loss: 1.5107 0.1726 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3520 Training loss: 1.5103 0.1664 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3520 Training loss: 1.5107 0.1718 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3520 Training loss: 1.5105 0.1667 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3520 Training loss: 1.5104 0.1657 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3520 Training loss: 1.5104 0.1736 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3520 Training loss: 1.5104 0.1723 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3520 Training loss: 1.5098 0.1728 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3520 Training loss: 1.5096 0.1686 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3520 Training loss: 1.5095 0.1690 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3520 Training loss: 1.5090 0.1752 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3520 Training loss: 1.5088 0.1728 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3520 Training loss: 1.5089 0.1635 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3520 Training loss: 1.5086 0.1686 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3520 Training loss: 1.5084 0.1729 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3520 Training loss: 1.5081 0.1673 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3520 Training loss: 1.5075 0.1733 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3520 Training loss: 1.5074 0.1670 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3520 Training loss: 1.5076 0.1706 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3520 Training loss: 1.5076 0.1726 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3520 Training loss: 1.5081 0.1661 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3520 Training loss: 1.5081 0.1656 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3520 Training loss: 1.5081 0.1685 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3520 Training loss: 1.5077 0.1651 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3520 Training loss: 1.5080 0.1723 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3520 Training loss: 1.5077 0.1713 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3520 Training loss: 1.5078 0.1720 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3520 Training loss: 1.5076 0.1677 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3520 Training loss: 1.5072 0.1759 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3520 Training loss: 1.5075 0.1679 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3520 Training loss: 1.5074 0.1684 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3520 Training loss: 1.5076 0.1737 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3520 Training loss: 1.5073 0.1686 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3520 Training loss: 1.5073 0.1725 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3520 Training loss: 1.5075 0.1714 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3520 Training loss: 1.5073 0.1656 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3520 Training loss: 1.5071 0.1673 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3520 Training loss: 1.5070 0.1687 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3520 Training loss: 1.5066 0.1737 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3520 Training loss: 1.5069 0.1727 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3520 Training loss: 1.5069 0.1670 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3520 Training loss: 1.5069 0.1692 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3520 Training loss: 1.5067 0.1730 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3520 Training loss: 1.5065 0.1731 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3520 Training loss: 1.5065 0.1668 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3520 Training loss: 1.5061 0.1689 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3520 Training loss: 1.5056 0.1651 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3520 Training loss: 1.5052 0.1674 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3520 Training loss: 1.5053 0.1691 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3520 Training loss: 1.5051 0.1829 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3520 Training loss: 1.5047 0.1661 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3520 Training loss: 1.5047 0.1731 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3520 Training loss: 1.5044 0.1731 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3520 Training loss: 1.5043 0.1666 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3520 Training loss: 1.5042 0.1719 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3520 Training loss: 1.5042 0.1672 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3520 Training loss: 1.5041 0.1676 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3520 Training loss: 1.5039 0.1717 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3520 Training loss: 1.5038 0.1677 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3520 Training loss: 1.5036 0.1685 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3520 Training loss: 1.5036 0.1705 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3520 Training loss: 1.5035 0.1741 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3520 Training loss: 1.5034 0.1682 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3520 Training loss: 1.5034 0.1729 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3520 Training loss: 1.5034 0.1635 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3520 Training loss: 1.5032 0.1728 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3520 Training loss: 1.5033 0.1685 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3520 Training loss: 1.5032 0.1649 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3520 Training loss: 1.5028 0.1734 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3520 Training loss: 1.5024 0.1728 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3520 Training loss: 1.5024 0.1728 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3520 Training loss: 1.5023 0.1708 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3520 Training loss: 1.5025 0.1741 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3520 Training loss: 1.5024 0.1686 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3520 Training loss: 1.5026 0.1731 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3520 Training loss: 1.5026 0.1731 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3520 Training loss: 1.5026 0.1690 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3520 Training loss: 1.5027 0.1656 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3520 Training loss: 1.5027 0.1669 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3520 Training loss: 1.5027 0.1729 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3520 Training loss: 1.5027 0.1676 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3520 Training loss: 1.5028 0.1724 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3520 Training loss: 1.5029 0.1668 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3520 Training loss: 1.5028 0.1690 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3520 Training loss: 1.5026 0.1697 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3520 Training loss: 1.5022 0.1743 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3520 Training loss: 1.5021 0.1655 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3520 Training loss: 1.5022 0.1655 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3520 Training loss: 1.5021 0.1726 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3520 Training loss: 1.5020 0.1655 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3520 Training loss: 1.5019 0.1687 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3520 Training loss: 1.5019 0.1685 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3520 Training loss: 1.5020 0.1721 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3520 Training loss: 1.5019 0.1680 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3520 Training loss: 1.5017 0.1729 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3520 Training loss: 1.5017 0.1691 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3520 Training loss: 1.5014 0.1681 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3520 Training loss: 1.5014 0.1651 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3520 Training loss: 1.5013 0.1726 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3520 Training loss: 1.5010 0.1660 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3520 Training loss: 1.5010 0.1659 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3520 Training loss: 1.5010 0.1676 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3520 Training loss: 1.5011 0.1693 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3520 Training loss: 1.5009 0.1726 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3520 Training loss: 1.5009 0.1786 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3520 Training loss: 1.5009 0.1763 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3520 Training loss: 1.5007 0.1725 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3520 Training loss: 1.5004 0.1672 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3520 Training loss: 1.5002 0.1750 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3520 Training loss: 1.5000 0.1735 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3520 Training loss: 1.5001 0.1728 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3520 Training loss: 1.5002 0.1730 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3520 Training loss: 1.5001 0.1699 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3520 Training loss: 1.5001 0.1665 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3520 Training loss: 1.5000 0.1649 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3520 Training loss: 1.4999 0.1696 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3520 Training loss: 1.4999 0.1725 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3520 Training loss: 1.4999 0.1736 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3520 Training loss: 1.5000 0.1763 sec/batch\n",
      "Epoch 11/20  Iteration 1761/3520 Training loss: 1.5134 0.1722 sec/batch\n",
      "Epoch 11/20  Iteration 1762/3520 Training loss: 1.4776 0.1753 sec/batch\n",
      "Epoch 11/20  Iteration 1763/3520 Training loss: 1.4821 0.1724 sec/batch\n",
      "Epoch 11/20  Iteration 1764/3520 Training loss: 1.4801 0.1731 sec/batch\n",
      "Epoch 11/20  Iteration 1765/3520 Training loss: 1.4835 0.1665 sec/batch\n",
      "Epoch 11/20  Iteration 1766/3520 Training loss: 1.4843 0.1659 sec/batch\n",
      "Epoch 11/20  Iteration 1767/3520 Training loss: 1.4832 0.1686 sec/batch\n",
      "Epoch 11/20  Iteration 1768/3520 Training loss: 1.4843 0.1740 sec/batch\n",
      "Epoch 11/20  Iteration 1769/3520 Training loss: 1.4827 0.1751 sec/batch\n",
      "Epoch 11/20  Iteration 1770/3520 Training loss: 1.4827 0.1675 sec/batch\n",
      "Epoch 11/20  Iteration 1771/3520 Training loss: 1.4844 0.1823 sec/batch\n",
      "Epoch 11/20  Iteration 1772/3520 Training loss: 1.4856 0.1726 sec/batch\n",
      "Epoch 11/20  Iteration 1773/3520 Training loss: 1.4844 0.1722 sec/batch\n",
      "Epoch 11/20  Iteration 1774/3520 Training loss: 1.4857 0.1727 sec/batch\n",
      "Epoch 11/20  Iteration 1775/3520 Training loss: 1.4852 0.1814 sec/batch\n",
      "Epoch 11/20  Iteration 1776/3520 Training loss: 1.4840 0.1751 sec/batch\n",
      "Epoch 11/20  Iteration 1777/3520 Training loss: 1.4832 0.1721 sec/batch\n",
      "Epoch 11/20  Iteration 1778/3520 Training loss: 1.4840 0.1705 sec/batch\n",
      "Epoch 11/20  Iteration 1779/3520 Training loss: 1.4830 0.1747 sec/batch\n",
      "Epoch 11/20  Iteration 1780/3520 Training loss: 1.4838 0.1707 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3520 Training loss: 1.4838 0.1754 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3520 Training loss: 1.4832 0.1730 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3520 Training loss: 1.4840 0.1730 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3520 Training loss: 1.4846 0.1666 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3520 Training loss: 1.4848 0.1725 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3520 Training loss: 1.4844 0.1742 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3520 Training loss: 1.4837 0.1677 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3520 Training loss: 1.4839 0.1679 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3520 Training loss: 1.4830 0.1724 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3520 Training loss: 1.4833 0.1689 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3520 Training loss: 1.4829 0.1775 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3520 Training loss: 1.4825 0.1750 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3520 Training loss: 1.4832 0.1722 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3520 Training loss: 1.4830 0.1726 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3520 Training loss: 1.4830 0.1681 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3520 Training loss: 1.4831 0.1693 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3520 Training loss: 1.4823 0.1697 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3520 Training loss: 1.4830 0.1712 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3520 Training loss: 1.4841 0.1666 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3520 Training loss: 1.4845 0.1730 sec/batch\n",
      "Validation loss: 1.33293 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 1801/3520 Training loss: 1.4869 0.1900 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3520 Training loss: 1.4865 0.1779 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3520 Training loss: 1.4865 0.1760 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3520 Training loss: 1.4859 0.1725 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3520 Training loss: 1.4856 0.1730 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3520 Training loss: 1.4853 0.1731 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3520 Training loss: 1.4851 0.1730 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3520 Training loss: 1.4851 0.1680 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3520 Training loss: 1.4852 0.1724 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3520 Training loss: 1.4851 0.1729 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3520 Training loss: 1.4850 0.1725 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3520 Training loss: 1.4851 0.1752 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3520 Training loss: 1.4851 0.1626 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3520 Training loss: 1.4851 0.1682 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3520 Training loss: 1.4847 0.1722 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3520 Training loss: 1.4845 0.1721 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3520 Training loss: 1.4842 0.1729 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3520 Training loss: 1.4841 0.1726 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3520 Training loss: 1.4835 0.1721 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3520 Training loss: 1.4840 0.1727 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3520 Training loss: 1.4837 0.1659 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3520 Training loss: 1.4838 0.1687 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3520 Training loss: 1.4836 0.1780 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3520 Training loss: 1.4836 0.1730 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3520 Training loss: 1.4829 0.1661 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3520 Training loss: 1.4827 0.1674 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3520 Training loss: 1.4827 0.1666 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3520 Training loss: 1.4821 0.1669 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3520 Training loss: 1.4819 0.1729 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3520 Training loss: 1.4821 0.1721 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3520 Training loss: 1.4818 0.1716 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3520 Training loss: 1.4815 0.1702 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3520 Training loss: 1.4812 0.1668 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3520 Training loss: 1.4805 0.1762 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3520 Training loss: 1.4804 0.1704 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3520 Training loss: 1.4806 0.1723 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3520 Training loss: 1.4806 0.1733 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3520 Training loss: 1.4810 0.1681 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3520 Training loss: 1.4809 0.1690 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3520 Training loss: 1.4807 0.1728 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3520 Training loss: 1.4803 0.1691 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3520 Training loss: 1.4808 0.1722 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3520 Training loss: 1.4805 0.1673 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3520 Training loss: 1.4806 0.1694 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3520 Training loss: 1.4804 0.1668 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3520 Training loss: 1.4801 0.1722 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3520 Training loss: 1.4805 0.1664 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3520 Training loss: 1.4804 0.1687 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3520 Training loss: 1.4806 0.1724 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3520 Training loss: 1.4804 0.1706 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3520 Training loss: 1.4804 0.1663 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3520 Training loss: 1.4806 0.1785 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3520 Training loss: 1.4803 0.1726 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3520 Training loss: 1.4801 0.1729 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3520 Training loss: 1.4798 0.1671 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3520 Training loss: 1.4795 0.1693 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3520 Training loss: 1.4797 0.1671 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3520 Training loss: 1.4797 0.1727 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3520 Training loss: 1.4798 0.1722 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3520 Training loss: 1.4796 0.1724 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3520 Training loss: 1.4795 0.1670 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3520 Training loss: 1.4795 0.1724 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3520 Training loss: 1.4792 0.1676 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3520 Training loss: 1.4786 0.1739 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3520 Training loss: 1.4782 0.1643 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3520 Training loss: 1.4784 0.1700 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3520 Training loss: 1.4784 0.1728 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3520 Training loss: 1.4780 0.1715 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3520 Training loss: 1.4780 0.1700 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3520 Training loss: 1.4778 0.1667 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3520 Training loss: 1.4777 0.1674 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3520 Training loss: 1.4778 0.1681 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3520 Training loss: 1.4779 0.1657 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3520 Training loss: 1.4777 0.1675 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3520 Training loss: 1.4775 0.1720 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3520 Training loss: 1.4774 0.1648 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3520 Training loss: 1.4771 0.1661 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3520 Training loss: 1.4773 0.1683 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3520 Training loss: 1.4772 0.1720 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3520 Training loss: 1.4771 0.1641 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3520 Training loss: 1.4771 0.1671 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3520 Training loss: 1.4772 0.1668 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3520 Training loss: 1.4771 0.1730 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3520 Training loss: 1.4770 0.1688 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3520 Training loss: 1.4769 0.1665 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3520 Training loss: 1.4766 0.1631 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3520 Training loss: 1.4763 0.1728 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3520 Training loss: 1.4763 0.1725 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3520 Training loss: 1.4762 0.1671 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3520 Training loss: 1.4764 0.1696 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3520 Training loss: 1.4763 0.1663 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3520 Training loss: 1.4766 0.1724 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3520 Training loss: 1.4765 0.1695 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3520 Training loss: 1.4767 0.1680 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3520 Training loss: 1.4767 0.1735 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3520 Training loss: 1.4767 0.1678 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3520 Training loss: 1.4767 0.1643 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3520 Training loss: 1.4766 0.1714 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3520 Training loss: 1.4767 0.1693 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3520 Training loss: 1.4767 0.1667 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3520 Training loss: 1.4767 0.1658 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3520 Training loss: 1.4765 0.1672 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3520 Training loss: 1.4763 0.1730 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3520 Training loss: 1.4762 0.1672 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3520 Training loss: 1.4763 0.1709 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3520 Training loss: 1.4762 0.1681 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3520 Training loss: 1.4761 0.1742 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3520 Training loss: 1.4760 0.1722 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3520 Training loss: 1.4760 0.1684 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3520 Training loss: 1.4762 0.1730 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3520 Training loss: 1.4760 0.1720 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3520 Training loss: 1.4759 0.1665 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3520 Training loss: 1.4758 0.1738 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3520 Training loss: 1.4756 0.1683 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3520 Training loss: 1.4755 0.1675 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3520 Training loss: 1.4754 0.1653 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3520 Training loss: 1.4752 0.1657 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3520 Training loss: 1.4752 0.1704 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3520 Training loss: 1.4752 0.1676 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3520 Training loss: 1.4752 0.1655 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3520 Training loss: 1.4750 0.1668 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3520 Training loss: 1.4750 0.1703 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3520 Training loss: 1.4751 0.1708 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3520 Training loss: 1.4749 0.1665 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3520 Training loss: 1.4746 0.1704 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3520 Training loss: 1.4744 0.1677 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3520 Training loss: 1.4742 0.1667 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3520 Training loss: 1.4743 0.1752 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3520 Training loss: 1.4744 0.1732 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3520 Training loss: 1.4744 0.1750 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3520 Training loss: 1.4744 0.1671 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3520 Training loss: 1.4743 0.1685 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3520 Training loss: 1.4742 0.1733 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3520 Training loss: 1.4744 0.1724 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3520 Training loss: 1.4743 0.1667 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3520 Training loss: 1.4744 0.1705 sec/batch\n",
      "Epoch 12/20  Iteration 1937/3520 Training loss: 1.4874 0.1670 sec/batch\n",
      "Epoch 12/20  Iteration 1938/3520 Training loss: 1.4506 0.1681 sec/batch\n",
      "Epoch 12/20  Iteration 1939/3520 Training loss: 1.4551 0.1667 sec/batch\n",
      "Epoch 12/20  Iteration 1940/3520 Training loss: 1.4556 0.1679 sec/batch\n",
      "Epoch 12/20  Iteration 1941/3520 Training loss: 1.4602 0.1754 sec/batch\n",
      "Epoch 12/20  Iteration 1942/3520 Training loss: 1.4604 0.1703 sec/batch\n",
      "Epoch 12/20  Iteration 1943/3520 Training loss: 1.4586 0.1719 sec/batch\n",
      "Epoch 12/20  Iteration 1944/3520 Training loss: 1.4602 0.1650 sec/batch\n",
      "Epoch 12/20  Iteration 1945/3520 Training loss: 1.4579 0.1733 sec/batch\n",
      "Epoch 12/20  Iteration 1946/3520 Training loss: 1.4585 0.1674 sec/batch\n",
      "Epoch 12/20  Iteration 1947/3520 Training loss: 1.4592 0.1725 sec/batch\n",
      "Epoch 12/20  Iteration 1948/3520 Training loss: 1.4595 0.1636 sec/batch\n",
      "Epoch 12/20  Iteration 1949/3520 Training loss: 1.4584 0.1727 sec/batch\n",
      "Epoch 12/20  Iteration 1950/3520 Training loss: 1.4607 0.1724 sec/batch\n",
      "Epoch 12/20  Iteration 1951/3520 Training loss: 1.4599 0.1721 sec/batch\n",
      "Epoch 12/20  Iteration 1952/3520 Training loss: 1.4591 0.1660 sec/batch\n",
      "Epoch 12/20  Iteration 1953/3520 Training loss: 1.4585 0.1671 sec/batch\n",
      "Epoch 12/20  Iteration 1954/3520 Training loss: 1.4592 0.1657 sec/batch\n",
      "Epoch 12/20  Iteration 1955/3520 Training loss: 1.4582 0.1683 sec/batch\n",
      "Epoch 12/20  Iteration 1956/3520 Training loss: 1.4595 0.1757 sec/batch\n",
      "Epoch 12/20  Iteration 1957/3520 Training loss: 1.4598 0.1721 sec/batch\n",
      "Epoch 12/20  Iteration 1958/3520 Training loss: 1.4590 0.1690 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3520 Training loss: 1.4600 0.1687 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3520 Training loss: 1.4605 0.1724 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3520 Training loss: 1.4609 0.1667 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3520 Training loss: 1.4601 0.1695 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3520 Training loss: 1.4593 0.1729 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3520 Training loss: 1.4591 0.1680 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3520 Training loss: 1.4587 0.1729 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3520 Training loss: 1.4592 0.1716 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3520 Training loss: 1.4587 0.1654 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3520 Training loss: 1.4585 0.1683 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3520 Training loss: 1.4593 0.1718 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3520 Training loss: 1.4593 0.1705 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3520 Training loss: 1.4593 0.1678 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3520 Training loss: 1.4598 0.1688 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3520 Training loss: 1.4588 0.1723 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3520 Training loss: 1.4597 0.1699 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3520 Training loss: 1.4607 0.1732 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3520 Training loss: 1.4607 0.1666 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3520 Training loss: 1.4607 0.1659 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3520 Training loss: 1.4600 0.1734 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3520 Training loss: 1.4602 0.1732 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3520 Training loss: 1.4597 0.1728 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3520 Training loss: 1.4595 0.1665 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3520 Training loss: 1.4591 0.1672 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3520 Training loss: 1.4590 0.1723 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3520 Training loss: 1.4593 0.1724 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3520 Training loss: 1.4594 0.1664 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3520 Training loss: 1.4593 0.1706 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3520 Training loss: 1.4592 0.1724 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3520 Training loss: 1.4594 0.1748 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3520 Training loss: 1.4597 0.1723 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3520 Training loss: 1.4600 0.1726 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3520 Training loss: 1.4598 0.1661 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3520 Training loss: 1.4598 0.1753 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3520 Training loss: 1.4594 0.1719 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3520 Training loss: 1.4593 0.1685 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3520 Training loss: 1.4588 0.1662 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3520 Training loss: 1.4592 0.1680 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3520 Training loss: 1.4590 0.1651 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3520 Training loss: 1.4590 0.1689 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3520 Training loss: 1.4589 0.1688 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3520 Training loss: 1.4590 0.1668 sec/batch\n",
      "Validation loss: 1.31374 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 2001/3520 Training loss: 1.4600 0.1717 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3520 Training loss: 1.4601 0.1758 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3520 Training loss: 1.4600 0.1702 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3520 Training loss: 1.4596 0.1677 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3520 Training loss: 1.4594 0.1726 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3520 Training loss: 1.4594 0.1672 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3520 Training loss: 1.4592 0.1683 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3520 Training loss: 1.4590 0.1644 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3520 Training loss: 1.4588 0.1684 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3520 Training loss: 1.4582 0.1636 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3520 Training loss: 1.4581 0.1727 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3520 Training loss: 1.4583 0.1658 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3520 Training loss: 1.4585 0.1723 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3520 Training loss: 1.4588 0.1695 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3520 Training loss: 1.4586 0.1725 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3520 Training loss: 1.4584 0.1693 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3520 Training loss: 1.4580 0.1754 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3520 Training loss: 1.4584 0.1687 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3520 Training loss: 1.4581 0.1722 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3520 Training loss: 1.4582 0.1836 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3520 Training loss: 1.4580 0.1677 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3520 Training loss: 1.4575 0.1728 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3520 Training loss: 1.4580 0.1615 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3520 Training loss: 1.4577 0.1724 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3520 Training loss: 1.4579 0.1724 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3520 Training loss: 1.4576 0.1724 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3520 Training loss: 1.4575 0.1703 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3520 Training loss: 1.4577 0.1691 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3520 Training loss: 1.4575 0.1725 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3520 Training loss: 1.4572 0.1717 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3520 Training loss: 1.4572 0.1680 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3520 Training loss: 1.4567 0.1728 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3520 Training loss: 1.4570 0.1662 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3520 Training loss: 1.4570 0.1726 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3520 Training loss: 1.4571 0.1667 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3520 Training loss: 1.4568 0.1689 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3520 Training loss: 1.4567 0.1723 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3520 Training loss: 1.4569 0.1669 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3520 Training loss: 1.4566 0.1672 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3520 Training loss: 1.4561 0.1681 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3520 Training loss: 1.4557 0.1684 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3520 Training loss: 1.4559 0.1686 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3520 Training loss: 1.4557 0.1675 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3520 Training loss: 1.4552 0.1657 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3520 Training loss: 1.4553 0.1705 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3520 Training loss: 1.4551 0.1715 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3520 Training loss: 1.4551 0.1685 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3520 Training loss: 1.4551 0.1685 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3520 Training loss: 1.4552 0.1743 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3520 Training loss: 1.4551 0.1715 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3520 Training loss: 1.4549 0.1648 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3520 Training loss: 1.4549 0.1653 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3520 Training loss: 1.4546 0.1696 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3520 Training loss: 1.4548 0.1731 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3520 Training loss: 1.4546 0.1832 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3520 Training loss: 1.4546 0.1683 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3520 Training loss: 1.4547 0.1728 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3520 Training loss: 1.4547 0.1751 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3520 Training loss: 1.4546 0.1750 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3520 Training loss: 1.4546 0.1641 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3520 Training loss: 1.4545 0.1652 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3520 Training loss: 1.4543 0.1727 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3520 Training loss: 1.4540 0.1686 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3520 Training loss: 1.4538 0.1668 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3520 Training loss: 1.4537 0.1664 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3520 Training loss: 1.4540 0.1688 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3520 Training loss: 1.4538 0.1697 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3520 Training loss: 1.4541 0.1727 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3520 Training loss: 1.4541 0.1694 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3520 Training loss: 1.4541 0.1721 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3520 Training loss: 1.4542 0.1709 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3520 Training loss: 1.4541 0.1658 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3520 Training loss: 1.4542 0.1679 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3520 Training loss: 1.4540 0.1694 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3520 Training loss: 1.4541 0.1754 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3520 Training loss: 1.4541 0.1690 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3520 Training loss: 1.4541 0.1712 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3520 Training loss: 1.4540 0.1722 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3520 Training loss: 1.4536 0.1689 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3520 Training loss: 1.4535 0.1669 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3520 Training loss: 1.4535 0.1659 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3520 Training loss: 1.4535 0.1675 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3520 Training loss: 1.4534 0.1664 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3520 Training loss: 1.4534 0.1697 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3520 Training loss: 1.4534 0.1669 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3520 Training loss: 1.4536 0.1675 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3520 Training loss: 1.4535 0.1726 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3520 Training loss: 1.4534 0.1678 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3520 Training loss: 1.4533 0.1622 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3520 Training loss: 1.4531 0.1723 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3520 Training loss: 1.4530 0.1853 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3520 Training loss: 1.4529 0.1688 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3520 Training loss: 1.4527 0.1726 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3520 Training loss: 1.4528 0.1703 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3520 Training loss: 1.4528 0.1727 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3520 Training loss: 1.4529 0.1721 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3520 Training loss: 1.4526 0.1808 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3520 Training loss: 1.4526 0.1673 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3520 Training loss: 1.4527 0.1728 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3520 Training loss: 1.4526 0.1731 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3520 Training loss: 1.4522 0.1729 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3520 Training loss: 1.4521 0.1728 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3520 Training loss: 1.4519 0.1747 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3520 Training loss: 1.4519 0.1724 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3520 Training loss: 1.4521 0.1727 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3520 Training loss: 1.4520 0.1725 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3520 Training loss: 1.4519 0.1688 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3520 Training loss: 1.4519 0.1744 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3520 Training loss: 1.4518 0.1675 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3520 Training loss: 1.4519 0.1725 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3520 Training loss: 1.4519 0.1747 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3520 Training loss: 1.4520 0.1700 sec/batch\n",
      "Epoch 13/20  Iteration 2113/3520 Training loss: 1.4729 0.1661 sec/batch\n",
      "Epoch 13/20  Iteration 2114/3520 Training loss: 1.4342 0.1862 sec/batch\n",
      "Epoch 13/20  Iteration 2115/3520 Training loss: 1.4357 0.1673 sec/batch\n",
      "Epoch 13/20  Iteration 2116/3520 Training loss: 1.4337 0.1724 sec/batch\n",
      "Epoch 13/20  Iteration 2117/3520 Training loss: 1.4351 0.1674 sec/batch\n",
      "Epoch 13/20  Iteration 2118/3520 Training loss: 1.4360 0.1722 sec/batch\n",
      "Epoch 13/20  Iteration 2119/3520 Training loss: 1.4348 0.1697 sec/batch\n",
      "Epoch 13/20  Iteration 2120/3520 Training loss: 1.4362 0.1723 sec/batch\n",
      "Epoch 13/20  Iteration 2121/3520 Training loss: 1.4338 0.1684 sec/batch\n",
      "Epoch 13/20  Iteration 2122/3520 Training loss: 1.4338 0.1726 sec/batch\n",
      "Epoch 13/20  Iteration 2123/3520 Training loss: 1.4351 0.1729 sec/batch\n",
      "Epoch 13/20  Iteration 2124/3520 Training loss: 1.4371 0.1729 sec/batch\n",
      "Epoch 13/20  Iteration 2125/3520 Training loss: 1.4363 0.1724 sec/batch\n",
      "Epoch 13/20  Iteration 2126/3520 Training loss: 1.4385 0.1751 sec/batch\n",
      "Epoch 13/20  Iteration 2127/3520 Training loss: 1.4383 0.1663 sec/batch\n",
      "Epoch 13/20  Iteration 2128/3520 Training loss: 1.4375 0.1671 sec/batch\n",
      "Epoch 13/20  Iteration 2129/3520 Training loss: 1.4369 0.1660 sec/batch\n",
      "Epoch 13/20  Iteration 2130/3520 Training loss: 1.4379 0.1732 sec/batch\n",
      "Epoch 13/20  Iteration 2131/3520 Training loss: 1.4371 0.1723 sec/batch\n",
      "Epoch 13/20  Iteration 2132/3520 Training loss: 1.4382 0.1649 sec/batch\n",
      "Epoch 13/20  Iteration 2133/3520 Training loss: 1.4387 0.1685 sec/batch\n",
      "Epoch 13/20  Iteration 2134/3520 Training loss: 1.4381 0.1689 sec/batch\n",
      "Epoch 13/20  Iteration 2135/3520 Training loss: 1.4391 0.1726 sec/batch\n",
      "Epoch 13/20  Iteration 2136/3520 Training loss: 1.4394 0.1727 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3520 Training loss: 1.4398 0.1725 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3520 Training loss: 1.4393 0.1666 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3520 Training loss: 1.4386 0.1662 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3520 Training loss: 1.4387 0.1683 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3520 Training loss: 1.4382 0.1619 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3520 Training loss: 1.4385 0.1721 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3520 Training loss: 1.4382 0.1731 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3520 Training loss: 1.4378 0.1727 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3520 Training loss: 1.4387 0.1725 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3520 Training loss: 1.4388 0.1727 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3520 Training loss: 1.4387 0.1667 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3520 Training loss: 1.4393 0.1664 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3520 Training loss: 1.4384 0.1723 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3520 Training loss: 1.4394 0.1726 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3520 Training loss: 1.4406 0.1678 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3520 Training loss: 1.4408 0.1679 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3520 Training loss: 1.4405 0.1681 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3520 Training loss: 1.4398 0.1686 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3520 Training loss: 1.4399 0.1677 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3520 Training loss: 1.4394 0.1730 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3520 Training loss: 1.4392 0.1657 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3520 Training loss: 1.4387 0.1650 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3520 Training loss: 1.4387 0.1728 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3520 Training loss: 1.4388 0.1730 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3520 Training loss: 1.4389 0.1669 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3520 Training loss: 1.4391 0.1671 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3520 Training loss: 1.4389 0.1724 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3520 Training loss: 1.4390 0.1720 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3520 Training loss: 1.4392 0.1675 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3520 Training loss: 1.4394 0.1708 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3520 Training loss: 1.4392 0.1719 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3520 Training loss: 1.4392 0.1695 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3520 Training loss: 1.4391 0.1725 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3520 Training loss: 1.4391 0.1700 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3520 Training loss: 1.4385 0.1727 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3520 Training loss: 1.4388 0.1689 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3520 Training loss: 1.4388 0.1667 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3520 Training loss: 1.4389 0.1665 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3520 Training loss: 1.4389 0.1665 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3520 Training loss: 1.4389 0.1729 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3520 Training loss: 1.4384 0.1678 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3520 Training loss: 1.4383 0.1726 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3520 Training loss: 1.4382 0.1724 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3520 Training loss: 1.4378 0.1692 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3520 Training loss: 1.4376 0.1717 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3520 Training loss: 1.4377 0.1727 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3520 Training loss: 1.4374 0.1723 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3520 Training loss: 1.4372 0.1674 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3520 Training loss: 1.4369 0.1731 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3520 Training loss: 1.4364 0.1768 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3520 Training loss: 1.4364 0.1737 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3520 Training loss: 1.4366 0.1674 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3520 Training loss: 1.4369 0.1735 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3520 Training loss: 1.4371 0.1684 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3520 Training loss: 1.4371 0.1731 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3520 Training loss: 1.4370 0.1730 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3520 Training loss: 1.4366 0.1733 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3520 Training loss: 1.4370 0.1671 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3520 Training loss: 1.4367 0.1640 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3520 Training loss: 1.4369 0.1687 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3520 Training loss: 1.4368 0.1675 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3520 Training loss: 1.4364 0.1724 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3520 Training loss: 1.4368 0.1683 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3520 Training loss: 1.4367 0.1737 sec/batch\n",
      "Validation loss: 1.29277 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 2201/3520 Training loss: 1.4381 0.1914 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3520 Training loss: 1.4380 0.1844 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3520 Training loss: 1.4379 0.1722 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3520 Training loss: 1.4380 0.1672 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3520 Training loss: 1.4380 0.1662 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3520 Training loss: 1.4379 0.1720 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3520 Training loss: 1.4378 0.1722 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3520 Training loss: 1.4375 0.1760 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3520 Training loss: 1.4378 0.1750 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3520 Training loss: 1.4378 0.1688 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3520 Training loss: 1.4380 0.1726 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3520 Training loss: 1.4378 0.1726 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3520 Training loss: 1.4377 0.1673 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3520 Training loss: 1.4378 0.1731 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3520 Training loss: 1.4375 0.1730 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3520 Training loss: 1.4371 0.1704 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3520 Training loss: 1.4367 0.1715 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3520 Training loss: 1.4368 0.1731 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3520 Training loss: 1.4367 0.1659 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3520 Training loss: 1.4364 0.1721 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3520 Training loss: 1.4365 0.1693 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3520 Training loss: 1.4363 0.1687 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3520 Training loss: 1.4362 0.1721 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3520 Training loss: 1.4363 0.1761 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3520 Training loss: 1.4364 0.1709 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3520 Training loss: 1.4362 0.1638 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3520 Training loss: 1.4361 0.1721 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3520 Training loss: 1.4360 0.1694 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3520 Training loss: 1.4359 0.1735 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3520 Training loss: 1.4360 0.1740 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3520 Training loss: 1.4359 0.1665 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3520 Training loss: 1.4359 0.1694 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3520 Training loss: 1.4359 0.1622 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3520 Training loss: 1.4358 0.1680 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3520 Training loss: 1.4357 0.1707 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3520 Training loss: 1.4357 0.1662 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3520 Training loss: 1.4356 0.1723 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3520 Training loss: 1.4353 0.1755 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3520 Training loss: 1.4350 0.1721 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3520 Training loss: 1.4350 0.1747 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3520 Training loss: 1.4349 0.1725 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3520 Training loss: 1.4351 0.1725 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3520 Training loss: 1.4352 0.1688 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3520 Training loss: 1.4354 0.1694 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3520 Training loss: 1.4354 0.1734 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3520 Training loss: 1.4355 0.1730 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3520 Training loss: 1.4356 0.1651 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3520 Training loss: 1.4355 0.1654 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3520 Training loss: 1.4356 0.1728 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3520 Training loss: 1.4355 0.1722 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3520 Training loss: 1.4356 0.1725 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3520 Training loss: 1.4356 0.1728 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3520 Training loss: 1.4356 0.1722 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3520 Training loss: 1.4354 0.1665 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3520 Training loss: 1.4351 0.1725 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3520 Training loss: 1.4350 0.1740 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3520 Training loss: 1.4350 0.1745 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3520 Training loss: 1.4350 0.1697 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3520 Training loss: 1.4349 0.1646 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3520 Training loss: 1.4348 0.1666 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3520 Training loss: 1.4347 0.1707 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3520 Training loss: 1.4348 0.1679 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3520 Training loss: 1.4348 0.1699 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3520 Training loss: 1.4346 0.1722 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3520 Training loss: 1.4346 0.1726 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3520 Training loss: 1.4344 0.1726 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3520 Training loss: 1.4344 0.1726 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3520 Training loss: 1.4343 0.1694 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3520 Training loss: 1.4342 0.1712 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3520 Training loss: 1.4342 0.1672 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3520 Training loss: 1.4342 0.1750 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3520 Training loss: 1.4343 0.1726 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3520 Training loss: 1.4341 0.1677 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3520 Training loss: 1.4340 0.1728 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3520 Training loss: 1.4340 0.1730 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3520 Training loss: 1.4338 0.1695 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3520 Training loss: 1.4335 0.1724 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3520 Training loss: 1.4334 0.1724 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3520 Training loss: 1.4332 0.1668 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3520 Training loss: 1.4332 0.1692 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3520 Training loss: 1.4333 0.1643 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3520 Training loss: 1.4332 0.1656 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3520 Training loss: 1.4333 0.1658 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3520 Training loss: 1.4332 0.1724 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3520 Training loss: 1.4332 0.1686 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3520 Training loss: 1.4333 0.1769 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3520 Training loss: 1.4332 0.1701 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3520 Training loss: 1.4334 0.1684 sec/batch\n",
      "Epoch 14/20  Iteration 2289/3520 Training loss: 1.4471 0.1729 sec/batch\n",
      "Epoch 14/20  Iteration 2290/3520 Training loss: 1.4135 0.1727 sec/batch\n",
      "Epoch 14/20  Iteration 2291/3520 Training loss: 1.4164 0.1729 sec/batch\n",
      "Epoch 14/20  Iteration 2292/3520 Training loss: 1.4153 0.1653 sec/batch\n",
      "Epoch 14/20  Iteration 2293/3520 Training loss: 1.4179 0.1704 sec/batch\n",
      "Epoch 14/20  Iteration 2294/3520 Training loss: 1.4195 0.1737 sec/batch\n",
      "Epoch 14/20  Iteration 2295/3520 Training loss: 1.4183 0.1694 sec/batch\n",
      "Epoch 14/20  Iteration 2296/3520 Training loss: 1.4193 0.1672 sec/batch\n",
      "Epoch 14/20  Iteration 2297/3520 Training loss: 1.4166 0.1755 sec/batch\n",
      "Epoch 14/20  Iteration 2298/3520 Training loss: 1.4170 0.1672 sec/batch\n",
      "Epoch 14/20  Iteration 2299/3520 Training loss: 1.4183 0.1748 sec/batch\n",
      "Epoch 14/20  Iteration 2300/3520 Training loss: 1.4197 0.1690 sec/batch\n",
      "Epoch 14/20  Iteration 2301/3520 Training loss: 1.4195 0.1719 sec/batch\n",
      "Epoch 14/20  Iteration 2302/3520 Training loss: 1.4214 0.1720 sec/batch\n",
      "Epoch 14/20  Iteration 2303/3520 Training loss: 1.4215 0.1833 sec/batch\n",
      "Epoch 14/20  Iteration 2304/3520 Training loss: 1.4205 0.1728 sec/batch\n",
      "Epoch 14/20  Iteration 2305/3520 Training loss: 1.4201 0.1728 sec/batch\n",
      "Epoch 14/20  Iteration 2306/3520 Training loss: 1.4212 0.1698 sec/batch\n",
      "Epoch 14/20  Iteration 2307/3520 Training loss: 1.4207 0.1729 sec/batch\n",
      "Epoch 14/20  Iteration 2308/3520 Training loss: 1.4216 0.1672 sec/batch\n",
      "Epoch 14/20  Iteration 2309/3520 Training loss: 1.4218 0.1668 sec/batch\n",
      "Epoch 14/20  Iteration 2310/3520 Training loss: 1.4211 0.1749 sec/batch\n",
      "Epoch 14/20  Iteration 2311/3520 Training loss: 1.4221 0.1721 sec/batch\n",
      "Epoch 14/20  Iteration 2312/3520 Training loss: 1.4226 0.1690 sec/batch\n",
      "Epoch 14/20  Iteration 2313/3520 Training loss: 1.4234 0.1662 sec/batch\n",
      "Epoch 14/20  Iteration 2314/3520 Training loss: 1.4229 0.1693 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3520 Training loss: 1.4223 0.1669 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3520 Training loss: 1.4226 0.1671 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3520 Training loss: 1.4215 0.1680 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3520 Training loss: 1.4218 0.1687 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3520 Training loss: 1.4213 0.1723 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3520 Training loss: 1.4216 0.1709 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3520 Training loss: 1.4221 0.1725 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3520 Training loss: 1.4221 0.1680 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3520 Training loss: 1.4219 0.1694 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3520 Training loss: 1.4222 0.1724 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3520 Training loss: 1.4213 0.1668 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3520 Training loss: 1.4223 0.1727 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3520 Training loss: 1.4234 0.1698 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3520 Training loss: 1.4234 0.1722 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3520 Training loss: 1.4232 0.1724 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3520 Training loss: 1.4226 0.1685 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3520 Training loss: 1.4226 0.1757 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3520 Training loss: 1.4221 0.1667 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3520 Training loss: 1.4218 0.1722 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3520 Training loss: 1.4217 0.1728 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3520 Training loss: 1.4216 0.1673 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3520 Training loss: 1.4217 0.1730 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3520 Training loss: 1.4218 0.1625 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3520 Training loss: 1.4216 0.1664 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3520 Training loss: 1.4215 0.1673 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3520 Training loss: 1.4216 0.1726 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3520 Training loss: 1.4217 0.1721 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3520 Training loss: 1.4218 0.1739 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3520 Training loss: 1.4216 0.1728 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3520 Training loss: 1.4213 0.1685 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3520 Training loss: 1.4208 0.1723 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3520 Training loss: 1.4208 0.1725 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3520 Training loss: 1.4203 0.1724 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3520 Training loss: 1.4206 0.1721 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3520 Training loss: 1.4205 0.1693 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3520 Training loss: 1.4206 0.1755 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3520 Training loss: 1.4207 0.1681 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3520 Training loss: 1.4206 0.1729 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3520 Training loss: 1.4202 0.1675 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3520 Training loss: 1.4202 0.1681 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3520 Training loss: 1.4202 0.1656 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3520 Training loss: 1.4197 0.1727 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3520 Training loss: 1.4195 0.1666 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3520 Training loss: 1.4196 0.1663 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3520 Training loss: 1.4194 0.1723 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3520 Training loss: 1.4191 0.1653 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3520 Training loss: 1.4188 0.1723 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3520 Training loss: 1.4184 0.1729 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3520 Training loss: 1.4183 0.1667 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3520 Training loss: 1.4185 0.1730 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3520 Training loss: 1.4188 0.1727 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3520 Training loss: 1.4193 0.1683 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3520 Training loss: 1.4193 0.1730 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3520 Training loss: 1.4192 0.1673 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3520 Training loss: 1.4188 0.1730 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3520 Training loss: 1.4193 0.1728 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3520 Training loss: 1.4190 0.1729 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3520 Training loss: 1.4192 0.1748 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3520 Training loss: 1.4191 0.1674 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3520 Training loss: 1.4187 0.1725 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3520 Training loss: 1.4191 0.1641 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3520 Training loss: 1.4190 0.1764 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3520 Training loss: 1.4192 0.1732 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3520 Training loss: 1.4190 0.1726 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3520 Training loss: 1.4189 0.1722 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3520 Training loss: 1.4191 0.1685 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3520 Training loss: 1.4190 0.1641 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3520 Training loss: 1.4188 0.1740 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3520 Training loss: 1.4186 0.1728 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3520 Training loss: 1.4182 0.1668 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3520 Training loss: 1.4185 0.1732 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3520 Training loss: 1.4185 0.1727 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3520 Training loss: 1.4187 0.1682 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3520 Training loss: 1.4185 0.1677 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3520 Training loss: 1.4184 0.1671 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3520 Training loss: 1.4185 0.1725 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3520 Training loss: 1.4181 0.1728 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3520 Training loss: 1.4176 0.1715 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3520 Training loss: 1.4172 0.1688 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3520 Training loss: 1.4174 0.1686 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3520 Training loss: 1.4173 0.1692 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3520 Training loss: 1.4170 0.1647 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3520 Training loss: 1.4170 0.1724 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3520 Training loss: 1.4169 0.1729 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3520 Training loss: 1.4168 0.1753 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3520 Training loss: 1.4169 0.1728 sec/batch\n",
      "Validation loss: 1.2799 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 2401/3520 Training loss: 1.4180 0.1690 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3520 Training loss: 1.4178 0.1719 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3520 Training loss: 1.4178 0.1681 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3520 Training loss: 1.4177 0.1722 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3520 Training loss: 1.4175 0.1733 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3520 Training loss: 1.4176 0.1721 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3520 Training loss: 1.4176 0.1658 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3520 Training loss: 1.4176 0.1726 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3520 Training loss: 1.4176 0.1721 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3520 Training loss: 1.4176 0.1723 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3520 Training loss: 1.4177 0.1654 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3520 Training loss: 1.4177 0.1727 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3520 Training loss: 1.4177 0.1722 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3520 Training loss: 1.4173 0.1643 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3520 Training loss: 1.4171 0.1673 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3520 Training loss: 1.4171 0.1680 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3520 Training loss: 1.4171 0.1725 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3520 Training loss: 1.4173 0.1723 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3520 Training loss: 1.4173 0.1670 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3520 Training loss: 1.4175 0.1718 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3520 Training loss: 1.4175 0.1724 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3520 Training loss: 1.4176 0.1695 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3520 Training loss: 1.4177 0.1707 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3520 Training loss: 1.4176 0.1727 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3520 Training loss: 1.4177 0.1674 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3520 Training loss: 1.4176 0.1791 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3520 Training loss: 1.4177 0.1684 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3520 Training loss: 1.4179 0.1670 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3520 Training loss: 1.4179 0.1719 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3520 Training loss: 1.4178 0.1720 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3520 Training loss: 1.4175 0.1719 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3520 Training loss: 1.4174 0.1724 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3520 Training loss: 1.4175 0.1674 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3520 Training loss: 1.4175 0.1725 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3520 Training loss: 1.4174 0.1678 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3520 Training loss: 1.4173 0.1759 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3520 Training loss: 1.4173 0.1751 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3520 Training loss: 1.4175 0.1728 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3520 Training loss: 1.4173 0.1730 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3520 Training loss: 1.4172 0.1806 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3520 Training loss: 1.4171 0.1688 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3520 Training loss: 1.4170 0.1731 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3520 Training loss: 1.4169 0.1672 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3520 Training loss: 1.4168 0.1647 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3520 Training loss: 1.4167 0.1725 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3520 Training loss: 1.4168 0.1654 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3520 Training loss: 1.4168 0.1694 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3520 Training loss: 1.4168 0.1681 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3520 Training loss: 1.4166 0.1729 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3520 Training loss: 1.4166 0.1678 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3520 Training loss: 1.4167 0.1719 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3520 Training loss: 1.4165 0.1726 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3520 Training loss: 1.4161 0.1667 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3520 Training loss: 1.4159 0.1750 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3520 Training loss: 1.4157 0.1685 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3520 Training loss: 1.4157 0.1731 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3520 Training loss: 1.4159 0.1727 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3520 Training loss: 1.4158 0.1685 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3520 Training loss: 1.4158 0.1625 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3520 Training loss: 1.4158 0.1724 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3520 Training loss: 1.4158 0.1668 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3520 Training loss: 1.4159 0.1731 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3520 Training loss: 1.4159 0.1669 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3520 Training loss: 1.4161 0.1696 sec/batch\n",
      "Epoch 15/20  Iteration 2465/3520 Training loss: 1.4373 0.1642 sec/batch\n",
      "Epoch 15/20  Iteration 2466/3520 Training loss: 1.4041 0.1725 sec/batch\n",
      "Epoch 15/20  Iteration 2467/3520 Training loss: 1.4047 0.1723 sec/batch\n",
      "Epoch 15/20  Iteration 2468/3520 Training loss: 1.4038 0.1717 sec/batch\n",
      "Epoch 15/20  Iteration 2469/3520 Training loss: 1.4070 0.1731 sec/batch\n",
      "Epoch 15/20  Iteration 2470/3520 Training loss: 1.4052 0.1724 sec/batch\n",
      "Epoch 15/20  Iteration 2471/3520 Training loss: 1.4025 0.1846 sec/batch\n",
      "Epoch 15/20  Iteration 2472/3520 Training loss: 1.4023 0.1730 sec/batch\n",
      "Epoch 15/20  Iteration 2473/3520 Training loss: 1.4016 0.1728 sec/batch\n",
      "Epoch 15/20  Iteration 2474/3520 Training loss: 1.4030 0.1721 sec/batch\n",
      "Epoch 15/20  Iteration 2475/3520 Training loss: 1.4041 0.1739 sec/batch\n",
      "Epoch 15/20  Iteration 2476/3520 Training loss: 1.4048 0.1726 sec/batch\n",
      "Epoch 15/20  Iteration 2477/3520 Training loss: 1.4040 0.1724 sec/batch\n",
      "Epoch 15/20  Iteration 2478/3520 Training loss: 1.4051 0.1722 sec/batch\n",
      "Epoch 15/20  Iteration 2479/3520 Training loss: 1.4051 0.1659 sec/batch\n",
      "Epoch 15/20  Iteration 2480/3520 Training loss: 1.4045 0.1726 sec/batch\n",
      "Epoch 15/20  Iteration 2481/3520 Training loss: 1.4035 0.1718 sec/batch\n",
      "Epoch 15/20  Iteration 2482/3520 Training loss: 1.4042 0.1722 sec/batch\n",
      "Epoch 15/20  Iteration 2483/3520 Training loss: 1.4035 0.1754 sec/batch\n",
      "Epoch 15/20  Iteration 2484/3520 Training loss: 1.4050 0.1679 sec/batch\n",
      "Epoch 15/20  Iteration 2485/3520 Training loss: 1.4054 0.1671 sec/batch\n",
      "Epoch 15/20  Iteration 2486/3520 Training loss: 1.4052 0.1702 sec/batch\n",
      "Epoch 15/20  Iteration 2487/3520 Training loss: 1.4062 0.1664 sec/batch\n",
      "Epoch 15/20  Iteration 2488/3520 Training loss: 1.4066 0.1726 sec/batch\n",
      "Epoch 15/20  Iteration 2489/3520 Training loss: 1.4072 0.1671 sec/batch\n",
      "Epoch 15/20  Iteration 2490/3520 Training loss: 1.4068 0.1726 sec/batch\n",
      "Epoch 15/20  Iteration 2491/3520 Training loss: 1.4059 0.1723 sec/batch\n",
      "Epoch 15/20  Iteration 2492/3520 Training loss: 1.4059 0.1649 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3520 Training loss: 1.4054 0.1728 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3520 Training loss: 1.4060 0.1728 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3520 Training loss: 1.4056 0.1667 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3520 Training loss: 1.4054 0.1669 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3520 Training loss: 1.4063 0.1724 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3520 Training loss: 1.4059 0.1783 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3520 Training loss: 1.4058 0.1621 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3520 Training loss: 1.4063 0.1659 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3520 Training loss: 1.4052 0.1720 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3520 Training loss: 1.4062 0.1689 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3520 Training loss: 1.4072 0.1729 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3520 Training loss: 1.4073 0.1681 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3520 Training loss: 1.4071 0.1728 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3520 Training loss: 1.4067 0.1676 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3520 Training loss: 1.4070 0.1674 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3520 Training loss: 1.4063 0.1725 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3520 Training loss: 1.4061 0.1647 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3520 Training loss: 1.4058 0.1697 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3520 Training loss: 1.4056 0.1658 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3520 Training loss: 1.4056 0.1848 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3520 Training loss: 1.4057 0.1701 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3520 Training loss: 1.4059 0.1724 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3520 Training loss: 1.4058 0.1722 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3520 Training loss: 1.4059 0.1633 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3520 Training loss: 1.4061 0.1649 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3520 Training loss: 1.4060 0.1726 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3520 Training loss: 1.4058 0.1658 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3520 Training loss: 1.4058 0.1722 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3520 Training loss: 1.4056 0.1719 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3520 Training loss: 1.4056 0.1638 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3520 Training loss: 1.4050 0.1723 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3520 Training loss: 1.4052 0.1731 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3520 Training loss: 1.4051 0.1693 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3520 Training loss: 1.4052 0.1699 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3520 Training loss: 1.4053 0.1660 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3520 Training loss: 1.4051 0.1752 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3520 Training loss: 1.4048 0.1735 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3520 Training loss: 1.4048 0.1687 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3520 Training loss: 1.4048 0.1637 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3520 Training loss: 1.4043 0.1661 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3520 Training loss: 1.4040 0.1727 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3520 Training loss: 1.4042 0.1689 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3520 Training loss: 1.4040 0.1652 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3520 Training loss: 1.4039 0.1724 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3520 Training loss: 1.4036 0.1701 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3520 Training loss: 1.4030 0.1659 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3520 Training loss: 1.4030 0.1641 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3520 Training loss: 1.4031 0.1728 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3520 Training loss: 1.4034 0.1726 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3520 Training loss: 1.4038 0.1710 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3520 Training loss: 1.4038 0.1672 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3520 Training loss: 1.4036 0.1723 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3520 Training loss: 1.4035 0.1721 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3520 Training loss: 1.4039 0.1684 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3520 Training loss: 1.4037 0.1708 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3520 Training loss: 1.4039 0.1688 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3520 Training loss: 1.4037 0.1685 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3520 Training loss: 1.4033 0.1665 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3520 Training loss: 1.4037 0.1716 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3520 Training loss: 1.4036 0.1666 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3520 Training loss: 1.4039 0.1732 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3520 Training loss: 1.4038 0.1675 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3520 Training loss: 1.4037 0.1659 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3520 Training loss: 1.4040 0.1755 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3520 Training loss: 1.4038 0.1679 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3520 Training loss: 1.4036 0.1712 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3520 Training loss: 1.4035 0.1727 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3520 Training loss: 1.4032 0.1749 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3520 Training loss: 1.4034 0.1728 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3520 Training loss: 1.4035 0.1733 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3520 Training loss: 1.4036 0.1698 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3520 Training loss: 1.4034 0.1691 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3520 Training loss: 1.4033 0.1653 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3520 Training loss: 1.4035 0.1732 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3520 Training loss: 1.4031 0.1729 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3520 Training loss: 1.4025 0.1679 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3520 Training loss: 1.4022 0.1683 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3520 Training loss: 1.4025 0.1675 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3520 Training loss: 1.4023 0.1658 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3520 Training loss: 1.4020 0.1722 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3520 Training loss: 1.4020 0.1685 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3520 Training loss: 1.4018 0.1703 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3520 Training loss: 1.4018 0.1658 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3520 Training loss: 1.4019 0.1717 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3520 Training loss: 1.4020 0.1722 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3520 Training loss: 1.4018 0.1726 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3520 Training loss: 1.4016 0.1729 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3520 Training loss: 1.4016 0.1732 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3520 Training loss: 1.4015 0.1676 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3520 Training loss: 1.4016 0.1730 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3520 Training loss: 1.4016 0.1724 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3520 Training loss: 1.4016 0.1702 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3520 Training loss: 1.4016 0.1722 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3520 Training loss: 1.4016 0.1724 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3520 Training loss: 1.4016 0.1686 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3520 Training loss: 1.4016 0.1828 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3520 Training loss: 1.4015 0.1692 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3520 Training loss: 1.4013 0.1733 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3520 Training loss: 1.4010 0.1739 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3520 Training loss: 1.4009 0.1728 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3520 Training loss: 1.4009 0.1733 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3520 Training loss: 1.4010 0.1667 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3520 Training loss: 1.4010 0.1721 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3520 Training loss: 1.4013 0.1729 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3520 Training loss: 1.4012 0.1691 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3520 Training loss: 1.4013 0.1730 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3520 Training loss: 1.4014 0.1648 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3520 Training loss: 1.4013 0.1673 sec/batch\n",
      "Validation loss: 1.26374 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 2601/3520 Training loss: 1.4021 0.1920 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3520 Training loss: 1.4021 0.1785 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3520 Training loss: 1.4022 0.1732 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3520 Training loss: 1.4023 0.1666 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3520 Training loss: 1.4023 0.1655 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3520 Training loss: 1.4022 0.1690 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3520 Training loss: 1.4019 0.1656 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3520 Training loss: 1.4019 0.1637 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3520 Training loss: 1.4020 0.1726 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3520 Training loss: 1.4020 0.1608 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3520 Training loss: 1.4019 0.1716 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3520 Training loss: 1.4018 0.1677 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3520 Training loss: 1.4019 0.1723 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3520 Training loss: 1.4021 0.1747 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3520 Training loss: 1.4019 0.1721 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3520 Training loss: 1.4018 0.1731 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3520 Training loss: 1.4017 0.1711 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3520 Training loss: 1.4015 0.1723 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3520 Training loss: 1.4014 0.1685 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3520 Training loss: 1.4014 0.1729 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3520 Training loss: 1.4013 0.1678 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3520 Training loss: 1.4013 0.1657 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3520 Training loss: 1.4014 0.1722 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3520 Training loss: 1.4015 0.1659 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3520 Training loss: 1.4013 0.1657 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3520 Training loss: 1.4013 0.1681 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3520 Training loss: 1.4014 0.1763 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3520 Training loss: 1.4012 0.1702 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3520 Training loss: 1.4009 0.1724 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3520 Training loss: 1.4007 0.1694 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3520 Training loss: 1.4006 0.1686 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3520 Training loss: 1.4007 0.1723 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3520 Training loss: 1.4007 0.1720 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3520 Training loss: 1.4007 0.1725 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3520 Training loss: 1.4007 0.1674 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3520 Training loss: 1.4007 0.1678 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3520 Training loss: 1.4006 0.1755 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3520 Training loss: 1.4007 0.1732 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3520 Training loss: 1.4006 0.1725 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3520 Training loss: 1.4008 0.1719 sec/batch\n",
      "Epoch 16/20  Iteration 2641/3520 Training loss: 1.4219 0.1722 sec/batch\n",
      "Epoch 16/20  Iteration 2642/3520 Training loss: 1.3894 0.1685 sec/batch\n",
      "Epoch 16/20  Iteration 2643/3520 Training loss: 1.3887 0.1691 sec/batch\n",
      "Epoch 16/20  Iteration 2644/3520 Training loss: 1.3878 0.1723 sec/batch\n",
      "Epoch 16/20  Iteration 2645/3520 Training loss: 1.3926 0.1659 sec/batch\n",
      "Epoch 16/20  Iteration 2646/3520 Training loss: 1.3925 0.1650 sec/batch\n",
      "Epoch 16/20  Iteration 2647/3520 Training loss: 1.3902 0.1729 sec/batch\n",
      "Epoch 16/20  Iteration 2648/3520 Training loss: 1.3905 0.1735 sec/batch\n",
      "Epoch 16/20  Iteration 2649/3520 Training loss: 1.3894 0.1684 sec/batch\n",
      "Epoch 16/20  Iteration 2650/3520 Training loss: 1.3881 0.1722 sec/batch\n",
      "Epoch 16/20  Iteration 2651/3520 Training loss: 1.3896 0.1729 sec/batch\n",
      "Epoch 16/20  Iteration 2652/3520 Training loss: 1.3904 0.1718 sec/batch\n",
      "Epoch 16/20  Iteration 2653/3520 Training loss: 1.3901 0.1663 sec/batch\n",
      "Epoch 16/20  Iteration 2654/3520 Training loss: 1.3923 0.1727 sec/batch\n",
      "Epoch 16/20  Iteration 2655/3520 Training loss: 1.3925 0.1649 sec/batch\n",
      "Epoch 16/20  Iteration 2656/3520 Training loss: 1.3911 0.1691 sec/batch\n",
      "Epoch 16/20  Iteration 2657/3520 Training loss: 1.3908 0.1698 sec/batch\n",
      "Epoch 16/20  Iteration 2658/3520 Training loss: 1.3917 0.1721 sec/batch\n",
      "Epoch 16/20  Iteration 2659/3520 Training loss: 1.3908 0.1750 sec/batch\n",
      "Epoch 16/20  Iteration 2660/3520 Training loss: 1.3919 0.1717 sec/batch\n",
      "Epoch 16/20  Iteration 2661/3520 Training loss: 1.3924 0.1700 sec/batch\n",
      "Epoch 16/20  Iteration 2662/3520 Training loss: 1.3921 0.1731 sec/batch\n",
      "Epoch 16/20  Iteration 2663/3520 Training loss: 1.3926 0.1662 sec/batch\n",
      "Epoch 16/20  Iteration 2664/3520 Training loss: 1.3929 0.1672 sec/batch\n",
      "Epoch 16/20  Iteration 2665/3520 Training loss: 1.3934 0.1794 sec/batch\n",
      "Epoch 16/20  Iteration 2666/3520 Training loss: 1.3929 0.1685 sec/batch\n",
      "Epoch 16/20  Iteration 2667/3520 Training loss: 1.3922 0.1650 sec/batch\n",
      "Epoch 16/20  Iteration 2668/3520 Training loss: 1.3924 0.1729 sec/batch\n",
      "Epoch 16/20  Iteration 2669/3520 Training loss: 1.3920 0.1728 sec/batch\n",
      "Epoch 16/20  Iteration 2670/3520 Training loss: 1.3925 0.1726 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3520 Training loss: 1.3920 0.1726 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3520 Training loss: 1.3917 0.1677 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3520 Training loss: 1.3922 0.1724 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3520 Training loss: 1.3917 0.1742 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3520 Training loss: 1.3916 0.1655 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3520 Training loss: 1.3923 0.1675 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3520 Training loss: 1.3913 0.1721 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3520 Training loss: 1.3920 0.1733 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3520 Training loss: 1.3929 0.1630 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3520 Training loss: 1.3930 0.1659 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3520 Training loss: 1.3930 0.1682 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3520 Training loss: 1.3926 0.1733 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3520 Training loss: 1.3924 0.1712 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3520 Training loss: 1.3918 0.1642 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3520 Training loss: 1.3915 0.1701 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3520 Training loss: 1.3913 0.1750 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3520 Training loss: 1.3911 0.1673 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3520 Training loss: 1.3914 0.1725 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3520 Training loss: 1.3915 0.1682 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3520 Training loss: 1.3915 0.1685 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3520 Training loss: 1.3916 0.1689 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3520 Training loss: 1.3917 0.1727 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3520 Training loss: 1.3921 0.1693 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3520 Training loss: 1.3920 0.1725 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3520 Training loss: 1.3917 0.1726 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3520 Training loss: 1.3917 0.1673 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3520 Training loss: 1.3916 0.1729 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3520 Training loss: 1.3916 0.1686 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3520 Training loss: 1.3912 0.1679 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3520 Training loss: 1.3914 0.1680 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3520 Training loss: 1.3914 0.1644 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3520 Training loss: 1.3916 0.1732 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3520 Training loss: 1.3915 0.1670 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3520 Training loss: 1.3915 0.1664 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3520 Training loss: 1.3910 0.1639 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3520 Training loss: 1.3910 0.1691 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3520 Training loss: 1.3909 0.1678 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3520 Training loss: 1.3907 0.1729 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3520 Training loss: 1.3906 0.1726 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3520 Training loss: 1.3906 0.1681 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3520 Training loss: 1.3904 0.1733 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3520 Training loss: 1.3902 0.1666 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3520 Training loss: 1.3900 0.1728 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3520 Training loss: 1.3893 0.1712 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3520 Training loss: 1.3893 0.1715 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3520 Training loss: 1.3893 0.1724 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3520 Training loss: 1.3895 0.1757 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3520 Training loss: 1.3900 0.1670 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3520 Training loss: 1.3898 0.1662 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3520 Training loss: 1.3896 0.1681 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3520 Training loss: 1.3893 0.1633 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3520 Training loss: 1.3897 0.1768 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3520 Training loss: 1.3895 0.1681 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3520 Training loss: 1.3897 0.1724 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3520 Training loss: 1.3896 0.1674 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3520 Training loss: 1.3892 0.1672 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3520 Training loss: 1.3896 0.1683 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3520 Training loss: 1.3896 0.1731 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3520 Training loss: 1.3898 0.1729 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3520 Training loss: 1.3895 0.1676 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3520 Training loss: 1.3895 0.1726 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3520 Training loss: 1.3897 0.1725 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3520 Training loss: 1.3896 0.1755 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3520 Training loss: 1.3894 0.1827 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3520 Training loss: 1.3893 0.1721 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3520 Training loss: 1.3891 0.1668 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3520 Training loss: 1.3894 0.1722 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3520 Training loss: 1.3895 0.1700 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3520 Training loss: 1.3896 0.1736 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3520 Training loss: 1.3894 0.1736 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3520 Training loss: 1.3892 0.1726 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3520 Training loss: 1.3894 0.1732 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3520 Training loss: 1.3891 0.1731 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3520 Training loss: 1.3886 0.1727 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3520 Training loss: 1.3882 0.1729 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3520 Training loss: 1.3883 0.1732 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3520 Training loss: 1.3881 0.1726 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3520 Training loss: 1.3878 0.1721 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3520 Training loss: 1.3878 0.1735 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3520 Training loss: 1.3876 0.1672 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3520 Training loss: 1.3877 0.1758 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3520 Training loss: 1.3878 0.1677 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3520 Training loss: 1.3879 0.1694 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3520 Training loss: 1.3877 0.1682 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3520 Training loss: 1.3876 0.1704 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3520 Training loss: 1.3875 0.1673 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3520 Training loss: 1.3874 0.1662 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3520 Training loss: 1.3876 0.1753 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3520 Training loss: 1.3876 0.1675 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3520 Training loss: 1.3875 0.1613 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3520 Training loss: 1.3876 0.1678 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3520 Training loss: 1.3876 0.1724 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3520 Training loss: 1.3876 0.1726 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3520 Training loss: 1.3877 0.1691 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3520 Training loss: 1.3876 0.1727 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3520 Training loss: 1.3872 0.1659 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3520 Training loss: 1.3870 0.1720 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3520 Training loss: 1.3870 0.1678 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3520 Training loss: 1.3870 0.1723 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3520 Training loss: 1.3871 0.1729 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3520 Training loss: 1.3871 0.1743 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3520 Training loss: 1.3874 0.1686 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3520 Training loss: 1.3873 0.1686 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3520 Training loss: 1.3874 0.1673 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3520 Training loss: 1.3875 0.1729 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3520 Training loss: 1.3875 0.1714 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3520 Training loss: 1.3875 0.1647 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3520 Training loss: 1.3875 0.1677 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3520 Training loss: 1.3876 0.1725 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3520 Training loss: 1.3877 0.1704 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3520 Training loss: 1.3877 0.1693 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3520 Training loss: 1.3876 0.1729 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3520 Training loss: 1.3873 0.1730 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3520 Training loss: 1.3872 0.1723 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3520 Training loss: 1.3873 0.1722 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3520 Training loss: 1.3872 0.1726 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3520 Training loss: 1.3871 0.1669 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3520 Training loss: 1.3871 0.1691 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3520 Training loss: 1.3870 0.1680 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3520 Training loss: 1.3873 0.1684 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3520 Training loss: 1.3872 0.1729 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3520 Training loss: 1.3871 0.1670 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3520 Training loss: 1.3870 0.1731 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3520 Training loss: 1.3868 0.1666 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3520 Training loss: 1.3867 0.1685 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3520 Training loss: 1.3867 0.1674 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3520 Training loss: 1.3865 0.1666 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3520 Training loss: 1.3866 0.1685 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3520 Training loss: 1.3867 0.1672 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3520 Training loss: 1.3868 0.1681 sec/batch\n",
      "Validation loss: 1.25029 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 2801/3520 Training loss: 1.3872 0.1752 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3520 Training loss: 1.3873 0.1660 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3520 Training loss: 1.3874 0.1644 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3520 Training loss: 1.3872 0.1721 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3520 Training loss: 1.3869 0.1725 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3520 Training loss: 1.3868 0.1672 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3520 Training loss: 1.3866 0.1724 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3520 Training loss: 1.3866 0.1728 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3520 Training loss: 1.3867 0.1724 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3520 Training loss: 1.3867 0.1695 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3520 Training loss: 1.3867 0.1723 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3520 Training loss: 1.3867 0.1695 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3520 Training loss: 1.3867 0.1731 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3520 Training loss: 1.3868 0.1721 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3520 Training loss: 1.3867 0.1720 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3520 Training loss: 1.3870 0.1654 sec/batch\n",
      "Epoch 17/20  Iteration 2817/3520 Training loss: 1.4139 0.1722 sec/batch\n",
      "Epoch 17/20  Iteration 2818/3520 Training loss: 1.3752 0.1650 sec/batch\n",
      "Epoch 17/20  Iteration 2819/3520 Training loss: 1.3772 0.1724 sec/batch\n",
      "Epoch 17/20  Iteration 2820/3520 Training loss: 1.3748 0.1724 sec/batch\n",
      "Epoch 17/20  Iteration 2821/3520 Training loss: 1.3781 0.1761 sec/batch\n",
      "Epoch 17/20  Iteration 2822/3520 Training loss: 1.3793 0.1662 sec/batch\n",
      "Epoch 17/20  Iteration 2823/3520 Training loss: 1.3769 0.1809 sec/batch\n",
      "Epoch 17/20  Iteration 2824/3520 Training loss: 1.3772 0.1667 sec/batch\n",
      "Epoch 17/20  Iteration 2825/3520 Training loss: 1.3743 0.1724 sec/batch\n",
      "Epoch 17/20  Iteration 2826/3520 Training loss: 1.3743 0.1720 sec/batch\n",
      "Epoch 17/20  Iteration 2827/3520 Training loss: 1.3755 0.1680 sec/batch\n",
      "Epoch 17/20  Iteration 2828/3520 Training loss: 1.3764 0.1678 sec/batch\n",
      "Epoch 17/20  Iteration 2829/3520 Training loss: 1.3755 0.1671 sec/batch\n",
      "Epoch 17/20  Iteration 2830/3520 Training loss: 1.3771 0.1678 sec/batch\n",
      "Epoch 17/20  Iteration 2831/3520 Training loss: 1.3769 0.1681 sec/batch\n",
      "Epoch 17/20  Iteration 2832/3520 Training loss: 1.3763 0.1726 sec/batch\n",
      "Epoch 17/20  Iteration 2833/3520 Training loss: 1.3755 0.1730 sec/batch\n",
      "Epoch 17/20  Iteration 2834/3520 Training loss: 1.3765 0.1730 sec/batch\n",
      "Epoch 17/20  Iteration 2835/3520 Training loss: 1.3759 0.1654 sec/batch\n",
      "Epoch 17/20  Iteration 2836/3520 Training loss: 1.3772 0.1666 sec/batch\n",
      "Epoch 17/20  Iteration 2837/3520 Training loss: 1.3778 0.1733 sec/batch\n",
      "Epoch 17/20  Iteration 2838/3520 Training loss: 1.3774 0.1737 sec/batch\n",
      "Epoch 17/20  Iteration 2839/3520 Training loss: 1.3778 0.1730 sec/batch\n",
      "Epoch 17/20  Iteration 2840/3520 Training loss: 1.3783 0.1681 sec/batch\n",
      "Epoch 17/20  Iteration 2841/3520 Training loss: 1.3789 0.1718 sec/batch\n",
      "Epoch 17/20  Iteration 2842/3520 Training loss: 1.3786 0.1724 sec/batch\n",
      "Epoch 17/20  Iteration 2843/3520 Training loss: 1.3780 0.1723 sec/batch\n",
      "Epoch 17/20  Iteration 2844/3520 Training loss: 1.3780 0.1702 sec/batch\n",
      "Epoch 17/20  Iteration 2845/3520 Training loss: 1.3773 0.1680 sec/batch\n",
      "Epoch 17/20  Iteration 2846/3520 Training loss: 1.3778 0.1664 sec/batch\n",
      "Epoch 17/20  Iteration 2847/3520 Training loss: 1.3775 0.1726 sec/batch\n",
      "Epoch 17/20  Iteration 2848/3520 Training loss: 1.3774 0.1724 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3520 Training loss: 1.3783 0.1729 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3520 Training loss: 1.3778 0.1668 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3520 Training loss: 1.3776 0.1656 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3520 Training loss: 1.3781 0.1666 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3520 Training loss: 1.3775 0.1678 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3520 Training loss: 1.3784 0.1722 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3520 Training loss: 1.3795 0.1702 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3520 Training loss: 1.3796 0.1655 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3520 Training loss: 1.3797 0.1685 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3520 Training loss: 1.3792 0.1698 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3520 Training loss: 1.3793 0.1759 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3520 Training loss: 1.3785 0.1684 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3520 Training loss: 1.3784 0.1721 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3520 Training loss: 1.3781 0.1732 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3520 Training loss: 1.3781 0.1725 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3520 Training loss: 1.3780 0.1670 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3520 Training loss: 1.3781 0.1809 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3520 Training loss: 1.3782 0.1710 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3520 Training loss: 1.3779 0.1718 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3520 Training loss: 1.3779 0.1731 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3520 Training loss: 1.3781 0.1655 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3520 Training loss: 1.3783 0.1728 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3520 Training loss: 1.3781 0.1704 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3520 Training loss: 1.3782 0.1738 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3520 Training loss: 1.3778 0.1654 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3520 Training loss: 1.3778 0.1675 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3520 Training loss: 1.3774 0.1673 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3520 Training loss: 1.3776 0.1747 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3520 Training loss: 1.3775 0.1670 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3520 Training loss: 1.3777 0.1690 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3520 Training loss: 1.3778 0.1683 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3520 Training loss: 1.3779 0.1665 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3520 Training loss: 1.3775 0.1733 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3520 Training loss: 1.3775 0.1728 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3520 Training loss: 1.3775 0.1740 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3520 Training loss: 1.3771 0.1688 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3520 Training loss: 1.3768 0.1686 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3520 Training loss: 1.3768 0.1724 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3520 Training loss: 1.3767 0.1728 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3520 Training loss: 1.3765 0.1694 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3520 Training loss: 1.3763 0.1651 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3520 Training loss: 1.3758 0.1698 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3520 Training loss: 1.3758 0.1833 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3520 Training loss: 1.3760 0.1727 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3520 Training loss: 1.3762 0.1662 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3520 Training loss: 1.3767 0.1728 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3520 Training loss: 1.3766 0.1721 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3520 Training loss: 1.3766 0.1684 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3520 Training loss: 1.3763 0.1724 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3520 Training loss: 1.3767 0.1753 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3520 Training loss: 1.3766 0.1725 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3520 Training loss: 1.3767 0.1659 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3520 Training loss: 1.3766 0.1727 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3520 Training loss: 1.3762 0.1732 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3520 Training loss: 1.3766 0.1696 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3520 Training loss: 1.3765 0.1756 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3520 Training loss: 1.3767 0.1759 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3520 Training loss: 1.3765 0.1722 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3520 Training loss: 1.3766 0.1655 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3520 Training loss: 1.3768 0.1653 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3520 Training loss: 1.3765 0.1667 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3520 Training loss: 1.3764 0.1726 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3520 Training loss: 1.3763 0.1670 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3520 Training loss: 1.3760 0.1723 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3520 Training loss: 1.3762 0.1666 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3520 Training loss: 1.3762 0.1657 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3520 Training loss: 1.3763 0.1725 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3520 Training loss: 1.3761 0.1675 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3520 Training loss: 1.3760 0.1729 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3520 Training loss: 1.3760 0.1691 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3520 Training loss: 1.3758 0.1725 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3520 Training loss: 1.3753 0.1724 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3520 Training loss: 1.3749 0.1690 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3520 Training loss: 1.3750 0.1671 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3520 Training loss: 1.3749 0.1759 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3520 Training loss: 1.3747 0.1666 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3520 Training loss: 1.3746 0.1738 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3520 Training loss: 1.3745 0.1650 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3520 Training loss: 1.3745 0.1731 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3520 Training loss: 1.3746 0.1737 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3520 Training loss: 1.3748 0.1727 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3520 Training loss: 1.3747 0.1644 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3520 Training loss: 1.3745 0.1729 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3520 Training loss: 1.3744 0.1688 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3520 Training loss: 1.3742 0.1733 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3520 Training loss: 1.3743 0.1731 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3520 Training loss: 1.3742 0.1755 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3520 Training loss: 1.3742 0.1758 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3520 Training loss: 1.3742 0.1673 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3520 Training loss: 1.3742 0.1754 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3520 Training loss: 1.3741 0.1735 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3520 Training loss: 1.3742 0.1726 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3520 Training loss: 1.3741 0.1713 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3520 Training loss: 1.3738 0.1682 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3520 Training loss: 1.3736 0.1667 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3520 Training loss: 1.3736 0.1785 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3520 Training loss: 1.3736 0.1678 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3520 Training loss: 1.3738 0.1684 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3520 Training loss: 1.3738 0.1720 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3520 Training loss: 1.3742 0.1697 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3520 Training loss: 1.3741 0.1733 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3520 Training loss: 1.3743 0.1723 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3520 Training loss: 1.3744 0.1721 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3520 Training loss: 1.3743 0.1727 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3520 Training loss: 1.3744 0.1719 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3520 Training loss: 1.3743 0.1731 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3520 Training loss: 1.3744 0.1728 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3520 Training loss: 1.3745 0.1733 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3520 Training loss: 1.3745 0.1707 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3520 Training loss: 1.3743 0.1720 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3520 Training loss: 1.3741 0.1712 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3520 Training loss: 1.3740 0.1696 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3520 Training loss: 1.3742 0.1721 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3520 Training loss: 1.3742 0.1694 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3520 Training loss: 1.3742 0.1722 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3520 Training loss: 1.3741 0.1697 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3520 Training loss: 1.3741 0.1694 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3520 Training loss: 1.3743 0.1674 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3520 Training loss: 1.3743 0.1725 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3520 Training loss: 1.3741 0.1730 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3520 Training loss: 1.3741 0.1734 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3520 Training loss: 1.3739 0.1732 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3520 Training loss: 1.3738 0.1666 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3520 Training loss: 1.3737 0.1760 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3520 Training loss: 1.3736 0.1729 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3520 Training loss: 1.3737 0.1676 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3520 Training loss: 1.3738 0.1749 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3520 Training loss: 1.3739 0.1685 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3520 Training loss: 1.3737 0.1715 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3520 Training loss: 1.3738 0.1721 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3520 Training loss: 1.3739 0.1724 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3520 Training loss: 1.3737 0.1692 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3520 Training loss: 1.3735 0.1729 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3520 Training loss: 1.3734 0.1725 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3520 Training loss: 1.3732 0.1732 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3520 Training loss: 1.3733 0.1736 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3520 Training loss: 1.3734 0.1733 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3520 Training loss: 1.3734 0.1732 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3520 Training loss: 1.3734 0.1731 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3520 Training loss: 1.3735 0.1649 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3520 Training loss: 1.3734 0.1667 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3520 Training loss: 1.3735 0.1727 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3520 Training loss: 1.3735 0.1726 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3520 Training loss: 1.3737 0.1684 sec/batch\n",
      "Epoch 18/20  Iteration 2993/3520 Training loss: 1.3962 0.1719 sec/batch\n",
      "Epoch 18/20  Iteration 2994/3520 Training loss: 1.3649 0.1693 sec/batch\n",
      "Epoch 18/20  Iteration 2995/3520 Training loss: 1.3656 0.1732 sec/batch\n",
      "Epoch 18/20  Iteration 2996/3520 Training loss: 1.3642 0.1705 sec/batch\n",
      "Epoch 18/20  Iteration 2997/3520 Training loss: 1.3672 0.1682 sec/batch\n",
      "Epoch 18/20  Iteration 2998/3520 Training loss: 1.3673 0.1729 sec/batch\n",
      "Epoch 18/20  Iteration 2999/3520 Training loss: 1.3655 0.1761 sec/batch\n",
      "Epoch 18/20  Iteration 3000/3520 Training loss: 1.3662 0.1724 sec/batch\n",
      "Validation loss: 1.24241 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 3001/3520 Training loss: 1.3791 0.1910 sec/batch\n",
      "Epoch 18/20  Iteration 3002/3520 Training loss: 1.3784 0.1873 sec/batch\n",
      "Epoch 18/20  Iteration 3003/3520 Training loss: 1.3777 0.1748 sec/batch\n",
      "Epoch 18/20  Iteration 3004/3520 Training loss: 1.3773 0.1719 sec/batch\n",
      "Epoch 18/20  Iteration 3005/3520 Training loss: 1.3762 0.1762 sec/batch\n",
      "Epoch 18/20  Iteration 3006/3520 Training loss: 1.3777 0.1725 sec/batch\n",
      "Epoch 18/20  Iteration 3007/3520 Training loss: 1.3776 0.1725 sec/batch\n",
      "Epoch 18/20  Iteration 3008/3520 Training loss: 1.3759 0.1683 sec/batch\n",
      "Epoch 18/20  Iteration 3009/3520 Training loss: 1.3752 0.1653 sec/batch\n",
      "Epoch 18/20  Iteration 3010/3520 Training loss: 1.3759 0.1684 sec/batch\n",
      "Epoch 18/20  Iteration 3011/3520 Training loss: 1.3746 0.1671 sec/batch\n",
      "Epoch 18/20  Iteration 3012/3520 Training loss: 1.3757 0.1670 sec/batch\n",
      "Epoch 18/20  Iteration 3013/3520 Training loss: 1.3756 0.1689 sec/batch\n",
      "Epoch 18/20  Iteration 3014/3520 Training loss: 1.3748 0.1738 sec/batch\n",
      "Epoch 18/20  Iteration 3015/3520 Training loss: 1.3751 0.1670 sec/batch\n",
      "Epoch 18/20  Iteration 3016/3520 Training loss: 1.3749 0.1684 sec/batch\n",
      "Epoch 18/20  Iteration 3017/3520 Training loss: 1.3755 0.1652 sec/batch\n",
      "Epoch 18/20  Iteration 3018/3520 Training loss: 1.3749 0.1705 sec/batch\n",
      "Epoch 18/20  Iteration 3019/3520 Training loss: 1.3739 0.1729 sec/batch\n",
      "Epoch 18/20  Iteration 3020/3520 Training loss: 1.3739 0.1718 sec/batch\n",
      "Epoch 18/20  Iteration 3021/3520 Training loss: 1.3731 0.1690 sec/batch\n",
      "Epoch 18/20  Iteration 3022/3520 Training loss: 1.3733 0.1743 sec/batch\n",
      "Epoch 18/20  Iteration 3023/3520 Training loss: 1.3728 0.1699 sec/batch\n",
      "Epoch 18/20  Iteration 3024/3520 Training loss: 1.3723 0.1724 sec/batch\n",
      "Epoch 18/20  Iteration 3025/3520 Training loss: 1.3727 0.1724 sec/batch\n",
      "Epoch 18/20  Iteration 3026/3520 Training loss: 1.3723 0.1680 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3520 Training loss: 1.3719 0.1715 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3520 Training loss: 1.3723 0.1674 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3520 Training loss: 1.3710 0.1697 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3520 Training loss: 1.3719 0.1648 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3520 Training loss: 1.3726 0.1645 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3520 Training loss: 1.3725 0.1725 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3520 Training loss: 1.3722 0.1714 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3520 Training loss: 1.3714 0.1691 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3520 Training loss: 1.3714 0.1669 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3520 Training loss: 1.3707 0.1786 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3520 Training loss: 1.3702 0.1723 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3520 Training loss: 1.3700 0.1685 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3520 Training loss: 1.3700 0.1727 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3520 Training loss: 1.3700 0.1727 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3520 Training loss: 1.3700 0.1620 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3520 Training loss: 1.3700 0.1759 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3520 Training loss: 1.3698 0.1663 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3520 Training loss: 1.3698 0.1754 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3520 Training loss: 1.3699 0.1684 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3520 Training loss: 1.3699 0.1728 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3520 Training loss: 1.3697 0.1702 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3520 Training loss: 1.3696 0.1646 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3520 Training loss: 1.3695 0.1632 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3520 Training loss: 1.3694 0.1767 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3520 Training loss: 1.3688 0.1725 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3520 Training loss: 1.3691 0.1690 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3520 Training loss: 1.3690 0.1690 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3520 Training loss: 1.3690 0.1659 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3520 Training loss: 1.3690 0.1682 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3520 Training loss: 1.3690 0.1651 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3520 Training loss: 1.3686 0.1725 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3520 Training loss: 1.3684 0.1659 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3520 Training loss: 1.3684 0.1675 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3520 Training loss: 1.3681 0.1727 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3520 Training loss: 1.3678 0.1729 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3520 Training loss: 1.3679 0.1733 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3520 Training loss: 1.3677 0.1674 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3520 Training loss: 1.3676 0.1719 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3520 Training loss: 1.3673 0.1779 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3520 Training loss: 1.3668 0.1728 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3520 Training loss: 1.3667 0.1670 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3520 Training loss: 1.3669 0.1678 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3520 Training loss: 1.3672 0.1730 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3520 Training loss: 1.3676 0.1642 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3520 Training loss: 1.3675 0.1752 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3520 Training loss: 1.3673 0.1731 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3520 Training loss: 1.3669 0.1735 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3520 Training loss: 1.3673 0.1656 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3520 Training loss: 1.3671 0.1719 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3520 Training loss: 1.3673 0.1732 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3520 Training loss: 1.3672 0.1724 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3520 Training loss: 1.3668 0.1727 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3520 Training loss: 1.3671 0.1678 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3520 Training loss: 1.3671 0.1656 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3520 Training loss: 1.3674 0.1756 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3520 Training loss: 1.3672 0.1729 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3520 Training loss: 1.3671 0.1726 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3520 Training loss: 1.3673 0.1721 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3520 Training loss: 1.3671 0.1719 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3520 Training loss: 1.3669 0.1691 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3520 Training loss: 1.3669 0.1723 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3520 Training loss: 1.3665 0.1723 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3520 Training loss: 1.3668 0.1679 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3520 Training loss: 1.3667 0.1750 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3520 Training loss: 1.3669 0.1720 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3520 Training loss: 1.3667 0.1651 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3520 Training loss: 1.3666 0.1660 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3520 Training loss: 1.3666 0.1681 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3520 Training loss: 1.3664 0.1720 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3520 Training loss: 1.3659 0.1724 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3520 Training loss: 1.3655 0.1708 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3520 Training loss: 1.3657 0.1688 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3520 Training loss: 1.3656 0.1697 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3520 Training loss: 1.3653 0.1662 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3520 Training loss: 1.3653 0.1674 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3520 Training loss: 1.3652 0.1732 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3520 Training loss: 1.3652 0.1688 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3520 Training loss: 1.3653 0.1723 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3520 Training loss: 1.3655 0.1670 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3520 Training loss: 1.3654 0.1615 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3520 Training loss: 1.3652 0.1658 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3520 Training loss: 1.3652 0.1632 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3520 Training loss: 1.3651 0.1726 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3520 Training loss: 1.3653 0.1675 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3520 Training loss: 1.3652 0.1686 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3520 Training loss: 1.3652 0.1678 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3520 Training loss: 1.3652 0.1711 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3520 Training loss: 1.3652 0.1724 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3520 Training loss: 1.3652 0.1723 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3520 Training loss: 1.3652 0.1721 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3520 Training loss: 1.3650 0.1729 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3520 Training loss: 1.3647 0.1723 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3520 Training loss: 1.3645 0.1730 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3520 Training loss: 1.3645 0.1731 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3520 Training loss: 1.3645 0.1726 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3520 Training loss: 1.3646 0.1732 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3520 Training loss: 1.3646 0.1640 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3520 Training loss: 1.3649 0.1726 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3520 Training loss: 1.3649 0.1729 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3520 Training loss: 1.3650 0.1743 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3520 Training loss: 1.3651 0.1731 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3520 Training loss: 1.3651 0.1696 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3520 Training loss: 1.3651 0.1660 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3520 Training loss: 1.3650 0.1677 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3520 Training loss: 1.3651 0.1726 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3520 Training loss: 1.3652 0.1727 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3520 Training loss: 1.3652 0.1731 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3520 Training loss: 1.3650 0.1715 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3520 Training loss: 1.3648 0.1732 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3520 Training loss: 1.3647 0.1736 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3520 Training loss: 1.3648 0.1683 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3520 Training loss: 1.3648 0.1718 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3520 Training loss: 1.3647 0.1727 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3520 Training loss: 1.3647 0.1737 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3520 Training loss: 1.3646 0.1651 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3520 Training loss: 1.3648 0.1729 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3520 Training loss: 1.3647 0.1642 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3520 Training loss: 1.3646 0.1687 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3520 Training loss: 1.3645 0.1650 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3520 Training loss: 1.3643 0.1687 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3520 Training loss: 1.3643 0.1678 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3520 Training loss: 1.3643 0.1678 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3520 Training loss: 1.3640 0.1763 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3520 Training loss: 1.3641 0.1685 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3520 Training loss: 1.3642 0.1688 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3520 Training loss: 1.3643 0.1725 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3520 Training loss: 1.3641 0.1660 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3520 Training loss: 1.3641 0.1727 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3520 Training loss: 1.3641 0.1721 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3520 Training loss: 1.3639 0.1684 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3520 Training loss: 1.3636 0.1662 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3520 Training loss: 1.3635 0.1674 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3520 Training loss: 1.3633 0.1746 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3520 Training loss: 1.3634 0.1681 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3520 Training loss: 1.3635 0.1759 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3520 Training loss: 1.3634 0.1721 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3520 Training loss: 1.3634 0.1723 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3520 Training loss: 1.3634 0.1690 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3520 Training loss: 1.3633 0.1678 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3520 Training loss: 1.3634 0.1711 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3520 Training loss: 1.3634 0.1730 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3520 Training loss: 1.3636 0.1752 sec/batch\n",
      "Epoch 19/20  Iteration 3169/3520 Training loss: 1.3936 0.1665 sec/batch\n",
      "Epoch 19/20  Iteration 3170/3520 Training loss: 1.3550 0.1689 sec/batch\n",
      "Epoch 19/20  Iteration 3171/3520 Training loss: 1.3538 0.1653 sec/batch\n",
      "Epoch 19/20  Iteration 3172/3520 Training loss: 1.3547 0.1729 sec/batch\n",
      "Epoch 19/20  Iteration 3173/3520 Training loss: 1.3579 0.1753 sec/batch\n",
      "Epoch 19/20  Iteration 3174/3520 Training loss: 1.3588 0.1727 sec/batch\n",
      "Epoch 19/20  Iteration 3175/3520 Training loss: 1.3566 0.1688 sec/batch\n",
      "Epoch 19/20  Iteration 3176/3520 Training loss: 1.3561 0.1668 sec/batch\n",
      "Epoch 19/20  Iteration 3177/3520 Training loss: 1.3533 0.1722 sec/batch\n",
      "Epoch 19/20  Iteration 3178/3520 Training loss: 1.3533 0.1658 sec/batch\n",
      "Epoch 19/20  Iteration 3179/3520 Training loss: 1.3539 0.1735 sec/batch\n",
      "Epoch 19/20  Iteration 3180/3520 Training loss: 1.3549 0.1715 sec/batch\n",
      "Epoch 19/20  Iteration 3181/3520 Training loss: 1.3544 0.1728 sec/batch\n",
      "Epoch 19/20  Iteration 3182/3520 Training loss: 1.3567 0.1680 sec/batch\n",
      "Epoch 19/20  Iteration 3183/3520 Training loss: 1.3564 0.1689 sec/batch\n",
      "Epoch 19/20  Iteration 3184/3520 Training loss: 1.3556 0.1656 sec/batch\n",
      "Epoch 19/20  Iteration 3185/3520 Training loss: 1.3548 0.1751 sec/batch\n",
      "Epoch 19/20  Iteration 3186/3520 Training loss: 1.3558 0.1727 sec/batch\n",
      "Epoch 19/20  Iteration 3187/3520 Training loss: 1.3551 0.1718 sec/batch\n",
      "Epoch 19/20  Iteration 3188/3520 Training loss: 1.3565 0.1770 sec/batch\n",
      "Epoch 19/20  Iteration 3189/3520 Training loss: 1.3569 0.1703 sec/batch\n",
      "Epoch 19/20  Iteration 3190/3520 Training loss: 1.3566 0.1725 sec/batch\n",
      "Epoch 19/20  Iteration 3191/3520 Training loss: 1.3572 0.1741 sec/batch\n",
      "Epoch 19/20  Iteration 3192/3520 Training loss: 1.3573 0.1697 sec/batch\n",
      "Epoch 19/20  Iteration 3193/3520 Training loss: 1.3580 0.1799 sec/batch\n",
      "Epoch 19/20  Iteration 3194/3520 Training loss: 1.3575 0.1665 sec/batch\n",
      "Epoch 19/20  Iteration 3195/3520 Training loss: 1.3570 0.1674 sec/batch\n",
      "Epoch 19/20  Iteration 3196/3520 Training loss: 1.3572 0.1660 sec/batch\n",
      "Epoch 19/20  Iteration 3197/3520 Training loss: 1.3566 0.1666 sec/batch\n",
      "Epoch 19/20  Iteration 3198/3520 Training loss: 1.3568 0.1667 sec/batch\n",
      "Epoch 19/20  Iteration 3199/3520 Training loss: 1.3563 0.1730 sec/batch\n",
      "Epoch 19/20  Iteration 3200/3520 Training loss: 1.3563 0.1680 sec/batch\n",
      "Validation loss: 1.23099 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 3201/3520 Training loss: 1.3606 0.1766 sec/batch\n",
      "Epoch 19/20  Iteration 3202/3520 Training loss: 1.3606 0.1716 sec/batch\n",
      "Epoch 19/20  Iteration 3203/3520 Training loss: 1.3604 0.1732 sec/batch\n",
      "Epoch 19/20  Iteration 3204/3520 Training loss: 1.3608 0.1764 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3520 Training loss: 1.3599 0.1726 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3520 Training loss: 1.3608 0.1658 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3520 Training loss: 1.3618 0.1660 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3520 Training loss: 1.3621 0.1701 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3520 Training loss: 1.3618 0.1668 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3520 Training loss: 1.3612 0.1671 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3520 Training loss: 1.3612 0.1663 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3520 Training loss: 1.3605 0.1705 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3520 Training loss: 1.3603 0.1660 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3520 Training loss: 1.3602 0.1687 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3520 Training loss: 1.3601 0.1664 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3520 Training loss: 1.3604 0.1702 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3520 Training loss: 1.3601 0.1729 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3520 Training loss: 1.3600 0.1684 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3520 Training loss: 1.3598 0.1681 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3520 Training loss: 1.3597 0.1726 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3520 Training loss: 1.3600 0.1726 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3520 Training loss: 1.3600 0.1679 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3520 Training loss: 1.3598 0.1745 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3520 Training loss: 1.3597 0.1732 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3520 Training loss: 1.3593 0.1726 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3520 Training loss: 1.3591 0.1721 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3520 Training loss: 1.3586 0.1728 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3520 Training loss: 1.3588 0.1659 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3520 Training loss: 1.3584 0.1750 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3520 Training loss: 1.3585 0.1736 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3520 Training loss: 1.3586 0.1673 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3520 Training loss: 1.3585 0.1734 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3520 Training loss: 1.3579 0.1728 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3520 Training loss: 1.3579 0.1682 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3520 Training loss: 1.3577 0.1719 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3520 Training loss: 1.3573 0.1725 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3520 Training loss: 1.3571 0.1689 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3520 Training loss: 1.3571 0.1684 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3520 Training loss: 1.3569 0.1736 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3520 Training loss: 1.3566 0.1730 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3520 Training loss: 1.3563 0.1727 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3520 Training loss: 1.3558 0.1731 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3520 Training loss: 1.3556 0.1729 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3520 Training loss: 1.3557 0.1732 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3520 Training loss: 1.3560 0.1730 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3520 Training loss: 1.3564 0.1645 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3520 Training loss: 1.3563 0.1728 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3520 Training loss: 1.3562 0.1710 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3520 Training loss: 1.3559 0.1725 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3520 Training loss: 1.3563 0.1667 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3520 Training loss: 1.3562 0.1658 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3520 Training loss: 1.3565 0.1690 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3520 Training loss: 1.3564 0.1729 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3520 Training loss: 1.3561 0.1726 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3520 Training loss: 1.3564 0.1680 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3520 Training loss: 1.3563 0.1625 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3520 Training loss: 1.3565 0.1724 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3520 Training loss: 1.3563 0.1665 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3520 Training loss: 1.3562 0.1728 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3520 Training loss: 1.3563 0.1731 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3520 Training loss: 1.3560 0.1685 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3520 Training loss: 1.3558 0.1727 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3520 Training loss: 1.3558 0.1705 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3520 Training loss: 1.3555 0.1690 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3520 Training loss: 1.3558 0.1679 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3520 Training loss: 1.3557 0.1737 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3520 Training loss: 1.3559 0.1726 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3520 Training loss: 1.3557 0.1730 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3520 Training loss: 1.3557 0.1729 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3520 Training loss: 1.3558 0.1649 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3520 Training loss: 1.3556 0.1727 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3520 Training loss: 1.3550 0.1729 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3520 Training loss: 1.3547 0.1729 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3520 Training loss: 1.3549 0.1730 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3520 Training loss: 1.3548 0.1726 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3520 Training loss: 1.3546 0.1684 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3520 Training loss: 1.3546 0.1686 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3520 Training loss: 1.3545 0.1697 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3520 Training loss: 1.3545 0.1731 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3520 Training loss: 1.3546 0.1668 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3520 Training loss: 1.3547 0.1761 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3520 Training loss: 1.3546 0.1663 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3520 Training loss: 1.3544 0.1671 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3520 Training loss: 1.3543 0.1688 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3520 Training loss: 1.3541 0.1653 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3520 Training loss: 1.3542 0.1733 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3520 Training loss: 1.3542 0.1660 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3520 Training loss: 1.3541 0.1678 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3520 Training loss: 1.3542 0.1733 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3520 Training loss: 1.3541 0.1649 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3520 Training loss: 1.3540 0.1732 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3520 Training loss: 1.3541 0.1728 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3520 Training loss: 1.3541 0.1666 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3520 Training loss: 1.3537 0.1720 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3520 Training loss: 1.3535 0.1691 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3520 Training loss: 1.3535 0.1727 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3520 Training loss: 1.3534 0.1678 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3520 Training loss: 1.3536 0.1656 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3520 Training loss: 1.3535 0.1664 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3520 Training loss: 1.3538 0.1718 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3520 Training loss: 1.3537 0.1633 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3520 Training loss: 1.3538 0.1681 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3520 Training loss: 1.3539 0.1647 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3520 Training loss: 1.3539 0.1720 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3520 Training loss: 1.3539 0.1671 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3520 Training loss: 1.3538 0.1706 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3520 Training loss: 1.3539 0.1648 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3520 Training loss: 1.3540 0.1660 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3520 Training loss: 1.3540 0.1724 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3520 Training loss: 1.3538 0.1768 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3520 Training loss: 1.3536 0.1656 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3520 Training loss: 1.3535 0.1726 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3520 Training loss: 1.3536 0.1681 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3520 Training loss: 1.3537 0.1691 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3520 Training loss: 1.3537 0.1727 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3520 Training loss: 1.3536 0.1654 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3520 Training loss: 1.3537 0.1646 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3520 Training loss: 1.3539 0.1670 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3520 Training loss: 1.3537 0.1668 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3520 Training loss: 1.3536 0.1660 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3520 Training loss: 1.3534 0.1663 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3520 Training loss: 1.3533 0.1670 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3520 Training loss: 1.3532 0.1672 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3520 Training loss: 1.3532 0.1732 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3520 Training loss: 1.3530 0.1683 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3520 Training loss: 1.3531 0.1721 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3520 Training loss: 1.3532 0.1724 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3520 Training loss: 1.3533 0.1670 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3520 Training loss: 1.3531 0.1686 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3520 Training loss: 1.3531 0.1754 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3520 Training loss: 1.3532 0.1726 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3520 Training loss: 1.3530 0.1662 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3520 Training loss: 1.3527 0.1653 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3520 Training loss: 1.3525 0.1754 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3520 Training loss: 1.3523 0.1827 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3520 Training loss: 1.3524 0.1654 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3520 Training loss: 1.3525 0.1626 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3520 Training loss: 1.3525 0.1649 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3520 Training loss: 1.3526 0.1722 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3520 Training loss: 1.3526 0.1691 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3520 Training loss: 1.3526 0.1730 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3520 Training loss: 1.3527 0.1681 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3520 Training loss: 1.3526 0.1731 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3520 Training loss: 1.3529 0.1730 sec/batch\n",
      "Epoch 20/20  Iteration 3345/3520 Training loss: 1.3944 0.1722 sec/batch\n",
      "Epoch 20/20  Iteration 3346/3520 Training loss: 1.3530 0.1725 sec/batch\n",
      "Epoch 20/20  Iteration 3347/3520 Training loss: 1.3530 0.1740 sec/batch\n",
      "Epoch 20/20  Iteration 3348/3520 Training loss: 1.3496 0.1683 sec/batch\n",
      "Epoch 20/20  Iteration 3349/3520 Training loss: 1.3513 0.1719 sec/batch\n",
      "Epoch 20/20  Iteration 3350/3520 Training loss: 1.3510 0.1729 sec/batch\n",
      "Epoch 20/20  Iteration 3351/3520 Training loss: 1.3485 0.1725 sec/batch\n",
      "Epoch 20/20  Iteration 3352/3520 Training loss: 1.3481 0.1680 sec/batch\n",
      "Epoch 20/20  Iteration 3353/3520 Training loss: 1.3454 0.1675 sec/batch\n",
      "Epoch 20/20  Iteration 3354/3520 Training loss: 1.3455 0.1732 sec/batch\n",
      "Epoch 20/20  Iteration 3355/3520 Training loss: 1.3454 0.1725 sec/batch\n",
      "Epoch 20/20  Iteration 3356/3520 Training loss: 1.3459 0.1716 sec/batch\n",
      "Epoch 20/20  Iteration 3357/3520 Training loss: 1.3450 0.1721 sec/batch\n",
      "Epoch 20/20  Iteration 3358/3520 Training loss: 1.3472 0.1766 sec/batch\n",
      "Epoch 20/20  Iteration 3359/3520 Training loss: 1.3467 0.1726 sec/batch\n",
      "Epoch 20/20  Iteration 3360/3520 Training loss: 1.3455 0.1726 sec/batch\n",
      "Epoch 20/20  Iteration 3361/3520 Training loss: 1.3453 0.1729 sec/batch\n",
      "Epoch 20/20  Iteration 3362/3520 Training loss: 1.3462 0.1734 sec/batch\n",
      "Epoch 20/20  Iteration 3363/3520 Training loss: 1.3455 0.1731 sec/batch\n",
      "Epoch 20/20  Iteration 3364/3520 Training loss: 1.3467 0.1726 sec/batch\n",
      "Epoch 20/20  Iteration 3365/3520 Training loss: 1.3472 0.1674 sec/batch\n",
      "Epoch 20/20  Iteration 3366/3520 Training loss: 1.3471 0.1686 sec/batch\n",
      "Epoch 20/20  Iteration 3367/3520 Training loss: 1.3475 0.1768 sec/batch\n",
      "Epoch 20/20  Iteration 3368/3520 Training loss: 1.3482 0.1743 sec/batch\n",
      "Epoch 20/20  Iteration 3369/3520 Training loss: 1.3485 0.1731 sec/batch\n",
      "Epoch 20/20  Iteration 3370/3520 Training loss: 1.3481 0.1725 sec/batch\n",
      "Epoch 20/20  Iteration 3371/3520 Training loss: 1.3475 0.1731 sec/batch\n",
      "Epoch 20/20  Iteration 3372/3520 Training loss: 1.3477 0.1665 sec/batch\n",
      "Epoch 20/20  Iteration 3373/3520 Training loss: 1.3471 0.1788 sec/batch\n",
      "Epoch 20/20  Iteration 3374/3520 Training loss: 1.3475 0.1723 sec/batch\n",
      "Epoch 20/20  Iteration 3375/3520 Training loss: 1.3472 0.1683 sec/batch\n",
      "Epoch 20/20  Iteration 3376/3520 Training loss: 1.3469 0.1731 sec/batch\n",
      "Epoch 20/20  Iteration 3377/3520 Training loss: 1.3477 0.1732 sec/batch\n",
      "Epoch 20/20  Iteration 3378/3520 Training loss: 1.3473 0.1729 sec/batch\n",
      "Epoch 20/20  Iteration 3379/3520 Training loss: 1.3470 0.1677 sec/batch\n",
      "Epoch 20/20  Iteration 3380/3520 Training loss: 1.3476 0.1730 sec/batch\n",
      "Epoch 20/20  Iteration 3381/3520 Training loss: 1.3467 0.1716 sec/batch\n",
      "Epoch 20/20  Iteration 3382/3520 Training loss: 1.3477 0.1685 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3520 Training loss: 1.3487 0.1660 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3520 Training loss: 1.3489 0.1688 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3520 Training loss: 1.3487 0.1726 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3520 Training loss: 1.3482 0.1727 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3520 Training loss: 1.3485 0.1663 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3520 Training loss: 1.3478 0.1678 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3520 Training loss: 1.3472 0.1694 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3520 Training loss: 1.3473 0.1689 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3520 Training loss: 1.3474 0.1709 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3520 Training loss: 1.3475 0.1735 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3520 Training loss: 1.3475 0.1722 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3520 Training loss: 1.3475 0.1675 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3520 Training loss: 1.3473 0.1729 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3520 Training loss: 1.3473 0.1727 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3520 Training loss: 1.3474 0.1665 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3520 Training loss: 1.3474 0.1664 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3520 Training loss: 1.3472 0.1663 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3520 Training loss: 1.3472 0.1815 sec/batch\n",
      "Validation loss: 1.22116 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 3401/3520 Training loss: 1.3490 0.1917 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3520 Training loss: 1.3491 0.1765 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3520 Training loss: 1.3485 0.1682 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3520 Training loss: 1.3491 0.1670 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3520 Training loss: 1.3490 0.1727 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3520 Training loss: 1.3490 0.1721 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3520 Training loss: 1.3490 0.1725 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3520 Training loss: 1.3490 0.1658 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3520 Training loss: 1.3486 0.1680 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3520 Training loss: 1.3487 0.1692 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3520 Training loss: 1.3485 0.1675 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3520 Training loss: 1.3481 0.1647 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3520 Training loss: 1.3479 0.1724 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3520 Training loss: 1.3480 0.1691 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3520 Training loss: 1.3479 0.1688 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3520 Training loss: 1.3476 0.1723 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3520 Training loss: 1.3474 0.1729 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3520 Training loss: 1.3467 0.1654 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3520 Training loss: 1.3466 0.1724 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3520 Training loss: 1.3468 0.1647 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3520 Training loss: 1.3471 0.1656 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3520 Training loss: 1.3474 0.1654 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3520 Training loss: 1.3473 0.1725 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3520 Training loss: 1.3472 0.1729 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3520 Training loss: 1.3468 0.1649 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3520 Training loss: 1.3470 0.1725 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3520 Training loss: 1.3468 0.1725 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3520 Training loss: 1.3471 0.1696 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3520 Training loss: 1.3471 0.1675 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3520 Training loss: 1.3467 0.1673 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3520 Training loss: 1.3471 0.1704 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3520 Training loss: 1.3470 0.1721 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3520 Training loss: 1.3471 0.1728 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3520 Training loss: 1.3470 0.1676 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3520 Training loss: 1.3469 0.1726 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3520 Training loss: 1.3472 0.1665 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3520 Training loss: 1.3470 0.1688 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3520 Training loss: 1.3469 0.1620 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3520 Training loss: 1.3469 0.1660 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3520 Training loss: 1.3466 0.1729 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3520 Training loss: 1.3469 0.1674 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3520 Training loss: 1.3470 0.1722 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3520 Training loss: 1.3470 0.1725 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3520 Training loss: 1.3468 0.1690 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3520 Training loss: 1.3467 0.1688 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3520 Training loss: 1.3468 0.1746 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3520 Training loss: 1.3465 0.1688 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3520 Training loss: 1.3460 0.1693 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3520 Training loss: 1.3457 0.1727 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3520 Training loss: 1.3459 0.1687 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3520 Training loss: 1.3458 0.1728 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3520 Training loss: 1.3456 0.1669 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3520 Training loss: 1.3457 0.1676 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3520 Training loss: 1.3456 0.1682 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3520 Training loss: 1.3457 0.1676 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3520 Training loss: 1.3457 0.1726 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3520 Training loss: 1.3459 0.1725 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3520 Training loss: 1.3457 0.1692 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3520 Training loss: 1.3455 0.1762 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3520 Training loss: 1.3455 0.1726 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3520 Training loss: 1.3454 0.1746 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3520 Training loss: 1.3455 0.1678 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3520 Training loss: 1.3454 0.1686 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3520 Training loss: 1.3454 0.1729 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3520 Training loss: 1.3453 0.1663 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3520 Training loss: 1.3453 0.1691 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3520 Training loss: 1.3453 0.1667 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3520 Training loss: 1.3453 0.1720 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3520 Training loss: 1.3451 0.1649 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3520 Training loss: 1.3448 0.1730 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3520 Training loss: 1.3446 0.1724 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3520 Training loss: 1.3447 0.1688 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3520 Training loss: 1.3447 0.1728 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3520 Training loss: 1.3448 0.1730 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3520 Training loss: 1.3448 0.1714 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3520 Training loss: 1.3451 0.1670 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3520 Training loss: 1.3450 0.1668 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3520 Training loss: 1.3452 0.1700 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3520 Training loss: 1.3453 0.1724 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3520 Training loss: 1.3453 0.1688 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3520 Training loss: 1.3453 0.1668 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3520 Training loss: 1.3451 0.1671 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3520 Training loss: 1.3452 0.1733 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3520 Training loss: 1.3454 0.1654 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3520 Training loss: 1.3453 0.1670 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3520 Training loss: 1.3452 0.1667 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3520 Training loss: 1.3450 0.1696 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3520 Training loss: 1.3448 0.1697 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3520 Training loss: 1.3450 0.1686 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3520 Training loss: 1.3451 0.1685 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3520 Training loss: 1.3450 0.1646 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3520 Training loss: 1.3449 0.1730 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3520 Training loss: 1.3450 0.1698 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3520 Training loss: 1.3451 0.1667 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3520 Training loss: 1.3449 0.1702 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3520 Training loss: 1.3449 0.1804 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3520 Training loss: 1.3448 0.1728 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3520 Training loss: 1.3446 0.1727 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3520 Training loss: 1.3446 0.1652 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3520 Training loss: 1.3445 0.1672 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3520 Training loss: 1.3443 0.1662 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3520 Training loss: 1.3444 0.1724 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3520 Training loss: 1.3445 0.1670 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3520 Training loss: 1.3446 0.1692 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3520 Training loss: 1.3444 0.1640 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3520 Training loss: 1.3445 0.1687 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3520 Training loss: 1.3445 0.1645 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3520 Training loss: 1.3444 0.1708 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3520 Training loss: 1.3441 0.1669 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3520 Training loss: 1.3440 0.1697 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3520 Training loss: 1.3438 0.1723 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3520 Training loss: 1.3438 0.1729 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3520 Training loss: 1.3439 0.1727 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3520 Training loss: 1.3439 0.1680 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3520 Training loss: 1.3439 0.1725 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3520 Training loss: 1.3439 0.1681 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3520 Training loss: 1.3438 0.1662 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3520 Training loss: 1.3439 0.1682 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3520 Training loss: 1.3439 0.1670 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3520 Training loss: 1.3442 0.1669 sec/batch\n",
      "Validation loss: 1.21932 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 1/3520 Training loss: 4.3520 0.6673 sec/batch\n",
      "Epoch 1/20  Iteration 2/3520 Training loss: 4.3070 0.1374 sec/batch\n",
      "Epoch 1/20  Iteration 3/3520 Training loss: 4.1025 0.1379 sec/batch\n",
      "Epoch 1/20  Iteration 4/3520 Training loss: 4.0095 0.1313 sec/batch\n",
      "Epoch 1/20  Iteration 5/3520 Training loss: 3.9005 0.1349 sec/batch\n",
      "Epoch 1/20  Iteration 6/3520 Training loss: 3.8082 0.1378 sec/batch\n",
      "Epoch 1/20  Iteration 7/3520 Training loss: 3.7464 0.1389 sec/batch\n",
      "Epoch 1/20  Iteration 8/3520 Training loss: 3.6909 0.1363 sec/batch\n",
      "Epoch 1/20  Iteration 9/3520 Training loss: 3.6441 0.1378 sec/batch\n",
      "Epoch 1/20  Iteration 10/3520 Training loss: 3.6065 0.1373 sec/batch\n",
      "Epoch 1/20  Iteration 11/3520 Training loss: 3.5727 0.1369 sec/batch\n",
      "Epoch 1/20  Iteration 12/3520 Training loss: 3.5431 0.1373 sec/batch\n",
      "Epoch 1/20  Iteration 13/3520 Training loss: 3.5189 0.1372 sec/batch\n",
      "Epoch 1/20  Iteration 14/3520 Training loss: 3.4946 0.1363 sec/batch\n",
      "Epoch 1/20  Iteration 15/3520 Training loss: 3.4726 0.1354 sec/batch\n",
      "Epoch 1/20  Iteration 16/3520 Training loss: 3.4532 0.1369 sec/batch\n",
      "Epoch 1/20  Iteration 17/3520 Training loss: 3.4358 0.1358 sec/batch\n",
      "Epoch 1/20  Iteration 18/3520 Training loss: 3.4207 0.1350 sec/batch\n",
      "Epoch 1/20  Iteration 19/3520 Training loss: 3.4055 0.1348 sec/batch\n",
      "Epoch 1/20  Iteration 20/3520 Training loss: 3.3925 0.1363 sec/batch\n",
      "Epoch 1/20  Iteration 21/3520 Training loss: 3.3796 0.1355 sec/batch\n",
      "Epoch 1/20  Iteration 22/3520 Training loss: 3.3685 0.1362 sec/batch\n",
      "Epoch 1/20  Iteration 23/3520 Training loss: 3.3579 0.1373 sec/batch\n",
      "Epoch 1/20  Iteration 24/3520 Training loss: 3.3481 0.1385 sec/batch\n",
      "Epoch 1/20  Iteration 25/3520 Training loss: 3.3393 0.1369 sec/batch\n",
      "Epoch 1/20  Iteration 26/3520 Training loss: 3.3304 0.1379 sec/batch\n",
      "Epoch 1/20  Iteration 27/3520 Training loss: 3.3223 0.1316 sec/batch\n",
      "Epoch 1/20  Iteration 28/3520 Training loss: 3.3139 0.1362 sec/batch\n",
      "Epoch 1/20  Iteration 29/3520 Training loss: 3.3063 0.1357 sec/batch\n",
      "Epoch 1/20  Iteration 30/3520 Training loss: 3.2989 0.1310 sec/batch\n",
      "Epoch 1/20  Iteration 31/3520 Training loss: 3.2914 0.1358 sec/batch\n",
      "Epoch 1/20  Iteration 32/3520 Training loss: 3.2842 0.1379 sec/batch\n",
      "Epoch 1/20  Iteration 33/3520 Training loss: 3.2775 0.1368 sec/batch\n",
      "Epoch 1/20  Iteration 34/3520 Training loss: 3.2719 0.1365 sec/batch\n",
      "Epoch 1/20  Iteration 35/3520 Training loss: 3.2656 0.1382 sec/batch\n",
      "Epoch 1/20  Iteration 36/3520 Training loss: 3.2598 0.1369 sec/batch\n",
      "Epoch 1/20  Iteration 37/3520 Training loss: 3.2544 0.1362 sec/batch\n",
      "Epoch 1/20  Iteration 38/3520 Training loss: 3.2492 0.1356 sec/batch\n",
      "Epoch 1/20  Iteration 39/3520 Training loss: 3.2441 0.1379 sec/batch\n",
      "Epoch 1/20  Iteration 40/3520 Training loss: 3.2392 0.1357 sec/batch\n",
      "Epoch 1/20  Iteration 41/3520 Training loss: 3.2344 0.1365 sec/batch\n",
      "Epoch 1/20  Iteration 42/3520 Training loss: 3.2293 0.1372 sec/batch\n",
      "Epoch 1/20  Iteration 43/3520 Training loss: 3.2245 0.1356 sec/batch\n",
      "Epoch 1/20  Iteration 44/3520 Training loss: 3.2194 0.1341 sec/batch\n",
      "Epoch 1/20  Iteration 45/3520 Training loss: 3.2145 0.1367 sec/batch\n",
      "Epoch 1/20  Iteration 46/3520 Training loss: 3.2097 0.1364 sec/batch\n",
      "Epoch 1/20  Iteration 47/3520 Training loss: 3.2049 0.1370 sec/batch\n",
      "Epoch 1/20  Iteration 48/3520 Training loss: 3.2004 0.1371 sec/batch\n",
      "Epoch 1/20  Iteration 49/3520 Training loss: 3.1960 0.1376 sec/batch\n",
      "Epoch 1/20  Iteration 50/3520 Training loss: 3.1917 0.1367 sec/batch\n",
      "Epoch 1/20  Iteration 51/3520 Training loss: 3.1874 0.1303 sec/batch\n",
      "Epoch 1/20  Iteration 52/3520 Training loss: 3.1831 0.1357 sec/batch\n",
      "Epoch 1/20  Iteration 53/3520 Training loss: 3.1785 0.1354 sec/batch\n",
      "Epoch 1/20  Iteration 54/3520 Training loss: 3.1743 0.1287 sec/batch\n",
      "Epoch 1/20  Iteration 55/3520 Training loss: 3.1696 0.1364 sec/batch\n",
      "Epoch 1/20  Iteration 56/3520 Training loss: 3.1651 0.1383 sec/batch\n",
      "Epoch 1/20  Iteration 57/3520 Training loss: 3.1600 0.1361 sec/batch\n",
      "Epoch 1/20  Iteration 58/3520 Training loss: 3.1551 0.1383 sec/batch\n",
      "Epoch 1/20  Iteration 59/3520 Training loss: 3.1502 0.1370 sec/batch\n",
      "Epoch 1/20  Iteration 60/3520 Training loss: 3.1451 0.1383 sec/batch\n",
      "Epoch 1/20  Iteration 61/3520 Training loss: 3.1398 0.1390 sec/batch\n",
      "Epoch 1/20  Iteration 62/3520 Training loss: 3.1347 0.1379 sec/batch\n",
      "Epoch 1/20  Iteration 63/3520 Training loss: 3.1292 0.1360 sec/batch\n",
      "Epoch 1/20  Iteration 64/3520 Training loss: 3.1233 0.1376 sec/batch\n",
      "Epoch 1/20  Iteration 65/3520 Training loss: 3.1178 0.1383 sec/batch\n",
      "Epoch 1/20  Iteration 66/3520 Training loss: 3.1121 0.1383 sec/batch\n",
      "Epoch 1/20  Iteration 67/3520 Training loss: 3.1067 0.1356 sec/batch\n",
      "Epoch 1/20  Iteration 68/3520 Training loss: 3.1009 0.1346 sec/batch\n",
      "Epoch 1/20  Iteration 69/3520 Training loss: 3.0951 0.1379 sec/batch\n",
      "Epoch 1/20  Iteration 70/3520 Training loss: 3.0896 0.1373 sec/batch\n",
      "Epoch 1/20  Iteration 71/3520 Training loss: 3.0839 0.1386 sec/batch\n",
      "Epoch 1/20  Iteration 72/3520 Training loss: 3.0782 0.1385 sec/batch\n",
      "Epoch 1/20  Iteration 73/3520 Training loss: 3.0723 0.1356 sec/batch\n",
      "Epoch 1/20  Iteration 74/3520 Training loss: 3.0662 0.1368 sec/batch\n",
      "Epoch 1/20  Iteration 75/3520 Training loss: 3.0602 0.1355 sec/batch\n",
      "Epoch 1/20  Iteration 76/3520 Training loss: 3.0555 0.1347 sec/batch\n",
      "Epoch 1/20  Iteration 77/3520 Training loss: 3.0511 0.1330 sec/batch\n",
      "Epoch 1/20  Iteration 78/3520 Training loss: 3.0453 0.1359 sec/batch\n",
      "Epoch 1/20  Iteration 79/3520 Training loss: 3.0397 0.1370 sec/batch\n",
      "Epoch 1/20  Iteration 80/3520 Training loss: 3.0344 0.1363 sec/batch\n",
      "Epoch 1/20  Iteration 81/3520 Training loss: 3.0287 0.1383 sec/batch\n",
      "Epoch 1/20  Iteration 82/3520 Training loss: 3.0234 0.1363 sec/batch\n",
      "Epoch 1/20  Iteration 83/3520 Training loss: 3.0179 0.1389 sec/batch\n",
      "Epoch 1/20  Iteration 84/3520 Training loss: 3.0128 0.1379 sec/batch\n",
      "Epoch 1/20  Iteration 85/3520 Training loss: 3.0075 0.1373 sec/batch\n",
      "Epoch 1/20  Iteration 86/3520 Training loss: 3.0022 0.1382 sec/batch\n",
      "Epoch 1/20  Iteration 87/3520 Training loss: 2.9968 0.1414 sec/batch\n",
      "Epoch 1/20  Iteration 88/3520 Training loss: 2.9914 0.1391 sec/batch\n",
      "Epoch 1/20  Iteration 89/3520 Training loss: 2.9860 0.1360 sec/batch\n",
      "Epoch 1/20  Iteration 90/3520 Training loss: 2.9808 0.1377 sec/batch\n",
      "Epoch 1/20  Iteration 91/3520 Training loss: 2.9757 0.1364 sec/batch\n",
      "Epoch 1/20  Iteration 92/3520 Training loss: 2.9707 0.1377 sec/batch\n",
      "Epoch 1/20  Iteration 93/3520 Training loss: 2.9657 0.1393 sec/batch\n",
      "Epoch 1/20  Iteration 94/3520 Training loss: 2.9604 0.1381 sec/batch\n",
      "Epoch 1/20  Iteration 95/3520 Training loss: 2.9552 0.1390 sec/batch\n",
      "Epoch 1/20  Iteration 96/3520 Training loss: 2.9500 0.1392 sec/batch\n",
      "Epoch 1/20  Iteration 97/3520 Training loss: 2.9453 0.1369 sec/batch\n",
      "Epoch 1/20  Iteration 98/3520 Training loss: 2.9405 0.1373 sec/batch\n",
      "Epoch 1/20  Iteration 99/3520 Training loss: 2.9358 0.1389 sec/batch\n",
      "Epoch 1/20  Iteration 100/3520 Training loss: 2.9305 0.1331 sec/batch\n",
      "Epoch 1/20  Iteration 101/3520 Training loss: 2.9256 0.1375 sec/batch\n",
      "Epoch 1/20  Iteration 102/3520 Training loss: 2.9210 0.1359 sec/batch\n",
      "Epoch 1/20  Iteration 103/3520 Training loss: 2.9162 0.1375 sec/batch\n",
      "Epoch 1/20  Iteration 104/3520 Training loss: 2.9112 0.1404 sec/batch\n",
      "Epoch 1/20  Iteration 105/3520 Training loss: 2.9063 0.1348 sec/batch\n",
      "Epoch 1/20  Iteration 106/3520 Training loss: 2.9017 0.1370 sec/batch\n",
      "Epoch 1/20  Iteration 107/3520 Training loss: 2.8967 0.1362 sec/batch\n",
      "Epoch 1/20  Iteration 108/3520 Training loss: 2.8916 0.1360 sec/batch\n",
      "Epoch 1/20  Iteration 109/3520 Training loss: 2.8869 0.1361 sec/batch\n",
      "Epoch 1/20  Iteration 110/3520 Training loss: 2.8821 0.1369 sec/batch\n",
      "Epoch 1/20  Iteration 111/3520 Training loss: 2.8772 0.1361 sec/batch\n",
      "Epoch 1/20  Iteration 112/3520 Training loss: 2.8726 0.1365 sec/batch\n",
      "Epoch 1/20  Iteration 113/3520 Training loss: 2.8682 0.1364 sec/batch\n",
      "Epoch 1/20  Iteration 114/3520 Training loss: 2.8635 0.1372 sec/batch\n",
      "Epoch 1/20  Iteration 115/3520 Training loss: 2.8589 0.1363 sec/batch\n",
      "Epoch 1/20  Iteration 116/3520 Training loss: 2.8545 0.1385 sec/batch\n",
      "Epoch 1/20  Iteration 117/3520 Training loss: 2.8501 0.1371 sec/batch\n",
      "Epoch 1/20  Iteration 118/3520 Training loss: 2.8458 0.1375 sec/batch\n",
      "Epoch 1/20  Iteration 119/3520 Training loss: 2.8415 0.1383 sec/batch\n",
      "Epoch 1/20  Iteration 120/3520 Training loss: 2.8371 0.1361 sec/batch\n",
      "Epoch 1/20  Iteration 121/3520 Training loss: 2.8330 0.1343 sec/batch\n",
      "Epoch 1/20  Iteration 122/3520 Training loss: 2.8289 0.1359 sec/batch\n",
      "Epoch 1/20  Iteration 123/3520 Training loss: 2.8247 0.1329 sec/batch\n",
      "Epoch 1/20  Iteration 124/3520 Training loss: 2.8207 0.1362 sec/batch\n",
      "Epoch 1/20  Iteration 125/3520 Training loss: 2.8169 0.1368 sec/batch\n",
      "Epoch 1/20  Iteration 126/3520 Training loss: 2.8129 0.1364 sec/batch\n",
      "Epoch 1/20  Iteration 127/3520 Training loss: 2.8087 0.1373 sec/batch\n",
      "Epoch 1/20  Iteration 128/3520 Training loss: 2.8048 0.1379 sec/batch\n",
      "Epoch 1/20  Iteration 129/3520 Training loss: 2.8007 0.1332 sec/batch\n",
      "Epoch 1/20  Iteration 130/3520 Training loss: 2.7969 0.1318 sec/batch\n",
      "Epoch 1/20  Iteration 131/3520 Training loss: 2.7932 0.1385 sec/batch\n",
      "Epoch 1/20  Iteration 132/3520 Training loss: 2.7900 0.1367 sec/batch\n",
      "Epoch 1/20  Iteration 133/3520 Training loss: 2.7869 0.1373 sec/batch\n",
      "Epoch 1/20  Iteration 134/3520 Training loss: 2.7835 0.1387 sec/batch\n",
      "Epoch 1/20  Iteration 135/3520 Training loss: 2.7801 0.1364 sec/batch\n",
      "Epoch 1/20  Iteration 136/3520 Training loss: 2.7768 0.1362 sec/batch\n",
      "Epoch 1/20  Iteration 137/3520 Training loss: 2.7736 0.1367 sec/batch\n",
      "Epoch 1/20  Iteration 138/3520 Training loss: 2.7702 0.1359 sec/batch\n",
      "Epoch 1/20  Iteration 139/3520 Training loss: 2.7669 0.1305 sec/batch\n",
      "Epoch 1/20  Iteration 140/3520 Training loss: 2.7637 0.1372 sec/batch\n",
      "Epoch 1/20  Iteration 141/3520 Training loss: 2.7605 0.1332 sec/batch\n",
      "Epoch 1/20  Iteration 142/3520 Training loss: 2.7572 0.1338 sec/batch\n",
      "Epoch 1/20  Iteration 143/3520 Training loss: 2.7540 0.1364 sec/batch\n",
      "Epoch 1/20  Iteration 144/3520 Training loss: 2.7508 0.1370 sec/batch\n",
      "Epoch 1/20  Iteration 145/3520 Training loss: 2.7476 0.1369 sec/batch\n",
      "Epoch 1/20  Iteration 146/3520 Training loss: 2.7445 0.1368 sec/batch\n",
      "Epoch 1/20  Iteration 147/3520 Training loss: 2.7413 0.1355 sec/batch\n",
      "Epoch 1/20  Iteration 148/3520 Training loss: 2.7384 0.1369 sec/batch\n",
      "Epoch 1/20  Iteration 149/3520 Training loss: 2.7354 0.1370 sec/batch\n",
      "Epoch 1/20  Iteration 150/3520 Training loss: 2.7324 0.1373 sec/batch\n",
      "Epoch 1/20  Iteration 151/3520 Training loss: 2.7293 0.1432 sec/batch\n",
      "Epoch 1/20  Iteration 152/3520 Training loss: 2.7262 0.1360 sec/batch\n",
      "Epoch 1/20  Iteration 153/3520 Training loss: 2.7232 0.1360 sec/batch\n",
      "Epoch 1/20  Iteration 154/3520 Training loss: 2.7201 0.1390 sec/batch\n",
      "Epoch 1/20  Iteration 155/3520 Training loss: 2.7172 0.1316 sec/batch\n",
      "Epoch 1/20  Iteration 156/3520 Training loss: 2.7140 0.1354 sec/batch\n",
      "Epoch 1/20  Iteration 157/3520 Training loss: 2.7110 0.1308 sec/batch\n",
      "Epoch 1/20  Iteration 158/3520 Training loss: 2.7079 0.1320 sec/batch\n",
      "Epoch 1/20  Iteration 159/3520 Training loss: 2.7049 0.1322 sec/batch\n",
      "Epoch 1/20  Iteration 160/3520 Training loss: 2.7020 0.1362 sec/batch\n",
      "Epoch 1/20  Iteration 161/3520 Training loss: 2.6991 0.1330 sec/batch\n",
      "Epoch 1/20  Iteration 162/3520 Training loss: 2.6961 0.1383 sec/batch\n",
      "Epoch 1/20  Iteration 163/3520 Training loss: 2.6933 0.1367 sec/batch\n",
      "Epoch 1/20  Iteration 164/3520 Training loss: 2.6901 0.1362 sec/batch\n",
      "Epoch 1/20  Iteration 165/3520 Training loss: 2.6872 0.1357 sec/batch\n",
      "Epoch 1/20  Iteration 166/3520 Training loss: 2.6843 0.1386 sec/batch\n",
      "Epoch 1/20  Iteration 167/3520 Training loss: 2.6814 0.1355 sec/batch\n",
      "Epoch 1/20  Iteration 168/3520 Training loss: 2.6787 0.1364 sec/batch\n",
      "Epoch 1/20  Iteration 169/3520 Training loss: 2.6759 0.1352 sec/batch\n",
      "Epoch 1/20  Iteration 170/3520 Training loss: 2.6730 0.1358 sec/batch\n",
      "Epoch 1/20  Iteration 171/3520 Training loss: 2.6702 0.1345 sec/batch\n",
      "Epoch 1/20  Iteration 172/3520 Training loss: 2.6674 0.1323 sec/batch\n",
      "Epoch 1/20  Iteration 173/3520 Training loss: 2.6647 0.1383 sec/batch\n",
      "Epoch 1/20  Iteration 174/3520 Training loss: 2.6620 0.1363 sec/batch\n",
      "Epoch 1/20  Iteration 175/3520 Training loss: 2.6591 0.1338 sec/batch\n",
      "Epoch 1/20  Iteration 176/3520 Training loss: 2.6566 0.1360 sec/batch\n",
      "Epoch 2/20  Iteration 177/3520 Training loss: 2.2073 0.1374 sec/batch\n",
      "Epoch 2/20  Iteration 178/3520 Training loss: 2.1762 0.1366 sec/batch\n",
      "Epoch 2/20  Iteration 179/3520 Training loss: 2.1785 0.1375 sec/batch\n",
      "Epoch 2/20  Iteration 180/3520 Training loss: 2.1791 0.1335 sec/batch\n",
      "Epoch 2/20  Iteration 181/3520 Training loss: 2.1780 0.1373 sec/batch\n",
      "Epoch 2/20  Iteration 182/3520 Training loss: 2.1790 0.1363 sec/batch\n",
      "Epoch 2/20  Iteration 183/3520 Training loss: 2.1786 0.1365 sec/batch\n",
      "Epoch 2/20  Iteration 184/3520 Training loss: 2.1806 0.1374 sec/batch\n",
      "Epoch 2/20  Iteration 185/3520 Training loss: 2.1786 0.1337 sec/batch\n",
      "Epoch 2/20  Iteration 186/3520 Training loss: 2.1792 0.1384 sec/batch\n",
      "Epoch 2/20  Iteration 187/3520 Training loss: 2.1780 0.1361 sec/batch\n",
      "Epoch 2/20  Iteration 188/3520 Training loss: 2.1781 0.1360 sec/batch\n",
      "Epoch 2/20  Iteration 189/3520 Training loss: 2.1751 0.1358 sec/batch\n",
      "Epoch 2/20  Iteration 190/3520 Training loss: 2.1745 0.1325 sec/batch\n",
      "Epoch 2/20  Iteration 191/3520 Training loss: 2.1717 0.1331 sec/batch\n",
      "Epoch 2/20  Iteration 192/3520 Training loss: 2.1706 0.1393 sec/batch\n",
      "Epoch 2/20  Iteration 193/3520 Training loss: 2.1688 0.1362 sec/batch\n",
      "Epoch 2/20  Iteration 194/3520 Training loss: 2.1688 0.1363 sec/batch\n",
      "Epoch 2/20  Iteration 195/3520 Training loss: 2.1660 0.1364 sec/batch\n",
      "Epoch 2/20  Iteration 196/3520 Training loss: 2.1658 0.1354 sec/batch\n",
      "Epoch 2/20  Iteration 197/3520 Training loss: 2.1653 0.1362 sec/batch\n",
      "Epoch 2/20  Iteration 198/3520 Training loss: 2.1644 0.1373 sec/batch\n",
      "Epoch 2/20  Iteration 199/3520 Training loss: 2.1655 0.1386 sec/batch\n",
      "Epoch 2/20  Iteration 200/3520 Training loss: 2.1648 0.1368 sec/batch\n",
      "Validation loss: 2.07635 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 201/3520 Training loss: 2.1645 0.1635 sec/batch\n",
      "Epoch 2/20  Iteration 202/3520 Training loss: 2.1624 0.1578 sec/batch\n",
      "Epoch 2/20  Iteration 203/3520 Training loss: 2.1602 0.1465 sec/batch\n",
      "Epoch 2/20  Iteration 204/3520 Training loss: 2.1588 0.1353 sec/batch\n",
      "Epoch 2/20  Iteration 205/3520 Training loss: 2.1569 0.1333 sec/batch\n",
      "Epoch 2/20  Iteration 206/3520 Training loss: 2.1557 0.1374 sec/batch\n",
      "Epoch 2/20  Iteration 207/3520 Training loss: 2.1546 0.1377 sec/batch\n",
      "Epoch 2/20  Iteration 208/3520 Training loss: 2.1531 0.1357 sec/batch\n",
      "Epoch 2/20  Iteration 209/3520 Training loss: 2.1520 0.1370 sec/batch\n",
      "Epoch 2/20  Iteration 210/3520 Training loss: 2.1517 0.1368 sec/batch\n",
      "Epoch 2/20  Iteration 211/3520 Training loss: 2.1502 0.1379 sec/batch\n",
      "Epoch 2/20  Iteration 212/3520 Training loss: 2.1491 0.1338 sec/batch\n",
      "Epoch 2/20  Iteration 213/3520 Training loss: 2.1477 0.1374 sec/batch\n",
      "Epoch 2/20  Iteration 214/3520 Training loss: 2.1477 0.1323 sec/batch\n",
      "Epoch 2/20  Iteration 215/3520 Training loss: 2.1477 0.1359 sec/batch\n",
      "Epoch 2/20  Iteration 216/3520 Training loss: 2.1470 0.1379 sec/batch\n",
      "Epoch 2/20  Iteration 217/3520 Training loss: 2.1461 0.1339 sec/batch\n",
      "Epoch 2/20  Iteration 218/3520 Training loss: 2.1440 0.1310 sec/batch\n",
      "Epoch 2/20  Iteration 219/3520 Training loss: 2.1432 0.1379 sec/batch\n",
      "Epoch 2/20  Iteration 220/3520 Training loss: 2.1415 0.1368 sec/batch\n",
      "Epoch 2/20  Iteration 221/3520 Training loss: 2.1407 0.1341 sec/batch\n",
      "Epoch 2/20  Iteration 222/3520 Training loss: 2.1390 0.1359 sec/batch\n",
      "Epoch 2/20  Iteration 223/3520 Training loss: 2.1375 0.1384 sec/batch\n",
      "Epoch 2/20  Iteration 224/3520 Training loss: 2.1364 0.1343 sec/batch\n",
      "Epoch 2/20  Iteration 225/3520 Training loss: 2.1356 0.1367 sec/batch\n",
      "Epoch 2/20  Iteration 226/3520 Training loss: 2.1348 0.1359 sec/batch\n",
      "Epoch 2/20  Iteration 227/3520 Training loss: 2.1338 0.1355 sec/batch\n",
      "Epoch 2/20  Iteration 228/3520 Training loss: 2.1329 0.1346 sec/batch\n",
      "Epoch 2/20  Iteration 229/3520 Training loss: 2.1322 0.1360 sec/batch\n",
      "Epoch 2/20  Iteration 230/3520 Training loss: 2.1314 0.1357 sec/batch\n",
      "Epoch 2/20  Iteration 231/3520 Training loss: 2.1298 0.1379 sec/batch\n",
      "Epoch 2/20  Iteration 232/3520 Training loss: 2.1281 0.1376 sec/batch\n",
      "Epoch 2/20  Iteration 233/3520 Training loss: 2.1268 0.1389 sec/batch\n",
      "Epoch 2/20  Iteration 234/3520 Training loss: 2.1256 0.1354 sec/batch\n",
      "Epoch 2/20  Iteration 235/3520 Training loss: 2.1241 0.1358 sec/batch\n",
      "Epoch 2/20  Iteration 236/3520 Training loss: 2.1233 0.1369 sec/batch\n",
      "Epoch 2/20  Iteration 237/3520 Training loss: 2.1221 0.1365 sec/batch\n",
      "Epoch 2/20  Iteration 238/3520 Training loss: 2.1213 0.1371 sec/batch\n",
      "Epoch 2/20  Iteration 239/3520 Training loss: 2.1201 0.1378 sec/batch\n",
      "Epoch 2/20  Iteration 240/3520 Training loss: 2.1190 0.1366 sec/batch\n",
      "Epoch 2/20  Iteration 241/3520 Training loss: 2.1176 0.1319 sec/batch\n",
      "Epoch 2/20  Iteration 242/3520 Training loss: 2.1164 0.1363 sec/batch\n",
      "Epoch 2/20  Iteration 243/3520 Training loss: 2.1152 0.1337 sec/batch\n",
      "Epoch 2/20  Iteration 244/3520 Training loss: 2.1140 0.1352 sec/batch\n",
      "Epoch 2/20  Iteration 245/3520 Training loss: 2.1128 0.1357 sec/batch\n",
      "Epoch 2/20  Iteration 246/3520 Training loss: 2.1121 0.1352 sec/batch\n",
      "Epoch 2/20  Iteration 247/3520 Training loss: 2.1109 0.1382 sec/batch\n",
      "Epoch 2/20  Iteration 248/3520 Training loss: 2.1098 0.1366 sec/batch\n",
      "Epoch 2/20  Iteration 249/3520 Training loss: 2.1086 0.1366 sec/batch\n",
      "Epoch 2/20  Iteration 250/3520 Training loss: 2.1071 0.1381 sec/batch\n",
      "Epoch 2/20  Iteration 251/3520 Training loss: 2.1060 0.1337 sec/batch\n",
      "Epoch 2/20  Iteration 252/3520 Training loss: 2.1051 0.1309 sec/batch\n",
      "Epoch 2/20  Iteration 253/3520 Training loss: 2.1041 0.1330 sec/batch\n",
      "Epoch 2/20  Iteration 254/3520 Training loss: 2.1031 0.1366 sec/batch\n",
      "Epoch 2/20  Iteration 255/3520 Training loss: 2.1017 0.1362 sec/batch\n",
      "Epoch 2/20  Iteration 256/3520 Training loss: 2.1007 0.1486 sec/batch\n",
      "Epoch 2/20  Iteration 257/3520 Training loss: 2.0991 0.1374 sec/batch\n",
      "Epoch 2/20  Iteration 258/3520 Training loss: 2.0983 0.1359 sec/batch\n",
      "Epoch 2/20  Iteration 259/3520 Training loss: 2.0970 0.1375 sec/batch\n",
      "Epoch 2/20  Iteration 260/3520 Training loss: 2.0963 0.1357 sec/batch\n",
      "Epoch 2/20  Iteration 261/3520 Training loss: 2.0952 0.1364 sec/batch\n",
      "Epoch 2/20  Iteration 262/3520 Training loss: 2.0940 0.1353 sec/batch\n",
      "Epoch 2/20  Iteration 263/3520 Training loss: 2.0933 0.1359 sec/batch\n",
      "Epoch 2/20  Iteration 264/3520 Training loss: 2.0922 0.1374 sec/batch\n",
      "Epoch 2/20  Iteration 265/3520 Training loss: 2.0913 0.1330 sec/batch\n",
      "Epoch 2/20  Iteration 266/3520 Training loss: 2.0903 0.1367 sec/batch\n",
      "Epoch 2/20  Iteration 267/3520 Training loss: 2.0895 0.1362 sec/batch\n",
      "Epoch 2/20  Iteration 268/3520 Training loss: 2.0889 0.1323 sec/batch\n",
      "Epoch 2/20  Iteration 269/3520 Training loss: 2.0881 0.1366 sec/batch\n",
      "Epoch 2/20  Iteration 270/3520 Training loss: 2.0870 0.1375 sec/batch\n",
      "Epoch 2/20  Iteration 271/3520 Training loss: 2.0860 0.1367 sec/batch\n",
      "Epoch 2/20  Iteration 272/3520 Training loss: 2.0848 0.1321 sec/batch\n",
      "Epoch 2/20  Iteration 273/3520 Training loss: 2.0842 0.1330 sec/batch\n",
      "Epoch 2/20  Iteration 274/3520 Training loss: 2.0833 0.1388 sec/batch\n",
      "Epoch 2/20  Iteration 275/3520 Training loss: 2.0825 0.1365 sec/batch\n",
      "Epoch 2/20  Iteration 276/3520 Training loss: 2.0813 0.1378 sec/batch\n",
      "Epoch 2/20  Iteration 277/3520 Training loss: 2.0804 0.1369 sec/batch\n",
      "Epoch 2/20  Iteration 278/3520 Training loss: 2.0796 0.1363 sec/batch\n",
      "Epoch 2/20  Iteration 279/3520 Training loss: 2.0784 0.1368 sec/batch\n",
      "Epoch 2/20  Iteration 280/3520 Training loss: 2.0772 0.1303 sec/batch\n",
      "Epoch 2/20  Iteration 281/3520 Training loss: 2.0760 0.1374 sec/batch\n",
      "Epoch 2/20  Iteration 282/3520 Training loss: 2.0753 0.1376 sec/batch\n",
      "Epoch 2/20  Iteration 283/3520 Training loss: 2.0742 0.1362 sec/batch\n",
      "Epoch 2/20  Iteration 284/3520 Training loss: 2.0729 0.1368 sec/batch\n",
      "Epoch 2/20  Iteration 285/3520 Training loss: 2.0719 0.1314 sec/batch\n",
      "Epoch 2/20  Iteration 286/3520 Training loss: 2.0708 0.1339 sec/batch\n",
      "Epoch 2/20  Iteration 287/3520 Training loss: 2.0697 0.1366 sec/batch\n",
      "Epoch 2/20  Iteration 288/3520 Training loss: 2.0689 0.1375 sec/batch\n",
      "Epoch 2/20  Iteration 289/3520 Training loss: 2.0681 0.1376 sec/batch\n",
      "Epoch 2/20  Iteration 290/3520 Training loss: 2.0671 0.1379 sec/batch\n",
      "Epoch 2/20  Iteration 291/3520 Training loss: 2.0660 0.1364 sec/batch\n",
      "Epoch 2/20  Iteration 292/3520 Training loss: 2.0651 0.1366 sec/batch\n",
      "Epoch 2/20  Iteration 293/3520 Training loss: 2.0641 0.1379 sec/batch\n",
      "Epoch 2/20  Iteration 294/3520 Training loss: 2.0633 0.1373 sec/batch\n",
      "Epoch 2/20  Iteration 295/3520 Training loss: 2.0624 0.1376 sec/batch\n",
      "Epoch 2/20  Iteration 296/3520 Training loss: 2.0615 0.1376 sec/batch\n",
      "Epoch 2/20  Iteration 297/3520 Training loss: 2.0607 0.1365 sec/batch\n",
      "Epoch 2/20  Iteration 298/3520 Training loss: 2.0598 0.1364 sec/batch\n",
      "Epoch 2/20  Iteration 299/3520 Training loss: 2.0588 0.1363 sec/batch\n",
      "Epoch 2/20  Iteration 300/3520 Training loss: 2.0579 0.1352 sec/batch\n",
      "Epoch 2/20  Iteration 301/3520 Training loss: 2.0572 0.1366 sec/batch\n",
      "Epoch 2/20  Iteration 302/3520 Training loss: 2.0561 0.1375 sec/batch\n",
      "Epoch 2/20  Iteration 303/3520 Training loss: 2.0549 0.1360 sec/batch\n",
      "Epoch 2/20  Iteration 304/3520 Training loss: 2.0539 0.1364 sec/batch\n",
      "Epoch 2/20  Iteration 305/3520 Training loss: 2.0529 0.1362 sec/batch\n",
      "Epoch 2/20  Iteration 306/3520 Training loss: 2.0522 0.1377 sec/batch\n",
      "Epoch 2/20  Iteration 307/3520 Training loss: 2.0513 0.1352 sec/batch\n",
      "Epoch 2/20  Iteration 308/3520 Training loss: 2.0506 0.1381 sec/batch\n",
      "Epoch 2/20  Iteration 309/3520 Training loss: 2.0500 0.1336 sec/batch\n",
      "Epoch 2/20  Iteration 310/3520 Training loss: 2.0492 0.1354 sec/batch\n",
      "Epoch 2/20  Iteration 311/3520 Training loss: 2.0484 0.1370 sec/batch\n",
      "Epoch 2/20  Iteration 312/3520 Training loss: 2.0477 0.1363 sec/batch\n",
      "Epoch 2/20  Iteration 313/3520 Training loss: 2.0470 0.1386 sec/batch\n",
      "Epoch 2/20  Iteration 314/3520 Training loss: 2.0461 0.1380 sec/batch\n",
      "Epoch 2/20  Iteration 315/3520 Training loss: 2.0454 0.1383 sec/batch\n",
      "Epoch 2/20  Iteration 316/3520 Training loss: 2.0448 0.1351 sec/batch\n",
      "Epoch 2/20  Iteration 317/3520 Training loss: 2.0440 0.1371 sec/batch\n",
      "Epoch 2/20  Iteration 318/3520 Training loss: 2.0432 0.1493 sec/batch\n",
      "Epoch 2/20  Iteration 319/3520 Training loss: 2.0424 0.1337 sec/batch\n",
      "Epoch 2/20  Iteration 320/3520 Training loss: 2.0415 0.1294 sec/batch\n",
      "Epoch 2/20  Iteration 321/3520 Training loss: 2.0408 0.1367 sec/batch\n",
      "Epoch 2/20  Iteration 322/3520 Training loss: 2.0401 0.1357 sec/batch\n",
      "Epoch 2/20  Iteration 323/3520 Training loss: 2.0393 0.1358 sec/batch\n",
      "Epoch 2/20  Iteration 324/3520 Training loss: 2.0387 0.1368 sec/batch\n",
      "Epoch 2/20  Iteration 325/3520 Training loss: 2.0378 0.1363 sec/batch\n",
      "Epoch 2/20  Iteration 326/3520 Training loss: 2.0373 0.1363 sec/batch\n",
      "Epoch 2/20  Iteration 327/3520 Training loss: 2.0365 0.1334 sec/batch\n",
      "Epoch 2/20  Iteration 328/3520 Training loss: 2.0356 0.1373 sec/batch\n",
      "Epoch 2/20  Iteration 329/3520 Training loss: 2.0347 0.1370 sec/batch\n",
      "Epoch 2/20  Iteration 330/3520 Training loss: 2.0338 0.1361 sec/batch\n",
      "Epoch 2/20  Iteration 331/3520 Training loss: 2.0330 0.1358 sec/batch\n",
      "Epoch 2/20  Iteration 332/3520 Training loss: 2.0321 0.1334 sec/batch\n",
      "Epoch 2/20  Iteration 333/3520 Training loss: 2.0312 0.1364 sec/batch\n",
      "Epoch 2/20  Iteration 334/3520 Training loss: 2.0304 0.1345 sec/batch\n",
      "Epoch 2/20  Iteration 335/3520 Training loss: 2.0296 0.1370 sec/batch\n",
      "Epoch 2/20  Iteration 336/3520 Training loss: 2.0289 0.1367 sec/batch\n",
      "Epoch 2/20  Iteration 337/3520 Training loss: 2.0280 0.1314 sec/batch\n",
      "Epoch 2/20  Iteration 338/3520 Training loss: 2.0272 0.1370 sec/batch\n",
      "Epoch 2/20  Iteration 339/3520 Training loss: 2.0265 0.1368 sec/batch\n",
      "Epoch 2/20  Iteration 340/3520 Training loss: 2.0256 0.1393 sec/batch\n",
      "Epoch 2/20  Iteration 341/3520 Training loss: 2.0246 0.1316 sec/batch\n",
      "Epoch 2/20  Iteration 342/3520 Training loss: 2.0237 0.1331 sec/batch\n",
      "Epoch 2/20  Iteration 343/3520 Training loss: 2.0228 0.1449 sec/batch\n",
      "Epoch 2/20  Iteration 344/3520 Training loss: 2.0220 0.1312 sec/batch\n",
      "Epoch 2/20  Iteration 345/3520 Training loss: 2.0213 0.1355 sec/batch\n",
      "Epoch 2/20  Iteration 346/3520 Training loss: 2.0205 0.1373 sec/batch\n",
      "Epoch 2/20  Iteration 347/3520 Training loss: 2.0198 0.1426 sec/batch\n",
      "Epoch 2/20  Iteration 348/3520 Training loss: 2.0190 0.1370 sec/batch\n",
      "Epoch 2/20  Iteration 349/3520 Training loss: 2.0182 0.1366 sec/batch\n",
      "Epoch 2/20  Iteration 350/3520 Training loss: 2.0175 0.1349 sec/batch\n",
      "Epoch 2/20  Iteration 351/3520 Training loss: 2.0166 0.1383 sec/batch\n",
      "Epoch 2/20  Iteration 352/3520 Training loss: 2.0160 0.1366 sec/batch\n",
      "Epoch 3/20  Iteration 353/3520 Training loss: 1.9021 0.1360 sec/batch\n",
      "Epoch 3/20  Iteration 354/3520 Training loss: 1.8614 0.1350 sec/batch\n",
      "Epoch 3/20  Iteration 355/3520 Training loss: 1.8659 0.1382 sec/batch\n",
      "Epoch 3/20  Iteration 356/3520 Training loss: 1.8714 0.1376 sec/batch\n",
      "Epoch 3/20  Iteration 357/3520 Training loss: 1.8711 0.1356 sec/batch\n",
      "Epoch 3/20  Iteration 358/3520 Training loss: 1.8723 0.1370 sec/batch\n",
      "Epoch 3/20  Iteration 359/3520 Training loss: 1.8722 0.1391 sec/batch\n",
      "Epoch 3/20  Iteration 360/3520 Training loss: 1.8753 0.1370 sec/batch\n",
      "Epoch 3/20  Iteration 361/3520 Training loss: 1.8725 0.1354 sec/batch\n",
      "Epoch 3/20  Iteration 362/3520 Training loss: 1.8732 0.1365 sec/batch\n",
      "Epoch 3/20  Iteration 363/3520 Training loss: 1.8734 0.1369 sec/batch\n",
      "Epoch 3/20  Iteration 364/3520 Training loss: 1.8740 0.1355 sec/batch\n",
      "Epoch 3/20  Iteration 365/3520 Training loss: 1.8709 0.1375 sec/batch\n",
      "Epoch 3/20  Iteration 366/3520 Training loss: 1.8712 0.1379 sec/batch\n",
      "Epoch 3/20  Iteration 367/3520 Training loss: 1.8700 0.1362 sec/batch\n",
      "Epoch 3/20  Iteration 368/3520 Training loss: 1.8699 0.1364 sec/batch\n",
      "Epoch 3/20  Iteration 369/3520 Training loss: 1.8695 0.1371 sec/batch\n",
      "Epoch 3/20  Iteration 370/3520 Training loss: 1.8699 0.1371 sec/batch\n",
      "Epoch 3/20  Iteration 371/3520 Training loss: 1.8686 0.1360 sec/batch\n",
      "Epoch 3/20  Iteration 372/3520 Training loss: 1.8693 0.1369 sec/batch\n",
      "Epoch 3/20  Iteration 373/3520 Training loss: 1.8693 0.1361 sec/batch\n",
      "Epoch 3/20  Iteration 374/3520 Training loss: 1.8685 0.1325 sec/batch\n",
      "Epoch 3/20  Iteration 375/3520 Training loss: 1.8692 0.1389 sec/batch\n",
      "Epoch 3/20  Iteration 376/3520 Training loss: 1.8683 0.1382 sec/batch\n",
      "Epoch 3/20  Iteration 377/3520 Training loss: 1.8680 0.1385 sec/batch\n",
      "Epoch 3/20  Iteration 378/3520 Training loss: 1.8666 0.1383 sec/batch\n",
      "Epoch 3/20  Iteration 379/3520 Training loss: 1.8646 0.1377 sec/batch\n",
      "Epoch 3/20  Iteration 380/3520 Training loss: 1.8640 0.1366 sec/batch\n",
      "Epoch 3/20  Iteration 381/3520 Training loss: 1.8629 0.1350 sec/batch\n",
      "Epoch 3/20  Iteration 382/3520 Training loss: 1.8623 0.1343 sec/batch\n",
      "Epoch 3/20  Iteration 383/3520 Training loss: 1.8620 0.1360 sec/batch\n",
      "Epoch 3/20  Iteration 384/3520 Training loss: 1.8610 0.1350 sec/batch\n",
      "Epoch 3/20  Iteration 385/3520 Training loss: 1.8607 0.1356 sec/batch\n",
      "Epoch 3/20  Iteration 386/3520 Training loss: 1.8599 0.1370 sec/batch\n",
      "Epoch 3/20  Iteration 387/3520 Training loss: 1.8593 0.1386 sec/batch\n",
      "Epoch 3/20  Iteration 388/3520 Training loss: 1.8589 0.1321 sec/batch\n",
      "Epoch 3/20  Iteration 389/3520 Training loss: 1.8575 0.1372 sec/batch\n",
      "Epoch 3/20  Iteration 390/3520 Training loss: 1.8579 0.1374 sec/batch\n",
      "Epoch 3/20  Iteration 391/3520 Training loss: 1.8586 0.1372 sec/batch\n",
      "Epoch 3/20  Iteration 392/3520 Training loss: 1.8581 0.1384 sec/batch\n",
      "Epoch 3/20  Iteration 393/3520 Training loss: 1.8575 0.1368 sec/batch\n",
      "Epoch 3/20  Iteration 394/3520 Training loss: 1.8557 0.1368 sec/batch\n",
      "Epoch 3/20  Iteration 395/3520 Training loss: 1.8554 0.1302 sec/batch\n",
      "Epoch 3/20  Iteration 396/3520 Training loss: 1.8541 0.1383 sec/batch\n",
      "Epoch 3/20  Iteration 397/3520 Training loss: 1.8535 0.1359 sec/batch\n",
      "Epoch 3/20  Iteration 398/3520 Training loss: 1.8524 0.1311 sec/batch\n",
      "Epoch 3/20  Iteration 399/3520 Training loss: 1.8518 0.1364 sec/batch\n",
      "Epoch 3/20  Iteration 400/3520 Training loss: 1.8511 0.1348 sec/batch\n",
      "Validation loss: 1.74655 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 401/3520 Training loss: 1.8515 0.1369 sec/batch\n",
      "Epoch 3/20  Iteration 402/3520 Training loss: 1.8509 0.1380 sec/batch\n",
      "Epoch 3/20  Iteration 403/3520 Training loss: 1.8504 0.1361 sec/batch\n",
      "Epoch 3/20  Iteration 404/3520 Training loss: 1.8498 0.1388 sec/batch\n",
      "Epoch 3/20  Iteration 405/3520 Training loss: 1.8497 0.1362 sec/batch\n",
      "Epoch 3/20  Iteration 406/3520 Training loss: 1.8492 0.1359 sec/batch\n",
      "Epoch 3/20  Iteration 407/3520 Training loss: 1.8481 0.1368 sec/batch\n",
      "Epoch 3/20  Iteration 408/3520 Training loss: 1.8471 0.1377 sec/batch\n",
      "Epoch 3/20  Iteration 409/3520 Training loss: 1.8462 0.1383 sec/batch\n",
      "Epoch 3/20  Iteration 410/3520 Training loss: 1.8455 0.1342 sec/batch\n",
      "Epoch 3/20  Iteration 411/3520 Training loss: 1.8445 0.1369 sec/batch\n",
      "Epoch 3/20  Iteration 412/3520 Training loss: 1.8445 0.1380 sec/batch\n",
      "Epoch 3/20  Iteration 413/3520 Training loss: 1.8437 0.1394 sec/batch\n",
      "Epoch 3/20  Iteration 414/3520 Training loss: 1.8433 0.1385 sec/batch\n",
      "Epoch 3/20  Iteration 415/3520 Training loss: 1.8426 0.1417 sec/batch\n",
      "Epoch 3/20  Iteration 416/3520 Training loss: 1.8422 0.1370 sec/batch\n",
      "Epoch 3/20  Iteration 417/3520 Training loss: 1.8412 0.1373 sec/batch\n",
      "Epoch 3/20  Iteration 418/3520 Training loss: 1.8407 0.1353 sec/batch\n",
      "Epoch 3/20  Iteration 419/3520 Training loss: 1.8401 0.1367 sec/batch\n",
      "Epoch 3/20  Iteration 420/3520 Training loss: 1.8393 0.1379 sec/batch\n",
      "Epoch 3/20  Iteration 421/3520 Training loss: 1.8387 0.1321 sec/batch\n",
      "Epoch 3/20  Iteration 422/3520 Training loss: 1.8386 0.1371 sec/batch\n",
      "Epoch 3/20  Iteration 423/3520 Training loss: 1.8379 0.1381 sec/batch\n",
      "Epoch 3/20  Iteration 424/3520 Training loss: 1.8375 0.1357 sec/batch\n",
      "Epoch 3/20  Iteration 425/3520 Training loss: 1.8367 0.1379 sec/batch\n",
      "Epoch 3/20  Iteration 426/3520 Training loss: 1.8357 0.1370 sec/batch\n",
      "Epoch 3/20  Iteration 427/3520 Training loss: 1.8351 0.1358 sec/batch\n",
      "Epoch 3/20  Iteration 428/3520 Training loss: 1.8348 0.1372 sec/batch\n",
      "Epoch 3/20  Iteration 429/3520 Training loss: 1.8345 0.1353 sec/batch\n",
      "Epoch 3/20  Iteration 430/3520 Training loss: 1.8343 0.1367 sec/batch\n",
      "Epoch 3/20  Iteration 431/3520 Training loss: 1.8335 0.1383 sec/batch\n",
      "Epoch 3/20  Iteration 432/3520 Training loss: 1.8330 0.1362 sec/batch\n",
      "Epoch 3/20  Iteration 433/3520 Training loss: 1.8321 0.1376 sec/batch\n",
      "Epoch 3/20  Iteration 434/3520 Training loss: 1.8318 0.1387 sec/batch\n",
      "Epoch 3/20  Iteration 435/3520 Training loss: 1.8309 0.1392 sec/batch\n",
      "Epoch 3/20  Iteration 436/3520 Training loss: 1.8307 0.1312 sec/batch\n",
      "Epoch 3/20  Iteration 437/3520 Training loss: 1.8302 0.1354 sec/batch\n",
      "Epoch 3/20  Iteration 438/3520 Training loss: 1.8294 0.1356 sec/batch\n",
      "Epoch 3/20  Iteration 439/3520 Training loss: 1.8292 0.1367 sec/batch\n",
      "Epoch 3/20  Iteration 440/3520 Training loss: 1.8287 0.1361 sec/batch\n",
      "Epoch 3/20  Iteration 441/3520 Training loss: 1.8282 0.1338 sec/batch\n",
      "Epoch 3/20  Iteration 442/3520 Training loss: 1.8276 0.1363 sec/batch\n",
      "Epoch 3/20  Iteration 443/3520 Training loss: 1.8272 0.1361 sec/batch\n",
      "Epoch 3/20  Iteration 444/3520 Training loss: 1.8271 0.1390 sec/batch\n",
      "Epoch 3/20  Iteration 445/3520 Training loss: 1.8265 0.1363 sec/batch\n",
      "Epoch 3/20  Iteration 446/3520 Training loss: 1.8259 0.1354 sec/batch\n",
      "Epoch 3/20  Iteration 447/3520 Training loss: 1.8253 0.1364 sec/batch\n",
      "Epoch 3/20  Iteration 448/3520 Training loss: 1.8243 0.1365 sec/batch\n",
      "Epoch 3/20  Iteration 449/3520 Training loss: 1.8243 0.1345 sec/batch\n",
      "Epoch 3/20  Iteration 450/3520 Training loss: 1.8240 0.1377 sec/batch\n",
      "Epoch 3/20  Iteration 451/3520 Training loss: 1.8236 0.1370 sec/batch\n",
      "Epoch 3/20  Iteration 452/3520 Training loss: 1.8229 0.1377 sec/batch\n",
      "Epoch 3/20  Iteration 453/3520 Training loss: 1.8226 0.1377 sec/batch\n",
      "Epoch 3/20  Iteration 454/3520 Training loss: 1.8222 0.1324 sec/batch\n",
      "Epoch 3/20  Iteration 455/3520 Training loss: 1.8214 0.1359 sec/batch\n",
      "Epoch 3/20  Iteration 456/3520 Training loss: 1.8205 0.1384 sec/batch\n",
      "Epoch 3/20  Iteration 457/3520 Training loss: 1.8197 0.1363 sec/batch\n",
      "Epoch 3/20  Iteration 458/3520 Training loss: 1.8193 0.1340 sec/batch\n",
      "Epoch 3/20  Iteration 459/3520 Training loss: 1.8186 0.1364 sec/batch\n",
      "Epoch 3/20  Iteration 460/3520 Training loss: 1.8177 0.1384 sec/batch\n",
      "Epoch 3/20  Iteration 461/3520 Training loss: 1.8173 0.1365 sec/batch\n",
      "Epoch 3/20  Iteration 462/3520 Training loss: 1.8167 0.1359 sec/batch\n",
      "Epoch 3/20  Iteration 463/3520 Training loss: 1.8161 0.1382 sec/batch\n",
      "Epoch 3/20  Iteration 464/3520 Training loss: 1.8157 0.1375 sec/batch\n",
      "Epoch 3/20  Iteration 465/3520 Training loss: 1.8154 0.1317 sec/batch\n",
      "Epoch 3/20  Iteration 466/3520 Training loss: 1.8148 0.1365 sec/batch\n",
      "Epoch 3/20  Iteration 467/3520 Training loss: 1.8141 0.1378 sec/batch\n",
      "Epoch 3/20  Iteration 468/3520 Training loss: 1.8136 0.1363 sec/batch\n",
      "Epoch 3/20  Iteration 469/3520 Training loss: 1.8130 0.1296 sec/batch\n",
      "Epoch 3/20  Iteration 470/3520 Training loss: 1.8127 0.1385 sec/batch\n",
      "Epoch 3/20  Iteration 471/3520 Training loss: 1.8121 0.1376 sec/batch\n",
      "Epoch 3/20  Iteration 472/3520 Training loss: 1.8116 0.1366 sec/batch\n",
      "Epoch 3/20  Iteration 473/3520 Training loss: 1.8112 0.1358 sec/batch\n",
      "Epoch 3/20  Iteration 474/3520 Training loss: 1.8107 0.1377 sec/batch\n",
      "Epoch 3/20  Iteration 475/3520 Training loss: 1.8102 0.1367 sec/batch\n",
      "Epoch 3/20  Iteration 476/3520 Training loss: 1.8098 0.1349 sec/batch\n",
      "Epoch 3/20  Iteration 477/3520 Training loss: 1.8095 0.1372 sec/batch\n",
      "Epoch 3/20  Iteration 478/3520 Training loss: 1.8088 0.1361 sec/batch\n",
      "Epoch 3/20  Iteration 479/3520 Training loss: 1.8081 0.1363 sec/batch\n",
      "Epoch 3/20  Iteration 480/3520 Training loss: 1.8075 0.1360 sec/batch\n",
      "Epoch 3/20  Iteration 481/3520 Training loss: 1.8069 0.1438 sec/batch\n",
      "Epoch 3/20  Iteration 482/3520 Training loss: 1.8067 0.1371 sec/batch\n",
      "Epoch 3/20  Iteration 483/3520 Training loss: 1.8061 0.1368 sec/batch\n",
      "Epoch 3/20  Iteration 484/3520 Training loss: 1.8060 0.1355 sec/batch\n",
      "Epoch 3/20  Iteration 485/3520 Training loss: 1.8058 0.1363 sec/batch\n",
      "Epoch 3/20  Iteration 486/3520 Training loss: 1.8054 0.1495 sec/batch\n",
      "Epoch 3/20  Iteration 487/3520 Training loss: 1.8050 0.1378 sec/batch\n",
      "Epoch 3/20  Iteration 488/3520 Training loss: 1.8046 0.1358 sec/batch\n",
      "Epoch 3/20  Iteration 489/3520 Training loss: 1.8042 0.1313 sec/batch\n",
      "Epoch 3/20  Iteration 490/3520 Training loss: 1.8037 0.1369 sec/batch\n",
      "Epoch 3/20  Iteration 491/3520 Training loss: 1.8034 0.1350 sec/batch\n",
      "Epoch 3/20  Iteration 492/3520 Training loss: 1.8031 0.1347 sec/batch\n",
      "Epoch 3/20  Iteration 493/3520 Training loss: 1.8027 0.1339 sec/batch\n",
      "Epoch 3/20  Iteration 494/3520 Training loss: 1.8022 0.1359 sec/batch\n",
      "Epoch 3/20  Iteration 495/3520 Training loss: 1.8016 0.1365 sec/batch\n",
      "Epoch 3/20  Iteration 496/3520 Training loss: 1.8011 0.1365 sec/batch\n",
      "Epoch 3/20  Iteration 497/3520 Training loss: 1.8007 0.1387 sec/batch\n",
      "Epoch 3/20  Iteration 498/3520 Training loss: 1.8004 0.1335 sec/batch\n",
      "Epoch 3/20  Iteration 499/3520 Training loss: 1.7999 0.1326 sec/batch\n",
      "Epoch 3/20  Iteration 500/3520 Training loss: 1.7996 0.1327 sec/batch\n",
      "Epoch 3/20  Iteration 501/3520 Training loss: 1.7992 0.1387 sec/batch\n",
      "Epoch 3/20  Iteration 502/3520 Training loss: 1.7990 0.1327 sec/batch\n",
      "Epoch 3/20  Iteration 503/3520 Training loss: 1.7985 0.1384 sec/batch\n",
      "Epoch 3/20  Iteration 504/3520 Training loss: 1.7980 0.1388 sec/batch\n",
      "Epoch 3/20  Iteration 505/3520 Training loss: 1.7975 0.1310 sec/batch\n",
      "Epoch 3/20  Iteration 506/3520 Training loss: 1.7969 0.1358 sec/batch\n",
      "Epoch 3/20  Iteration 507/3520 Training loss: 1.7964 0.1386 sec/batch\n",
      "Epoch 3/20  Iteration 508/3520 Training loss: 1.7959 0.1386 sec/batch\n",
      "Epoch 3/20  Iteration 509/3520 Training loss: 1.7954 0.1368 sec/batch\n",
      "Epoch 3/20  Iteration 510/3520 Training loss: 1.7950 0.1334 sec/batch\n",
      "Epoch 3/20  Iteration 511/3520 Training loss: 1.7946 0.1367 sec/batch\n",
      "Epoch 3/20  Iteration 512/3520 Training loss: 1.7943 0.1302 sec/batch\n",
      "Epoch 3/20  Iteration 513/3520 Training loss: 1.7937 0.1342 sec/batch\n",
      "Epoch 3/20  Iteration 514/3520 Training loss: 1.7932 0.1388 sec/batch\n",
      "Epoch 3/20  Iteration 515/3520 Training loss: 1.7929 0.1387 sec/batch\n",
      "Epoch 3/20  Iteration 516/3520 Training loss: 1.7923 0.1370 sec/batch\n",
      "Epoch 3/20  Iteration 517/3520 Training loss: 1.7916 0.1364 sec/batch\n",
      "Epoch 3/20  Iteration 518/3520 Training loss: 1.7910 0.1369 sec/batch\n",
      "Epoch 3/20  Iteration 519/3520 Training loss: 1.7905 0.1375 sec/batch\n",
      "Epoch 3/20  Iteration 520/3520 Training loss: 1.7900 0.1330 sec/batch\n",
      "Epoch 3/20  Iteration 521/3520 Training loss: 1.7897 0.1372 sec/batch\n",
      "Epoch 3/20  Iteration 522/3520 Training loss: 1.7892 0.1331 sec/batch\n",
      "Epoch 3/20  Iteration 523/3520 Training loss: 1.7888 0.1376 sec/batch\n",
      "Epoch 3/20  Iteration 524/3520 Training loss: 1.7883 0.1360 sec/batch\n",
      "Epoch 3/20  Iteration 525/3520 Training loss: 1.7879 0.1369 sec/batch\n",
      "Epoch 3/20  Iteration 526/3520 Training loss: 1.7875 0.1350 sec/batch\n",
      "Epoch 3/20  Iteration 527/3520 Training loss: 1.7870 0.1362 sec/batch\n",
      "Epoch 3/20  Iteration 528/3520 Training loss: 1.7868 0.1377 sec/batch\n",
      "Epoch 4/20  Iteration 529/3520 Training loss: 1.7348 0.1361 sec/batch\n",
      "Epoch 4/20  Iteration 530/3520 Training loss: 1.6917 0.1384 sec/batch\n",
      "Epoch 4/20  Iteration 531/3520 Training loss: 1.6946 0.1338 sec/batch\n",
      "Epoch 4/20  Iteration 532/3520 Training loss: 1.6983 0.1360 sec/batch\n",
      "Epoch 4/20  Iteration 533/3520 Training loss: 1.7003 0.1360 sec/batch\n",
      "Epoch 4/20  Iteration 534/3520 Training loss: 1.7014 0.1363 sec/batch\n",
      "Epoch 4/20  Iteration 535/3520 Training loss: 1.7009 0.1355 sec/batch\n",
      "Epoch 4/20  Iteration 536/3520 Training loss: 1.7037 0.1360 sec/batch\n",
      "Epoch 4/20  Iteration 537/3520 Training loss: 1.7012 0.1347 sec/batch\n",
      "Epoch 4/20  Iteration 538/3520 Training loss: 1.7012 0.1378 sec/batch\n",
      "Epoch 4/20  Iteration 539/3520 Training loss: 1.7026 0.1311 sec/batch\n",
      "Epoch 4/20  Iteration 540/3520 Training loss: 1.7032 0.1387 sec/batch\n",
      "Epoch 4/20  Iteration 541/3520 Training loss: 1.7013 0.1382 sec/batch\n",
      "Epoch 4/20  Iteration 542/3520 Training loss: 1.7021 0.1351 sec/batch\n",
      "Epoch 4/20  Iteration 543/3520 Training loss: 1.7008 0.1372 sec/batch\n",
      "Epoch 4/20  Iteration 544/3520 Training loss: 1.7003 0.1357 sec/batch\n",
      "Epoch 4/20  Iteration 545/3520 Training loss: 1.7005 0.1362 sec/batch\n",
      "Epoch 4/20  Iteration 546/3520 Training loss: 1.7011 0.1365 sec/batch\n",
      "Epoch 4/20  Iteration 547/3520 Training loss: 1.6996 0.1374 sec/batch\n",
      "Epoch 4/20  Iteration 548/3520 Training loss: 1.7009 0.1375 sec/batch\n",
      "Epoch 4/20  Iteration 549/3520 Training loss: 1.7011 0.1378 sec/batch\n",
      "Epoch 4/20  Iteration 550/3520 Training loss: 1.7009 0.1366 sec/batch\n",
      "Epoch 4/20  Iteration 551/3520 Training loss: 1.7014 0.1401 sec/batch\n",
      "Epoch 4/20  Iteration 552/3520 Training loss: 1.7012 0.1374 sec/batch\n",
      "Epoch 4/20  Iteration 553/3520 Training loss: 1.7013 0.1365 sec/batch\n",
      "Epoch 4/20  Iteration 554/3520 Training loss: 1.7002 0.1376 sec/batch\n",
      "Epoch 4/20  Iteration 555/3520 Training loss: 1.6982 0.1372 sec/batch\n",
      "Epoch 4/20  Iteration 556/3520 Training loss: 1.6984 0.1367 sec/batch\n",
      "Epoch 4/20  Iteration 557/3520 Training loss: 1.6977 0.1363 sec/batch\n",
      "Epoch 4/20  Iteration 558/3520 Training loss: 1.6974 0.1374 sec/batch\n",
      "Epoch 4/20  Iteration 559/3520 Training loss: 1.6970 0.1388 sec/batch\n",
      "Epoch 4/20  Iteration 560/3520 Training loss: 1.6966 0.1350 sec/batch\n",
      "Epoch 4/20  Iteration 561/3520 Training loss: 1.6968 0.1456 sec/batch\n",
      "Epoch 4/20  Iteration 562/3520 Training loss: 1.6962 0.1339 sec/batch\n",
      "Epoch 4/20  Iteration 563/3520 Training loss: 1.6958 0.1358 sec/batch\n",
      "Epoch 4/20  Iteration 564/3520 Training loss: 1.6957 0.1323 sec/batch\n",
      "Epoch 4/20  Iteration 565/3520 Training loss: 1.6945 0.1360 sec/batch\n",
      "Epoch 4/20  Iteration 566/3520 Training loss: 1.6952 0.1364 sec/batch\n",
      "Epoch 4/20  Iteration 567/3520 Training loss: 1.6961 0.1337 sec/batch\n",
      "Epoch 4/20  Iteration 568/3520 Training loss: 1.6958 0.1368 sec/batch\n",
      "Epoch 4/20  Iteration 569/3520 Training loss: 1.6955 0.1358 sec/batch\n",
      "Epoch 4/20  Iteration 570/3520 Training loss: 1.6940 0.1364 sec/batch\n",
      "Epoch 4/20  Iteration 571/3520 Training loss: 1.6941 0.1371 sec/batch\n",
      "Epoch 4/20  Iteration 572/3520 Training loss: 1.6930 0.1360 sec/batch\n",
      "Epoch 4/20  Iteration 573/3520 Training loss: 1.6925 0.1370 sec/batch\n",
      "Epoch 4/20  Iteration 574/3520 Training loss: 1.6917 0.1323 sec/batch\n",
      "Epoch 4/20  Iteration 575/3520 Training loss: 1.6912 0.1335 sec/batch\n",
      "Epoch 4/20  Iteration 576/3520 Training loss: 1.6907 0.1364 sec/batch\n",
      "Epoch 4/20  Iteration 577/3520 Training loss: 1.6903 0.1388 sec/batch\n",
      "Epoch 4/20  Iteration 578/3520 Training loss: 1.6898 0.1358 sec/batch\n",
      "Epoch 4/20  Iteration 579/3520 Training loss: 1.6896 0.1360 sec/batch\n",
      "Epoch 4/20  Iteration 580/3520 Training loss: 1.6891 0.1383 sec/batch\n",
      "Epoch 4/20  Iteration 581/3520 Training loss: 1.6890 0.1391 sec/batch\n",
      "Epoch 4/20  Iteration 582/3520 Training loss: 1.6887 0.1386 sec/batch\n",
      "Epoch 4/20  Iteration 583/3520 Training loss: 1.6881 0.1366 sec/batch\n",
      "Epoch 4/20  Iteration 584/3520 Training loss: 1.6874 0.1364 sec/batch\n",
      "Epoch 4/20  Iteration 585/3520 Training loss: 1.6867 0.1376 sec/batch\n",
      "Epoch 4/20  Iteration 586/3520 Training loss: 1.6865 0.1323 sec/batch\n",
      "Epoch 4/20  Iteration 587/3520 Training loss: 1.6857 0.1364 sec/batch\n",
      "Epoch 4/20  Iteration 588/3520 Training loss: 1.6860 0.1446 sec/batch\n",
      "Epoch 4/20  Iteration 589/3520 Training loss: 1.6855 0.1359 sec/batch\n",
      "Epoch 4/20  Iteration 590/3520 Training loss: 1.6851 0.1364 sec/batch\n",
      "Epoch 4/20  Iteration 591/3520 Training loss: 1.6846 0.1383 sec/batch\n",
      "Epoch 4/20  Iteration 592/3520 Training loss: 1.6843 0.1366 sec/batch\n",
      "Epoch 4/20  Iteration 593/3520 Training loss: 1.6833 0.1378 sec/batch\n",
      "Epoch 4/20  Iteration 594/3520 Training loss: 1.6830 0.1383 sec/batch\n",
      "Epoch 4/20  Iteration 595/3520 Training loss: 1.6825 0.1363 sec/batch\n",
      "Epoch 4/20  Iteration 596/3520 Training loss: 1.6815 0.1367 sec/batch\n",
      "Epoch 4/20  Iteration 597/3520 Training loss: 1.6810 0.1370 sec/batch\n",
      "Epoch 4/20  Iteration 598/3520 Training loss: 1.6809 0.1366 sec/batch\n",
      "Epoch 4/20  Iteration 599/3520 Training loss: 1.6804 0.1360 sec/batch\n",
      "Epoch 4/20  Iteration 600/3520 Training loss: 1.6799 0.1345 sec/batch\n",
      "Validation loss: 1.56996 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 601/3520 Training loss: 1.6801 0.1480 sec/batch\n",
      "Epoch 4/20  Iteration 602/3520 Training loss: 1.6792 0.1571 sec/batch\n",
      "Epoch 4/20  Iteration 603/3520 Training loss: 1.6788 0.1475 sec/batch\n",
      "Epoch 4/20  Iteration 604/3520 Training loss: 1.6788 0.1348 sec/batch\n",
      "Epoch 4/20  Iteration 605/3520 Training loss: 1.6787 0.1369 sec/batch\n",
      "Epoch 4/20  Iteration 606/3520 Training loss: 1.6787 0.1381 sec/batch\n",
      "Epoch 4/20  Iteration 607/3520 Training loss: 1.6781 0.1366 sec/batch\n",
      "Epoch 4/20  Iteration 608/3520 Training loss: 1.6778 0.1385 sec/batch\n",
      "Epoch 4/20  Iteration 609/3520 Training loss: 1.6769 0.1361 sec/batch\n",
      "Epoch 4/20  Iteration 610/3520 Training loss: 1.6768 0.1327 sec/batch\n",
      "Epoch 4/20  Iteration 611/3520 Training loss: 1.6761 0.1371 sec/batch\n",
      "Epoch 4/20  Iteration 612/3520 Training loss: 1.6760 0.1307 sec/batch\n",
      "Epoch 4/20  Iteration 613/3520 Training loss: 1.6755 0.1362 sec/batch\n",
      "Epoch 4/20  Iteration 614/3520 Training loss: 1.6749 0.1372 sec/batch\n",
      "Epoch 4/20  Iteration 615/3520 Training loss: 1.6748 0.1377 sec/batch\n",
      "Epoch 4/20  Iteration 616/3520 Training loss: 1.6745 0.1298 sec/batch\n",
      "Epoch 4/20  Iteration 617/3520 Training loss: 1.6742 0.1368 sec/batch\n",
      "Epoch 4/20  Iteration 618/3520 Training loss: 1.6737 0.1362 sec/batch\n",
      "Epoch 4/20  Iteration 619/3520 Training loss: 1.6734 0.1370 sec/batch\n",
      "Epoch 4/20  Iteration 620/3520 Training loss: 1.6734 0.1376 sec/batch\n",
      "Epoch 4/20  Iteration 621/3520 Training loss: 1.6729 0.1325 sec/batch\n",
      "Epoch 4/20  Iteration 622/3520 Training loss: 1.6724 0.1374 sec/batch\n",
      "Epoch 4/20  Iteration 623/3520 Training loss: 1.6720 0.1320 sec/batch\n",
      "Epoch 4/20  Iteration 624/3520 Training loss: 1.6712 0.1376 sec/batch\n",
      "Epoch 4/20  Iteration 625/3520 Training loss: 1.6713 0.1366 sec/batch\n",
      "Epoch 4/20  Iteration 626/3520 Training loss: 1.6710 0.1360 sec/batch\n",
      "Epoch 4/20  Iteration 627/3520 Training loss: 1.6709 0.1362 sec/batch\n",
      "Epoch 4/20  Iteration 628/3520 Training loss: 1.6704 0.1375 sec/batch\n",
      "Epoch 4/20  Iteration 629/3520 Training loss: 1.6701 0.1386 sec/batch\n",
      "Epoch 4/20  Iteration 630/3520 Training loss: 1.6699 0.1347 sec/batch\n",
      "Epoch 4/20  Iteration 631/3520 Training loss: 1.6694 0.1336 sec/batch\n",
      "Epoch 4/20  Iteration 632/3520 Training loss: 1.6685 0.1368 sec/batch\n",
      "Epoch 4/20  Iteration 633/3520 Training loss: 1.6679 0.1371 sec/batch\n",
      "Epoch 4/20  Iteration 634/3520 Training loss: 1.6678 0.1364 sec/batch\n",
      "Epoch 4/20  Iteration 635/3520 Training loss: 1.6674 0.1364 sec/batch\n",
      "Epoch 4/20  Iteration 636/3520 Training loss: 1.6668 0.1371 sec/batch\n",
      "Epoch 4/20  Iteration 637/3520 Training loss: 1.6666 0.1351 sec/batch\n",
      "Epoch 4/20  Iteration 638/3520 Training loss: 1.6662 0.1386 sec/batch\n",
      "Epoch 4/20  Iteration 639/3520 Training loss: 1.6658 0.1378 sec/batch\n",
      "Epoch 4/20  Iteration 640/3520 Training loss: 1.6656 0.1372 sec/batch\n",
      "Epoch 4/20  Iteration 641/3520 Training loss: 1.6655 0.1369 sec/batch\n",
      "Epoch 4/20  Iteration 642/3520 Training loss: 1.6649 0.1367 sec/batch\n",
      "Epoch 4/20  Iteration 643/3520 Training loss: 1.6645 0.1327 sec/batch\n",
      "Epoch 4/20  Iteration 644/3520 Training loss: 1.6642 0.1366 sec/batch\n",
      "Epoch 4/20  Iteration 645/3520 Training loss: 1.6637 0.1389 sec/batch\n",
      "Epoch 4/20  Iteration 646/3520 Training loss: 1.6636 0.1367 sec/batch\n",
      "Epoch 4/20  Iteration 647/3520 Training loss: 1.6632 0.1354 sec/batch\n",
      "Epoch 4/20  Iteration 648/3520 Training loss: 1.6629 0.1381 sec/batch\n",
      "Epoch 4/20  Iteration 649/3520 Training loss: 1.6625 0.1371 sec/batch\n",
      "Epoch 4/20  Iteration 650/3520 Training loss: 1.6623 0.1300 sec/batch\n",
      "Epoch 4/20  Iteration 651/3520 Training loss: 1.6619 0.1390 sec/batch\n",
      "Epoch 4/20  Iteration 652/3520 Training loss: 1.6616 0.1323 sec/batch\n",
      "Epoch 4/20  Iteration 653/3520 Training loss: 1.6613 0.1313 sec/batch\n",
      "Epoch 4/20  Iteration 654/3520 Training loss: 1.6606 0.1379 sec/batch\n",
      "Epoch 4/20  Iteration 655/3520 Training loss: 1.6600 0.1378 sec/batch\n",
      "Epoch 4/20  Iteration 656/3520 Training loss: 1.6597 0.1378 sec/batch\n",
      "Epoch 4/20  Iteration 657/3520 Training loss: 1.6593 0.1348 sec/batch\n",
      "Epoch 4/20  Iteration 658/3520 Training loss: 1.6592 0.1385 sec/batch\n",
      "Epoch 4/20  Iteration 659/3520 Training loss: 1.6588 0.1357 sec/batch\n",
      "Epoch 4/20  Iteration 660/3520 Training loss: 1.6587 0.1386 sec/batch\n",
      "Epoch 4/20  Iteration 661/3520 Training loss: 1.6584 0.1344 sec/batch\n",
      "Epoch 4/20  Iteration 662/3520 Training loss: 1.6582 0.1363 sec/batch\n",
      "Epoch 4/20  Iteration 663/3520 Training loss: 1.6579 0.1365 sec/batch\n",
      "Epoch 4/20  Iteration 664/3520 Training loss: 1.6576 0.1341 sec/batch\n",
      "Epoch 4/20  Iteration 665/3520 Training loss: 1.6573 0.1369 sec/batch\n",
      "Epoch 4/20  Iteration 666/3520 Training loss: 1.6569 0.1364 sec/batch\n",
      "Epoch 4/20  Iteration 667/3520 Training loss: 1.6568 0.1325 sec/batch\n",
      "Epoch 4/20  Iteration 668/3520 Training loss: 1.6566 0.1381 sec/batch\n",
      "Epoch 4/20  Iteration 669/3520 Training loss: 1.6563 0.1379 sec/batch\n",
      "Epoch 4/20  Iteration 670/3520 Training loss: 1.6559 0.1345 sec/batch\n",
      "Epoch 4/20  Iteration 671/3520 Training loss: 1.6553 0.1362 sec/batch\n",
      "Epoch 4/20  Iteration 672/3520 Training loss: 1.6550 0.1355 sec/batch\n",
      "Epoch 4/20  Iteration 673/3520 Training loss: 1.6548 0.1362 sec/batch\n",
      "Epoch 4/20  Iteration 674/3520 Training loss: 1.6545 0.1343 sec/batch\n",
      "Epoch 4/20  Iteration 675/3520 Training loss: 1.6541 0.1358 sec/batch\n",
      "Epoch 4/20  Iteration 676/3520 Training loss: 1.6538 0.1359 sec/batch\n",
      "Epoch 4/20  Iteration 677/3520 Training loss: 1.6535 0.1356 sec/batch\n",
      "Epoch 4/20  Iteration 678/3520 Training loss: 1.6533 0.1354 sec/batch\n",
      "Epoch 4/20  Iteration 679/3520 Training loss: 1.6529 0.1322 sec/batch\n",
      "Epoch 4/20  Iteration 680/3520 Training loss: 1.6524 0.1374 sec/batch\n",
      "Epoch 4/20  Iteration 681/3520 Training loss: 1.6521 0.1379 sec/batch\n",
      "Epoch 4/20  Iteration 682/3520 Training loss: 1.6516 0.1376 sec/batch\n",
      "Epoch 4/20  Iteration 683/3520 Training loss: 1.6513 0.1373 sec/batch\n",
      "Epoch 4/20  Iteration 684/3520 Training loss: 1.6509 0.1384 sec/batch\n",
      "Epoch 4/20  Iteration 685/3520 Training loss: 1.6505 0.1369 sec/batch\n",
      "Epoch 4/20  Iteration 686/3520 Training loss: 1.6502 0.1338 sec/batch\n",
      "Epoch 4/20  Iteration 687/3520 Training loss: 1.6499 0.1317 sec/batch\n",
      "Epoch 4/20  Iteration 688/3520 Training loss: 1.6498 0.1375 sec/batch\n",
      "Epoch 4/20  Iteration 689/3520 Training loss: 1.6492 0.1351 sec/batch\n",
      "Epoch 4/20  Iteration 690/3520 Training loss: 1.6490 0.1364 sec/batch\n",
      "Epoch 4/20  Iteration 691/3520 Training loss: 1.6488 0.1361 sec/batch\n",
      "Epoch 4/20  Iteration 692/3520 Training loss: 1.6483 0.1366 sec/batch\n",
      "Epoch 4/20  Iteration 693/3520 Training loss: 1.6478 0.1362 sec/batch\n",
      "Epoch 4/20  Iteration 694/3520 Training loss: 1.6473 0.1367 sec/batch\n",
      "Epoch 4/20  Iteration 695/3520 Training loss: 1.6468 0.1376 sec/batch\n",
      "Epoch 4/20  Iteration 696/3520 Training loss: 1.6466 0.1367 sec/batch\n",
      "Epoch 4/20  Iteration 697/3520 Training loss: 1.6464 0.1375 sec/batch\n",
      "Epoch 4/20  Iteration 698/3520 Training loss: 1.6461 0.1347 sec/batch\n",
      "Epoch 4/20  Iteration 699/3520 Training loss: 1.6459 0.1374 sec/batch\n",
      "Epoch 4/20  Iteration 700/3520 Training loss: 1.6456 0.1376 sec/batch\n",
      "Epoch 4/20  Iteration 701/3520 Training loss: 1.6453 0.1358 sec/batch\n",
      "Epoch 4/20  Iteration 702/3520 Training loss: 1.6450 0.1374 sec/batch\n",
      "Epoch 4/20  Iteration 703/3520 Training loss: 1.6446 0.1352 sec/batch\n",
      "Epoch 4/20  Iteration 704/3520 Training loss: 1.6446 0.1372 sec/batch\n",
      "Epoch 5/20  Iteration 705/3520 Training loss: 1.6196 0.1320 sec/batch\n",
      "Epoch 5/20  Iteration 706/3520 Training loss: 1.5785 0.1321 sec/batch\n",
      "Epoch 5/20  Iteration 707/3520 Training loss: 1.5809 0.1326 sec/batch\n",
      "Epoch 5/20  Iteration 708/3520 Training loss: 1.5815 0.1357 sec/batch\n",
      "Epoch 5/20  Iteration 709/3520 Training loss: 1.5842 0.1339 sec/batch\n",
      "Epoch 5/20  Iteration 710/3520 Training loss: 1.5838 0.1358 sec/batch\n",
      "Epoch 5/20  Iteration 711/3520 Training loss: 1.5826 0.1388 sec/batch\n",
      "Epoch 5/20  Iteration 712/3520 Training loss: 1.5847 0.1361 sec/batch\n",
      "Epoch 5/20  Iteration 713/3520 Training loss: 1.5827 0.1361 sec/batch\n",
      "Epoch 5/20  Iteration 714/3520 Training loss: 1.5817 0.1380 sec/batch\n",
      "Epoch 5/20  Iteration 715/3520 Training loss: 1.5826 0.1370 sec/batch\n",
      "Epoch 5/20  Iteration 716/3520 Training loss: 1.5833 0.1365 sec/batch\n",
      "Epoch 5/20  Iteration 717/3520 Training loss: 1.5810 0.1357 sec/batch\n",
      "Epoch 5/20  Iteration 718/3520 Training loss: 1.5824 0.1388 sec/batch\n",
      "Epoch 5/20  Iteration 719/3520 Training loss: 1.5820 0.1355 sec/batch\n",
      "Epoch 5/20  Iteration 720/3520 Training loss: 1.5821 0.1474 sec/batch\n",
      "Epoch 5/20  Iteration 721/3520 Training loss: 1.5817 0.1383 sec/batch\n",
      "Epoch 5/20  Iteration 722/3520 Training loss: 1.5822 0.1372 sec/batch\n",
      "Epoch 5/20  Iteration 723/3520 Training loss: 1.5811 0.1367 sec/batch\n",
      "Epoch 5/20  Iteration 724/3520 Training loss: 1.5821 0.1362 sec/batch\n",
      "Epoch 5/20  Iteration 725/3520 Training loss: 1.5825 0.1359 sec/batch\n",
      "Epoch 5/20  Iteration 726/3520 Training loss: 1.5821 0.1374 sec/batch\n",
      "Epoch 5/20  Iteration 727/3520 Training loss: 1.5828 0.1379 sec/batch\n",
      "Epoch 5/20  Iteration 728/3520 Training loss: 1.5828 0.1373 sec/batch\n",
      "Epoch 5/20  Iteration 729/3520 Training loss: 1.5830 0.1378 sec/batch\n",
      "Epoch 5/20  Iteration 730/3520 Training loss: 1.5819 0.1362 sec/batch\n",
      "Epoch 5/20  Iteration 731/3520 Training loss: 1.5809 0.1370 sec/batch\n",
      "Epoch 5/20  Iteration 732/3520 Training loss: 1.5805 0.1314 sec/batch\n",
      "Epoch 5/20  Iteration 733/3520 Training loss: 1.5799 0.1380 sec/batch\n",
      "Epoch 5/20  Iteration 734/3520 Training loss: 1.5799 0.1364 sec/batch\n",
      "Epoch 5/20  Iteration 735/3520 Training loss: 1.5795 0.1358 sec/batch\n",
      "Epoch 5/20  Iteration 736/3520 Training loss: 1.5794 0.1362 sec/batch\n",
      "Epoch 5/20  Iteration 737/3520 Training loss: 1.5800 0.1351 sec/batch\n",
      "Epoch 5/20  Iteration 738/3520 Training loss: 1.5795 0.1367 sec/batch\n",
      "Epoch 5/20  Iteration 739/3520 Training loss: 1.5795 0.1377 sec/batch\n",
      "Epoch 5/20  Iteration 740/3520 Training loss: 1.5798 0.1322 sec/batch\n",
      "Epoch 5/20  Iteration 741/3520 Training loss: 1.5790 0.1377 sec/batch\n",
      "Epoch 5/20  Iteration 742/3520 Training loss: 1.5798 0.1292 sec/batch\n",
      "Epoch 5/20  Iteration 743/3520 Training loss: 1.5808 0.1368 sec/batch\n",
      "Epoch 5/20  Iteration 744/3520 Training loss: 1.5807 0.1366 sec/batch\n",
      "Epoch 5/20  Iteration 745/3520 Training loss: 1.5802 0.1358 sec/batch\n",
      "Epoch 5/20  Iteration 746/3520 Training loss: 1.5793 0.1364 sec/batch\n",
      "Epoch 5/20  Iteration 747/3520 Training loss: 1.5797 0.1376 sec/batch\n",
      "Epoch 5/20  Iteration 748/3520 Training loss: 1.5787 0.1380 sec/batch\n",
      "Epoch 5/20  Iteration 749/3520 Training loss: 1.5783 0.1287 sec/batch\n",
      "Epoch 5/20  Iteration 750/3520 Training loss: 1.5777 0.1373 sec/batch\n",
      "Epoch 5/20  Iteration 751/3520 Training loss: 1.5774 0.1379 sec/batch\n",
      "Epoch 5/20  Iteration 752/3520 Training loss: 1.5772 0.1379 sec/batch\n",
      "Epoch 5/20  Iteration 753/3520 Training loss: 1.5770 0.1383 sec/batch\n",
      "Epoch 5/20  Iteration 754/3520 Training loss: 1.5767 0.1358 sec/batch\n",
      "Epoch 5/20  Iteration 755/3520 Training loss: 1.5766 0.1305 sec/batch\n",
      "Epoch 5/20  Iteration 756/3520 Training loss: 1.5764 0.1370 sec/batch\n",
      "Epoch 5/20  Iteration 757/3520 Training loss: 1.5764 0.1384 sec/batch\n",
      "Epoch 5/20  Iteration 758/3520 Training loss: 1.5763 0.1360 sec/batch\n",
      "Epoch 5/20  Iteration 759/3520 Training loss: 1.5758 0.1378 sec/batch\n",
      "Epoch 5/20  Iteration 760/3520 Training loss: 1.5753 0.1364 sec/batch\n",
      "Epoch 5/20  Iteration 761/3520 Training loss: 1.5748 0.1366 sec/batch\n",
      "Epoch 5/20  Iteration 762/3520 Training loss: 1.5746 0.1374 sec/batch\n",
      "Epoch 5/20  Iteration 763/3520 Training loss: 1.5740 0.1362 sec/batch\n",
      "Epoch 5/20  Iteration 764/3520 Training loss: 1.5745 0.1337 sec/batch\n",
      "Epoch 5/20  Iteration 765/3520 Training loss: 1.5740 0.1367 sec/batch\n",
      "Epoch 5/20  Iteration 766/3520 Training loss: 1.5738 0.1360 sec/batch\n",
      "Epoch 5/20  Iteration 767/3520 Training loss: 1.5737 0.1413 sec/batch\n",
      "Epoch 5/20  Iteration 768/3520 Training loss: 1.5735 0.1383 sec/batch\n",
      "Epoch 5/20  Iteration 769/3520 Training loss: 1.5727 0.1352 sec/batch\n",
      "Epoch 5/20  Iteration 770/3520 Training loss: 1.5726 0.1387 sec/batch\n",
      "Epoch 5/20  Iteration 771/3520 Training loss: 1.5722 0.1384 sec/batch\n",
      "Epoch 5/20  Iteration 772/3520 Training loss: 1.5714 0.1360 sec/batch\n",
      "Epoch 5/20  Iteration 773/3520 Training loss: 1.5711 0.1375 sec/batch\n",
      "Epoch 5/20  Iteration 774/3520 Training loss: 1.5711 0.1366 sec/batch\n",
      "Epoch 5/20  Iteration 775/3520 Training loss: 1.5708 0.1364 sec/batch\n",
      "Epoch 5/20  Iteration 776/3520 Training loss: 1.5703 0.1352 sec/batch\n",
      "Epoch 5/20  Iteration 777/3520 Training loss: 1.5698 0.1361 sec/batch\n",
      "Epoch 5/20  Iteration 778/3520 Training loss: 1.5690 0.1384 sec/batch\n",
      "Epoch 5/20  Iteration 779/3520 Training loss: 1.5688 0.1389 sec/batch\n",
      "Epoch 5/20  Iteration 780/3520 Training loss: 1.5688 0.1362 sec/batch\n",
      "Epoch 5/20  Iteration 781/3520 Training loss: 1.5688 0.1364 sec/batch\n",
      "Epoch 5/20  Iteration 782/3520 Training loss: 1.5691 0.1361 sec/batch\n",
      "Epoch 5/20  Iteration 783/3520 Training loss: 1.5687 0.1362 sec/batch\n",
      "Epoch 5/20  Iteration 784/3520 Training loss: 1.5686 0.1360 sec/batch\n",
      "Epoch 5/20  Iteration 785/3520 Training loss: 1.5679 0.1360 sec/batch\n",
      "Epoch 5/20  Iteration 786/3520 Training loss: 1.5679 0.1377 sec/batch\n",
      "Epoch 5/20  Iteration 787/3520 Training loss: 1.5674 0.1369 sec/batch\n",
      "Epoch 5/20  Iteration 788/3520 Training loss: 1.5674 0.1376 sec/batch\n",
      "Epoch 5/20  Iteration 789/3520 Training loss: 1.5671 0.1379 sec/batch\n",
      "Epoch 5/20  Iteration 790/3520 Training loss: 1.5664 0.1382 sec/batch\n",
      "Epoch 5/20  Iteration 791/3520 Training loss: 1.5666 0.1361 sec/batch\n",
      "Epoch 5/20  Iteration 792/3520 Training loss: 1.5662 0.1386 sec/batch\n",
      "Epoch 5/20  Iteration 793/3520 Training loss: 1.5662 0.1378 sec/batch\n",
      "Epoch 5/20  Iteration 794/3520 Training loss: 1.5659 0.1363 sec/batch\n",
      "Epoch 5/20  Iteration 795/3520 Training loss: 1.5655 0.1350 sec/batch\n",
      "Epoch 5/20  Iteration 796/3520 Training loss: 1.5655 0.1376 sec/batch\n",
      "Epoch 5/20  Iteration 797/3520 Training loss: 1.5652 0.1382 sec/batch\n",
      "Epoch 5/20  Iteration 798/3520 Training loss: 1.5647 0.1360 sec/batch\n",
      "Epoch 5/20  Iteration 799/3520 Training loss: 1.5644 0.1394 sec/batch\n",
      "Epoch 5/20  Iteration 800/3520 Training loss: 1.5637 0.1345 sec/batch\n",
      "Validation loss: 1.45927 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 801/3520 Training loss: 1.5646 0.1377 sec/batch\n",
      "Epoch 5/20  Iteration 802/3520 Training loss: 1.5644 0.1313 sec/batch\n",
      "Epoch 5/20  Iteration 803/3520 Training loss: 1.5644 0.1342 sec/batch\n",
      "Epoch 5/20  Iteration 804/3520 Training loss: 1.5640 0.1369 sec/batch\n",
      "Epoch 5/20  Iteration 805/3520 Training loss: 1.5638 0.1352 sec/batch\n",
      "Epoch 5/20  Iteration 806/3520 Training loss: 1.5638 0.1340 sec/batch\n",
      "Epoch 5/20  Iteration 807/3520 Training loss: 1.5633 0.1366 sec/batch\n",
      "Epoch 5/20  Iteration 808/3520 Training loss: 1.5626 0.1361 sec/batch\n",
      "Epoch 5/20  Iteration 809/3520 Training loss: 1.5621 0.1380 sec/batch\n",
      "Epoch 5/20  Iteration 810/3520 Training loss: 1.5621 0.1353 sec/batch\n",
      "Epoch 5/20  Iteration 811/3520 Training loss: 1.5619 0.1351 sec/batch\n",
      "Epoch 5/20  Iteration 812/3520 Training loss: 1.5614 0.1370 sec/batch\n",
      "Epoch 5/20  Iteration 813/3520 Training loss: 1.5614 0.1336 sec/batch\n",
      "Epoch 5/20  Iteration 814/3520 Training loss: 1.5611 0.1338 sec/batch\n",
      "Epoch 5/20  Iteration 815/3520 Training loss: 1.5609 0.1350 sec/batch\n",
      "Epoch 5/20  Iteration 816/3520 Training loss: 1.5609 0.1324 sec/batch\n",
      "Epoch 5/20  Iteration 817/3520 Training loss: 1.5609 0.1368 sec/batch\n",
      "Epoch 5/20  Iteration 818/3520 Training loss: 1.5607 0.1364 sec/batch\n",
      "Epoch 5/20  Iteration 819/3520 Training loss: 1.5603 0.1363 sec/batch\n",
      "Epoch 5/20  Iteration 820/3520 Training loss: 1.5600 0.1385 sec/batch\n",
      "Epoch 5/20  Iteration 821/3520 Training loss: 1.5597 0.1368 sec/batch\n",
      "Epoch 5/20  Iteration 822/3520 Training loss: 1.5597 0.1377 sec/batch\n",
      "Epoch 5/20  Iteration 823/3520 Training loss: 1.5595 0.1358 sec/batch\n",
      "Epoch 5/20  Iteration 824/3520 Training loss: 1.5593 0.1380 sec/batch\n",
      "Epoch 5/20  Iteration 825/3520 Training loss: 1.5591 0.1380 sec/batch\n",
      "Epoch 5/20  Iteration 826/3520 Training loss: 1.5590 0.1372 sec/batch\n",
      "Epoch 5/20  Iteration 827/3520 Training loss: 1.5586 0.1372 sec/batch\n",
      "Epoch 5/20  Iteration 828/3520 Training loss: 1.5584 0.1379 sec/batch\n",
      "Epoch 5/20  Iteration 829/3520 Training loss: 1.5582 0.1359 sec/batch\n",
      "Epoch 5/20  Iteration 830/3520 Training loss: 1.5576 0.1362 sec/batch\n",
      "Epoch 5/20  Iteration 831/3520 Training loss: 1.5572 0.1361 sec/batch\n",
      "Epoch 5/20  Iteration 832/3520 Training loss: 1.5569 0.1374 sec/batch\n",
      "Epoch 5/20  Iteration 833/3520 Training loss: 1.5567 0.1339 sec/batch\n",
      "Epoch 5/20  Iteration 834/3520 Training loss: 1.5568 0.1330 sec/batch\n",
      "Epoch 5/20  Iteration 835/3520 Training loss: 1.5565 0.1370 sec/batch\n",
      "Epoch 5/20  Iteration 836/3520 Training loss: 1.5566 0.1368 sec/batch\n",
      "Epoch 5/20  Iteration 837/3520 Training loss: 1.5565 0.1372 sec/batch\n",
      "Epoch 5/20  Iteration 838/3520 Training loss: 1.5564 0.1364 sec/batch\n",
      "Epoch 5/20  Iteration 839/3520 Training loss: 1.5563 0.1381 sec/batch\n",
      "Epoch 5/20  Iteration 840/3520 Training loss: 1.5560 0.1347 sec/batch\n",
      "Epoch 5/20  Iteration 841/3520 Training loss: 1.5559 0.1365 sec/batch\n",
      "Epoch 5/20  Iteration 842/3520 Training loss: 1.5556 0.1363 sec/batch\n",
      "Epoch 5/20  Iteration 843/3520 Training loss: 1.5556 0.1342 sec/batch\n",
      "Epoch 5/20  Iteration 844/3520 Training loss: 1.5555 0.1380 sec/batch\n",
      "Epoch 5/20  Iteration 845/3520 Training loss: 1.5554 0.1305 sec/batch\n",
      "Epoch 5/20  Iteration 846/3520 Training loss: 1.5551 0.1322 sec/batch\n",
      "Epoch 5/20  Iteration 847/3520 Training loss: 1.5546 0.1382 sec/batch\n",
      "Epoch 5/20  Iteration 848/3520 Training loss: 1.5544 0.1311 sec/batch\n",
      "Epoch 5/20  Iteration 849/3520 Training loss: 1.5543 0.1360 sec/batch\n",
      "Epoch 5/20  Iteration 850/3520 Training loss: 1.5541 0.1339 sec/batch\n",
      "Epoch 5/20  Iteration 851/3520 Training loss: 1.5538 0.1359 sec/batch\n",
      "Epoch 5/20  Iteration 852/3520 Training loss: 1.5535 0.1362 sec/batch\n",
      "Epoch 5/20  Iteration 853/3520 Training loss: 1.5532 0.1340 sec/batch\n",
      "Epoch 5/20  Iteration 854/3520 Training loss: 1.5531 0.1383 sec/batch\n",
      "Epoch 5/20  Iteration 855/3520 Training loss: 1.5528 0.1379 sec/batch\n",
      "Epoch 5/20  Iteration 856/3520 Training loss: 1.5525 0.1362 sec/batch\n",
      "Epoch 5/20  Iteration 857/3520 Training loss: 1.5522 0.1376 sec/batch\n",
      "Epoch 5/20  Iteration 858/3520 Training loss: 1.5518 0.1372 sec/batch\n",
      "Epoch 5/20  Iteration 859/3520 Training loss: 1.5515 0.1378 sec/batch\n",
      "Epoch 5/20  Iteration 860/3520 Training loss: 1.5513 0.1362 sec/batch\n",
      "Epoch 5/20  Iteration 861/3520 Training loss: 1.5509 0.1373 sec/batch\n",
      "Epoch 5/20  Iteration 862/3520 Training loss: 1.5507 0.1321 sec/batch\n",
      "Epoch 5/20  Iteration 863/3520 Training loss: 1.5506 0.1366 sec/batch\n",
      "Epoch 5/20  Iteration 864/3520 Training loss: 1.5505 0.1387 sec/batch\n",
      "Epoch 5/20  Iteration 865/3520 Training loss: 1.5501 0.1382 sec/batch\n",
      "Epoch 5/20  Iteration 866/3520 Training loss: 1.5500 0.1358 sec/batch\n",
      "Epoch 5/20  Iteration 867/3520 Training loss: 1.5498 0.1340 sec/batch\n",
      "Epoch 5/20  Iteration 868/3520 Training loss: 1.5495 0.1375 sec/batch\n",
      "Epoch 5/20  Iteration 869/3520 Training loss: 1.5489 0.1359 sec/batch\n",
      "Epoch 5/20  Iteration 870/3520 Training loss: 1.5486 0.1376 sec/batch\n",
      "Epoch 5/20  Iteration 871/3520 Training loss: 1.5482 0.1376 sec/batch\n",
      "Epoch 5/20  Iteration 872/3520 Training loss: 1.5480 0.1364 sec/batch\n",
      "Epoch 5/20  Iteration 873/3520 Training loss: 1.5479 0.1361 sec/batch\n",
      "Epoch 5/20  Iteration 874/3520 Training loss: 1.5477 0.1372 sec/batch\n",
      "Epoch 5/20  Iteration 875/3520 Training loss: 1.5475 0.1342 sec/batch\n",
      "Epoch 5/20  Iteration 876/3520 Training loss: 1.5473 0.1355 sec/batch\n",
      "Epoch 5/20  Iteration 877/3520 Training loss: 1.5471 0.1374 sec/batch\n",
      "Epoch 5/20  Iteration 878/3520 Training loss: 1.5469 0.1332 sec/batch\n",
      "Epoch 5/20  Iteration 879/3520 Training loss: 1.5466 0.1372 sec/batch\n",
      "Epoch 5/20  Iteration 880/3520 Training loss: 1.5466 0.1365 sec/batch\n",
      "Epoch 6/20  Iteration 881/3520 Training loss: 1.5316 0.1376 sec/batch\n",
      "Epoch 6/20  Iteration 882/3520 Training loss: 1.4928 0.1389 sec/batch\n",
      "Epoch 6/20  Iteration 883/3520 Training loss: 1.4946 0.1361 sec/batch\n",
      "Epoch 6/20  Iteration 884/3520 Training loss: 1.4959 0.1359 sec/batch\n",
      "Epoch 6/20  Iteration 885/3520 Training loss: 1.4980 0.1367 sec/batch\n",
      "Epoch 6/20  Iteration 886/3520 Training loss: 1.4988 0.1297 sec/batch\n",
      "Epoch 6/20  Iteration 887/3520 Training loss: 1.4980 0.1371 sec/batch\n",
      "Epoch 6/20  Iteration 888/3520 Training loss: 1.4997 0.1356 sec/batch\n",
      "Epoch 6/20  Iteration 889/3520 Training loss: 1.4981 0.1364 sec/batch\n",
      "Epoch 6/20  Iteration 890/3520 Training loss: 1.4984 0.1371 sec/batch\n",
      "Epoch 6/20  Iteration 891/3520 Training loss: 1.4995 0.1330 sec/batch\n",
      "Epoch 6/20  Iteration 892/3520 Training loss: 1.5008 0.1346 sec/batch\n",
      "Epoch 6/20  Iteration 893/3520 Training loss: 1.5000 0.1368 sec/batch\n",
      "Epoch 6/20  Iteration 894/3520 Training loss: 1.5007 0.1346 sec/batch\n",
      "Epoch 6/20  Iteration 895/3520 Training loss: 1.5005 0.1330 sec/batch\n",
      "Epoch 6/20  Iteration 896/3520 Training loss: 1.5001 0.1359 sec/batch\n",
      "Epoch 6/20  Iteration 897/3520 Training loss: 1.4999 0.1375 sec/batch\n",
      "Epoch 6/20  Iteration 898/3520 Training loss: 1.5004 0.1382 sec/batch\n",
      "Epoch 6/20  Iteration 899/3520 Training loss: 1.4991 0.1361 sec/batch\n",
      "Epoch 6/20  Iteration 900/3520 Training loss: 1.5003 0.1369 sec/batch\n",
      "Epoch 6/20  Iteration 901/3520 Training loss: 1.5007 0.1353 sec/batch\n",
      "Epoch 6/20  Iteration 902/3520 Training loss: 1.4998 0.1353 sec/batch\n",
      "Epoch 6/20  Iteration 903/3520 Training loss: 1.5001 0.1371 sec/batch\n",
      "Epoch 6/20  Iteration 904/3520 Training loss: 1.5006 0.1387 sec/batch\n",
      "Epoch 6/20  Iteration 905/3520 Training loss: 1.5008 0.1359 sec/batch\n",
      "Epoch 6/20  Iteration 906/3520 Training loss: 1.5000 0.1364 sec/batch\n",
      "Epoch 6/20  Iteration 907/3520 Training loss: 1.4987 0.1367 sec/batch\n",
      "Epoch 6/20  Iteration 908/3520 Training loss: 1.4984 0.1367 sec/batch\n",
      "Epoch 6/20  Iteration 909/3520 Training loss: 1.4981 0.1443 sec/batch\n",
      "Epoch 6/20  Iteration 910/3520 Training loss: 1.4981 0.1375 sec/batch\n",
      "Epoch 6/20  Iteration 911/3520 Training loss: 1.4976 0.1389 sec/batch\n",
      "Epoch 6/20  Iteration 912/3520 Training loss: 1.4972 0.1347 sec/batch\n",
      "Epoch 6/20  Iteration 913/3520 Training loss: 1.4979 0.1336 sec/batch\n",
      "Epoch 6/20  Iteration 914/3520 Training loss: 1.4973 0.1358 sec/batch\n",
      "Epoch 6/20  Iteration 915/3520 Training loss: 1.4971 0.1351 sec/batch\n",
      "Epoch 6/20  Iteration 916/3520 Training loss: 1.4972 0.1361 sec/batch\n",
      "Epoch 6/20  Iteration 917/3520 Training loss: 1.4962 0.1355 sec/batch\n",
      "Epoch 6/20  Iteration 918/3520 Training loss: 1.4968 0.1364 sec/batch\n",
      "Epoch 6/20  Iteration 919/3520 Training loss: 1.4976 0.1372 sec/batch\n",
      "Epoch 6/20  Iteration 920/3520 Training loss: 1.4975 0.1373 sec/batch\n",
      "Epoch 6/20  Iteration 921/3520 Training loss: 1.4969 0.1373 sec/batch\n",
      "Epoch 6/20  Iteration 922/3520 Training loss: 1.4960 0.1390 sec/batch\n",
      "Epoch 6/20  Iteration 923/3520 Training loss: 1.4965 0.1378 sec/batch\n",
      "Epoch 6/20  Iteration 924/3520 Training loss: 1.4958 0.1373 sec/batch\n",
      "Epoch 6/20  Iteration 925/3520 Training loss: 1.4954 0.1367 sec/batch\n",
      "Epoch 6/20  Iteration 926/3520 Training loss: 1.4950 0.1456 sec/batch\n",
      "Epoch 6/20  Iteration 927/3520 Training loss: 1.4947 0.1368 sec/batch\n",
      "Epoch 6/20  Iteration 928/3520 Training loss: 1.4945 0.1331 sec/batch\n",
      "Epoch 6/20  Iteration 929/3520 Training loss: 1.4943 0.1380 sec/batch\n",
      "Epoch 6/20  Iteration 930/3520 Training loss: 1.4942 0.1323 sec/batch\n",
      "Epoch 6/20  Iteration 931/3520 Training loss: 1.4940 0.1365 sec/batch\n",
      "Epoch 6/20  Iteration 932/3520 Training loss: 1.4938 0.1367 sec/batch\n",
      "Epoch 6/20  Iteration 933/3520 Training loss: 1.4939 0.1363 sec/batch\n",
      "Epoch 6/20  Iteration 934/3520 Training loss: 1.4937 0.1326 sec/batch\n",
      "Epoch 6/20  Iteration 935/3520 Training loss: 1.4934 0.1427 sec/batch\n",
      "Epoch 6/20  Iteration 936/3520 Training loss: 1.4931 0.1391 sec/batch\n",
      "Epoch 6/20  Iteration 937/3520 Training loss: 1.4927 0.1353 sec/batch\n",
      "Epoch 6/20  Iteration 938/3520 Training loss: 1.4927 0.1366 sec/batch\n",
      "Epoch 6/20  Iteration 939/3520 Training loss: 1.4920 0.1326 sec/batch\n",
      "Epoch 6/20  Iteration 940/3520 Training loss: 1.4924 0.1321 sec/batch\n",
      "Epoch 6/20  Iteration 941/3520 Training loss: 1.4922 0.1365 sec/batch\n",
      "Epoch 6/20  Iteration 942/3520 Training loss: 1.4921 0.1347 sec/batch\n",
      "Epoch 6/20  Iteration 943/3520 Training loss: 1.4920 0.1360 sec/batch\n",
      "Epoch 6/20  Iteration 944/3520 Training loss: 1.4919 0.1292 sec/batch\n",
      "Epoch 6/20  Iteration 945/3520 Training loss: 1.4912 0.1376 sec/batch\n",
      "Epoch 6/20  Iteration 946/3520 Training loss: 1.4911 0.1373 sec/batch\n",
      "Epoch 6/20  Iteration 947/3520 Training loss: 1.4908 0.1363 sec/batch\n",
      "Epoch 6/20  Iteration 948/3520 Training loss: 1.4900 0.1376 sec/batch\n",
      "Epoch 6/20  Iteration 949/3520 Training loss: 1.4897 0.1321 sec/batch\n",
      "Epoch 6/20  Iteration 950/3520 Training loss: 1.4895 0.1345 sec/batch\n",
      "Epoch 6/20  Iteration 951/3520 Training loss: 1.4893 0.1369 sec/batch\n",
      "Epoch 6/20  Iteration 952/3520 Training loss: 1.4889 0.1370 sec/batch\n",
      "Epoch 6/20  Iteration 953/3520 Training loss: 1.4884 0.1387 sec/batch\n",
      "Epoch 6/20  Iteration 954/3520 Training loss: 1.4877 0.1367 sec/batch\n",
      "Epoch 6/20  Iteration 955/3520 Training loss: 1.4876 0.1361 sec/batch\n",
      "Epoch 6/20  Iteration 956/3520 Training loss: 1.4877 0.1382 sec/batch\n",
      "Epoch 6/20  Iteration 957/3520 Training loss: 1.4877 0.1353 sec/batch\n",
      "Epoch 6/20  Iteration 958/3520 Training loss: 1.4880 0.1386 sec/batch\n",
      "Epoch 6/20  Iteration 959/3520 Training loss: 1.4876 0.1370 sec/batch\n",
      "Epoch 6/20  Iteration 960/3520 Training loss: 1.4875 0.1361 sec/batch\n",
      "Epoch 6/20  Iteration 961/3520 Training loss: 1.4869 0.1358 sec/batch\n",
      "Epoch 6/20  Iteration 962/3520 Training loss: 1.4872 0.1311 sec/batch\n",
      "Epoch 6/20  Iteration 963/3520 Training loss: 1.4868 0.1381 sec/batch\n",
      "Epoch 6/20  Iteration 964/3520 Training loss: 1.4869 0.1362 sec/batch\n",
      "Epoch 6/20  Iteration 965/3520 Training loss: 1.4867 0.1365 sec/batch\n",
      "Epoch 6/20  Iteration 966/3520 Training loss: 1.4861 0.1335 sec/batch\n",
      "Epoch 6/20  Iteration 967/3520 Training loss: 1.4865 0.1390 sec/batch\n",
      "Epoch 6/20  Iteration 968/3520 Training loss: 1.4863 0.1339 sec/batch\n",
      "Epoch 6/20  Iteration 969/3520 Training loss: 1.4863 0.1443 sec/batch\n",
      "Epoch 6/20  Iteration 970/3520 Training loss: 1.4860 0.1355 sec/batch\n",
      "Epoch 6/20  Iteration 971/3520 Training loss: 1.4858 0.1373 sec/batch\n",
      "Epoch 6/20  Iteration 972/3520 Training loss: 1.4858 0.1361 sec/batch\n",
      "Epoch 6/20  Iteration 973/3520 Training loss: 1.4856 0.1368 sec/batch\n",
      "Epoch 6/20  Iteration 974/3520 Training loss: 1.4853 0.1362 sec/batch\n",
      "Epoch 6/20  Iteration 975/3520 Training loss: 1.4851 0.1365 sec/batch\n",
      "Epoch 6/20  Iteration 976/3520 Training loss: 1.4845 0.1298 sec/batch\n",
      "Epoch 6/20  Iteration 977/3520 Training loss: 1.4847 0.1379 sec/batch\n",
      "Epoch 6/20  Iteration 978/3520 Training loss: 1.4846 0.1368 sec/batch\n",
      "Epoch 6/20  Iteration 979/3520 Training loss: 1.4847 0.1333 sec/batch\n",
      "Epoch 6/20  Iteration 980/3520 Training loss: 1.4844 0.1347 sec/batch\n",
      "Epoch 6/20  Iteration 981/3520 Training loss: 1.4842 0.1362 sec/batch\n",
      "Epoch 6/20  Iteration 982/3520 Training loss: 1.4842 0.1360 sec/batch\n",
      "Epoch 6/20  Iteration 983/3520 Training loss: 1.4838 0.1382 sec/batch\n",
      "Epoch 6/20  Iteration 984/3520 Training loss: 1.4831 0.1378 sec/batch\n",
      "Epoch 6/20  Iteration 985/3520 Training loss: 1.4825 0.1344 sec/batch\n",
      "Epoch 6/20  Iteration 986/3520 Training loss: 1.4826 0.1370 sec/batch\n",
      "Epoch 6/20  Iteration 987/3520 Training loss: 1.4825 0.1361 sec/batch\n",
      "Epoch 6/20  Iteration 988/3520 Training loss: 1.4820 0.1359 sec/batch\n",
      "Epoch 6/20  Iteration 989/3520 Training loss: 1.4819 0.1376 sec/batch\n",
      "Epoch 6/20  Iteration 990/3520 Training loss: 1.4816 0.1379 sec/batch\n",
      "Epoch 6/20  Iteration 991/3520 Training loss: 1.4815 0.1379 sec/batch\n",
      "Epoch 6/20  Iteration 992/3520 Training loss: 1.4814 0.1360 sec/batch\n",
      "Epoch 6/20  Iteration 993/3520 Training loss: 1.4814 0.1358 sec/batch\n",
      "Epoch 6/20  Iteration 994/3520 Training loss: 1.4812 0.1375 sec/batch\n",
      "Epoch 6/20  Iteration 995/3520 Training loss: 1.4808 0.1361 sec/batch\n",
      "Epoch 6/20  Iteration 996/3520 Training loss: 1.4807 0.1366 sec/batch\n",
      "Epoch 6/20  Iteration 997/3520 Training loss: 1.4804 0.1351 sec/batch\n",
      "Epoch 6/20  Iteration 998/3520 Training loss: 1.4804 0.1377 sec/batch\n",
      "Epoch 6/20  Iteration 999/3520 Training loss: 1.4801 0.1356 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3520 Training loss: 1.4800 0.1326 sec/batch\n",
      "Validation loss: 1.38342 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1001/3520 Training loss: 1.4807 0.1429 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3520 Training loss: 1.4805 0.1598 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3520 Training loss: 1.4804 0.1475 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3520 Training loss: 1.4802 0.1380 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3520 Training loss: 1.4800 0.1370 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3520 Training loss: 1.4795 0.1390 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3520 Training loss: 1.4791 0.1364 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3520 Training loss: 1.4790 0.1330 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3520 Training loss: 1.4788 0.1375 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3520 Training loss: 1.4788 0.1390 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3520 Training loss: 1.4786 0.1369 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3520 Training loss: 1.4787 0.1359 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3520 Training loss: 1.4786 0.1369 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3520 Training loss: 1.4786 0.1361 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3520 Training loss: 1.4786 0.1364 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3520 Training loss: 1.4783 0.1349 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3520 Training loss: 1.4783 0.1366 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3520 Training loss: 1.4780 0.1360 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3520 Training loss: 1.4780 0.1380 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3520 Training loss: 1.4779 0.1364 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3520 Training loss: 1.4778 0.1323 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3520 Training loss: 1.4776 0.1387 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3520 Training loss: 1.4770 0.1291 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3520 Training loss: 1.4768 0.1316 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3520 Training loss: 1.4768 0.1371 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3520 Training loss: 1.4767 0.1358 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3520 Training loss: 1.4765 0.1379 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3520 Training loss: 1.4763 0.1333 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3520 Training loss: 1.4761 0.1350 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3520 Training loss: 1.4760 0.1368 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3520 Training loss: 1.4757 0.1368 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3520 Training loss: 1.4754 0.1397 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3520 Training loss: 1.4752 0.1360 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3520 Training loss: 1.4748 0.1368 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3520 Training loss: 1.4746 0.1335 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3520 Training loss: 1.4745 0.1365 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3520 Training loss: 1.4741 0.1374 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3520 Training loss: 1.4740 0.1363 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3520 Training loss: 1.4739 0.1298 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3520 Training loss: 1.4738 0.1357 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3520 Training loss: 1.4735 0.1359 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3520 Training loss: 1.4734 0.1370 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3520 Training loss: 1.4734 0.1358 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3520 Training loss: 1.4730 0.1379 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3520 Training loss: 1.4725 0.1384 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3520 Training loss: 1.4722 0.1356 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3520 Training loss: 1.4719 0.1367 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3520 Training loss: 1.4717 0.1383 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3520 Training loss: 1.4717 0.1358 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3520 Training loss: 1.4715 0.1370 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3520 Training loss: 1.4713 0.1357 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3520 Training loss: 1.4711 0.1360 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3520 Training loss: 1.4710 0.1382 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3520 Training loss: 1.4708 0.1392 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3520 Training loss: 1.4706 0.1340 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3520 Training loss: 1.4707 0.1371 sec/batch\n",
      "Epoch 7/20  Iteration 1057/3520 Training loss: 1.4838 0.1330 sec/batch\n",
      "Epoch 7/20  Iteration 1058/3520 Training loss: 1.4403 0.1361 sec/batch\n",
      "Epoch 7/20  Iteration 1059/3520 Training loss: 1.4383 0.1317 sec/batch\n",
      "Epoch 7/20  Iteration 1060/3520 Training loss: 1.4360 0.1387 sec/batch\n",
      "Epoch 7/20  Iteration 1061/3520 Training loss: 1.4380 0.1356 sec/batch\n",
      "Epoch 7/20  Iteration 1062/3520 Training loss: 1.4369 0.1348 sec/batch\n",
      "Epoch 7/20  Iteration 1063/3520 Training loss: 1.4347 0.1388 sec/batch\n",
      "Epoch 7/20  Iteration 1064/3520 Training loss: 1.4368 0.1380 sec/batch\n",
      "Epoch 7/20  Iteration 1065/3520 Training loss: 1.4338 0.1389 sec/batch\n",
      "Epoch 7/20  Iteration 1066/3520 Training loss: 1.4331 0.1363 sec/batch\n",
      "Epoch 7/20  Iteration 1067/3520 Training loss: 1.4334 0.1362 sec/batch\n",
      "Epoch 7/20  Iteration 1068/3520 Training loss: 1.4342 0.1336 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3520 Training loss: 1.4326 0.1340 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3520 Training loss: 1.4337 0.1351 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3520 Training loss: 1.4331 0.1377 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3520 Training loss: 1.4322 0.1362 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3520 Training loss: 1.4313 0.1349 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3520 Training loss: 1.4318 0.1349 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3520 Training loss: 1.4307 0.1387 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3520 Training loss: 1.4319 0.1373 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3520 Training loss: 1.4320 0.1368 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3520 Training loss: 1.4316 0.1370 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3520 Training loss: 1.4324 0.1373 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3520 Training loss: 1.4331 0.1361 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3520 Training loss: 1.4333 0.1384 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3520 Training loss: 1.4326 0.1377 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3520 Training loss: 1.4314 0.1363 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3520 Training loss: 1.4312 0.1351 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3520 Training loss: 1.4305 0.1302 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3520 Training loss: 1.4305 0.1328 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3520 Training loss: 1.4300 0.1380 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3520 Training loss: 1.4297 0.1357 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3520 Training loss: 1.4307 0.1360 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3520 Training loss: 1.4305 0.1353 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3520 Training loss: 1.4302 0.1355 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3520 Training loss: 1.4306 0.1314 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3520 Training loss: 1.4297 0.1370 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3520 Training loss: 1.4304 0.1307 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3520 Training loss: 1.4313 0.1331 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3520 Training loss: 1.4309 0.1359 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3520 Training loss: 1.4304 0.1377 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3520 Training loss: 1.4294 0.1376 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3520 Training loss: 1.4297 0.1370 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3520 Training loss: 1.4288 0.1363 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3520 Training loss: 1.4285 0.1369 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3520 Training loss: 1.4281 0.1390 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3520 Training loss: 1.4279 0.1388 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3520 Training loss: 1.4280 0.1382 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3520 Training loss: 1.4278 0.1357 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3520 Training loss: 1.4277 0.1366 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3520 Training loss: 1.4277 0.1390 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3520 Training loss: 1.4278 0.1395 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3520 Training loss: 1.4279 0.1374 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3520 Training loss: 1.4277 0.1373 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3520 Training loss: 1.4271 0.1374 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3520 Training loss: 1.4268 0.1368 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3520 Training loss: 1.4265 0.1349 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3520 Training loss: 1.4263 0.1364 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3520 Training loss: 1.4255 0.1307 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3520 Training loss: 1.4258 0.1366 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3520 Training loss: 1.4255 0.1335 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3520 Training loss: 1.4252 0.1368 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3520 Training loss: 1.4251 0.1382 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3520 Training loss: 1.4249 0.1363 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3520 Training loss: 1.4242 0.1367 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3520 Training loss: 1.4240 0.1364 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3520 Training loss: 1.4237 0.1378 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3520 Training loss: 1.4230 0.1350 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3520 Training loss: 1.4228 0.1358 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3520 Training loss: 1.4226 0.1362 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3520 Training loss: 1.4222 0.1392 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3520 Training loss: 1.4217 0.1356 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3520 Training loss: 1.4213 0.1369 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3520 Training loss: 1.4205 0.1375 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3520 Training loss: 1.4202 0.1348 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3520 Training loss: 1.4204 0.1356 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3520 Training loss: 1.4204 0.1341 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3520 Training loss: 1.4207 0.1367 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3520 Training loss: 1.4204 0.1366 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3520 Training loss: 1.4202 0.1367 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3520 Training loss: 1.4197 0.1385 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3520 Training loss: 1.4199 0.1348 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3520 Training loss: 1.4195 0.1362 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3520 Training loss: 1.4195 0.1371 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3520 Training loss: 1.4193 0.1344 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3520 Training loss: 1.4187 0.1379 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3520 Training loss: 1.4189 0.1358 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3520 Training loss: 1.4187 0.1342 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3520 Training loss: 1.4187 0.1371 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3520 Training loss: 1.4184 0.1346 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3520 Training loss: 1.4182 0.1363 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3520 Training loss: 1.4182 0.1366 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3520 Training loss: 1.4179 0.1433 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3520 Training loss: 1.4175 0.1359 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3520 Training loss: 1.4172 0.1363 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3520 Training loss: 1.4166 0.1389 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3520 Training loss: 1.4167 0.1377 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3520 Training loss: 1.4166 0.1376 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3520 Training loss: 1.4165 0.1356 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3520 Training loss: 1.4161 0.1361 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3520 Training loss: 1.4159 0.1362 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3520 Training loss: 1.4158 0.1341 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3520 Training loss: 1.4154 0.1356 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3520 Training loss: 1.4146 0.1371 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3520 Training loss: 1.4141 0.1350 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3520 Training loss: 1.4141 0.1379 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3520 Training loss: 1.4139 0.1322 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3520 Training loss: 1.4135 0.1385 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3520 Training loss: 1.4134 0.1363 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3520 Training loss: 1.4132 0.1367 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3520 Training loss: 1.4130 0.1363 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3520 Training loss: 1.4130 0.1348 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3520 Training loss: 1.4131 0.1353 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3520 Training loss: 1.4128 0.1361 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3520 Training loss: 1.4124 0.1307 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3520 Training loss: 1.4122 0.1364 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3520 Training loss: 1.4119 0.1379 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3520 Training loss: 1.4118 0.1314 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3520 Training loss: 1.4117 0.1325 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3520 Training loss: 1.4115 0.1369 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3520 Training loss: 1.4113 0.1352 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3520 Training loss: 1.4112 0.1323 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3520 Training loss: 1.4110 0.1323 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3520 Training loss: 1.4108 0.1367 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3520 Training loss: 1.4106 0.1373 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3520 Training loss: 1.4100 0.1377 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3520 Training loss: 1.4097 0.1366 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3520 Training loss: 1.4095 0.1352 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3520 Training loss: 1.4093 0.1356 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3520 Training loss: 1.4094 0.1319 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3520 Training loss: 1.4092 0.1375 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3520 Training loss: 1.4094 0.1353 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3520 Training loss: 1.4091 0.1381 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3520 Training loss: 1.4091 0.1359 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3520 Training loss: 1.4091 0.1382 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3520 Training loss: 1.4090 0.1316 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3520 Training loss: 1.4089 0.1374 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3520 Training loss: 1.4086 0.1355 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3520 Training loss: 1.4085 0.1349 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3520 Training loss: 1.4085 0.1379 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3520 Training loss: 1.4084 0.1370 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3520 Training loss: 1.4082 0.1360 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3520 Training loss: 1.4078 0.1372 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3520 Training loss: 1.4077 0.1388 sec/batch\n",
      "Validation loss: 1.31365 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1201/3520 Training loss: 1.4085 0.1385 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3520 Training loss: 1.4085 0.1362 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3520 Training loss: 1.4083 0.1327 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3520 Training loss: 1.4081 0.1362 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3520 Training loss: 1.4080 0.1369 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3520 Training loss: 1.4080 0.1362 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3520 Training loss: 1.4078 0.1365 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3520 Training loss: 1.4075 0.1349 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3520 Training loss: 1.4074 0.1322 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3520 Training loss: 1.4071 0.1360 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3520 Training loss: 1.4070 0.1307 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3520 Training loss: 1.4069 0.1380 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3520 Training loss: 1.4066 0.1371 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3520 Training loss: 1.4065 0.1362 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3520 Training loss: 1.4064 0.1377 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3520 Training loss: 1.4065 0.1370 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3520 Training loss: 1.4061 0.1362 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3520 Training loss: 1.4061 0.1369 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3520 Training loss: 1.4061 0.1366 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3520 Training loss: 1.4058 0.1370 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3520 Training loss: 1.4053 0.1359 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3520 Training loss: 1.4051 0.1365 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3520 Training loss: 1.4048 0.1376 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3520 Training loss: 1.4047 0.1357 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3520 Training loss: 1.4047 0.1359 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3520 Training loss: 1.4045 0.1366 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3520 Training loss: 1.4045 0.1353 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3520 Training loss: 1.4043 0.1361 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3520 Training loss: 1.4042 0.1360 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3520 Training loss: 1.4041 0.1337 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3520 Training loss: 1.4039 0.1349 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3520 Training loss: 1.4039 0.1366 sec/batch\n",
      "Epoch 8/20  Iteration 1233/3520 Training loss: 1.4562 0.1390 sec/batch\n",
      "Epoch 8/20  Iteration 1234/3520 Training loss: 1.3990 0.1365 sec/batch\n",
      "Epoch 8/20  Iteration 1235/3520 Training loss: 1.3964 0.1357 sec/batch\n",
      "Epoch 8/20  Iteration 1236/3520 Training loss: 1.3941 0.1307 sec/batch\n",
      "Epoch 8/20  Iteration 1237/3520 Training loss: 1.3958 0.1359 sec/batch\n",
      "Epoch 8/20  Iteration 1238/3520 Training loss: 1.3961 0.1299 sec/batch\n",
      "Epoch 8/20  Iteration 1239/3520 Training loss: 1.3947 0.1386 sec/batch\n",
      "Epoch 8/20  Iteration 1240/3520 Training loss: 1.3962 0.1385 sec/batch\n",
      "Epoch 8/20  Iteration 1241/3520 Training loss: 1.3920 0.1378 sec/batch\n",
      "Epoch 8/20  Iteration 1242/3520 Training loss: 1.3914 0.1353 sec/batch\n",
      "Epoch 8/20  Iteration 1243/3520 Training loss: 1.3917 0.1385 sec/batch\n",
      "Epoch 8/20  Iteration 1244/3520 Training loss: 1.3923 0.1357 sec/batch\n",
      "Epoch 8/20  Iteration 1245/3520 Training loss: 1.3910 0.1365 sec/batch\n",
      "Epoch 8/20  Iteration 1246/3520 Training loss: 1.3923 0.1391 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3520 Training loss: 1.3911 0.1394 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3520 Training loss: 1.3896 0.1365 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3520 Training loss: 1.3886 0.1375 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3520 Training loss: 1.3891 0.1355 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3520 Training loss: 1.3876 0.1329 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3520 Training loss: 1.3884 0.1374 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3520 Training loss: 1.3886 0.1326 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3520 Training loss: 1.3879 0.1353 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3520 Training loss: 1.3881 0.1335 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3520 Training loss: 1.3884 0.1382 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3520 Training loss: 1.3880 0.1363 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3520 Training loss: 1.3871 0.1385 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3520 Training loss: 1.3856 0.1371 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3520 Training loss: 1.3849 0.1371 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3520 Training loss: 1.3839 0.1382 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3520 Training loss: 1.3837 0.1360 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3520 Training loss: 1.3829 0.1356 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3520 Training loss: 1.3823 0.1369 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3520 Training loss: 1.3830 0.1389 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3520 Training loss: 1.3823 0.1339 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3520 Training loss: 1.3818 0.1386 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3520 Training loss: 1.3821 0.1312 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3520 Training loss: 1.3807 0.1362 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3520 Training loss: 1.3809 0.1329 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3520 Training loss: 1.3816 0.1367 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3520 Training loss: 1.3812 0.1362 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3520 Training loss: 1.3806 0.1334 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3520 Training loss: 1.3797 0.1365 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3520 Training loss: 1.3794 0.1365 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3520 Training loss: 1.3784 0.1376 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3520 Training loss: 1.3779 0.1366 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3520 Training loss: 1.3776 0.1368 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3520 Training loss: 1.3774 0.1382 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3520 Training loss: 1.3772 0.1358 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3520 Training loss: 1.3769 0.1358 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3520 Training loss: 1.3766 0.1395 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3520 Training loss: 1.3762 0.1362 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3520 Training loss: 1.3761 0.1381 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3520 Training loss: 1.3761 0.1350 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3520 Training loss: 1.3757 0.1374 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3520 Training loss: 1.3752 0.1380 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3520 Training loss: 1.3751 0.1369 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3520 Training loss: 1.3747 0.1383 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3520 Training loss: 1.3745 0.1380 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3520 Training loss: 1.3738 0.1377 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3520 Training loss: 1.3740 0.1360 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3520 Training loss: 1.3737 0.1325 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3520 Training loss: 1.3736 0.1368 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3520 Training loss: 1.3735 0.1385 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3520 Training loss: 1.3732 0.1305 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3520 Training loss: 1.3726 0.1371 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3520 Training loss: 1.3724 0.1332 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3520 Training loss: 1.3721 0.1356 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3520 Training loss: 1.3713 0.1355 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3520 Training loss: 1.3709 0.1373 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3520 Training loss: 1.3707 0.1364 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3520 Training loss: 1.3704 0.1365 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3520 Training loss: 1.3699 0.1334 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3520 Training loss: 1.3694 0.1377 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3520 Training loss: 1.3687 0.1329 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3520 Training loss: 1.3684 0.1368 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3520 Training loss: 1.3685 0.1357 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3520 Training loss: 1.3687 0.1372 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3520 Training loss: 1.3690 0.1394 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3520 Training loss: 1.3688 0.1321 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3520 Training loss: 1.3686 0.1361 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3520 Training loss: 1.3680 0.1387 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3520 Training loss: 1.3681 0.1364 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3520 Training loss: 1.3677 0.1353 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3520 Training loss: 1.3679 0.1363 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3520 Training loss: 1.3676 0.1346 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3520 Training loss: 1.3671 0.1367 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3520 Training loss: 1.3673 0.1362 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3520 Training loss: 1.3671 0.1372 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3520 Training loss: 1.3673 0.1313 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3520 Training loss: 1.3671 0.1375 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3520 Training loss: 1.3668 0.1421 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3520 Training loss: 1.3668 0.1363 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3520 Training loss: 1.3664 0.1365 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3520 Training loss: 1.3661 0.1367 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3520 Training loss: 1.3659 0.1332 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3520 Training loss: 1.3655 0.1369 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3520 Training loss: 1.3656 0.1372 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3520 Training loss: 1.3655 0.1363 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3520 Training loss: 1.3654 0.1357 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3520 Training loss: 1.3652 0.1329 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3520 Training loss: 1.3651 0.1366 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3520 Training loss: 1.3652 0.1297 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3520 Training loss: 1.3648 0.1369 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3520 Training loss: 1.3642 0.1359 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3520 Training loss: 1.3637 0.1366 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3520 Training loss: 1.3638 0.1365 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3520 Training loss: 1.3636 0.1370 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3520 Training loss: 1.3632 0.1361 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3520 Training loss: 1.3633 0.1385 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3520 Training loss: 1.3631 0.1418 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3520 Training loss: 1.3631 0.1371 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3520 Training loss: 1.3631 0.1382 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3520 Training loss: 1.3632 0.1384 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3520 Training loss: 1.3630 0.1352 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3520 Training loss: 1.3627 0.1343 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3520 Training loss: 1.3625 0.1366 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3520 Training loss: 1.3623 0.1366 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3520 Training loss: 1.3623 0.1368 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3520 Training loss: 1.3622 0.1332 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3520 Training loss: 1.3620 0.1413 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3520 Training loss: 1.3619 0.1374 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3520 Training loss: 1.3618 0.1363 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3520 Training loss: 1.3616 0.1369 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3520 Training loss: 1.3615 0.1379 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3520 Training loss: 1.3612 0.1367 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3520 Training loss: 1.3608 0.1367 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3520 Training loss: 1.3605 0.1357 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3520 Training loss: 1.3604 0.1375 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3520 Training loss: 1.3602 0.1390 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3520 Training loss: 1.3604 0.1375 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3520 Training loss: 1.3603 0.1373 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3520 Training loss: 1.3605 0.1356 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3520 Training loss: 1.3604 0.1322 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3520 Training loss: 1.3604 0.1370 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3520 Training loss: 1.3604 0.1339 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3520 Training loss: 1.3602 0.1324 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3520 Training loss: 1.3602 0.1369 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3520 Training loss: 1.3600 0.1364 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3520 Training loss: 1.3601 0.1362 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3520 Training loss: 1.3600 0.1364 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3520 Training loss: 1.3599 0.1342 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3520 Training loss: 1.3597 0.1368 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3520 Training loss: 1.3593 0.1319 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3520 Training loss: 1.3592 0.1371 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3520 Training loss: 1.3593 0.1382 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3520 Training loss: 1.3592 0.1380 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3520 Training loss: 1.3590 0.1367 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3520 Training loss: 1.3588 0.1379 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3520 Training loss: 1.3587 0.1339 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3520 Training loss: 1.3587 0.1371 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3520 Training loss: 1.3584 0.1303 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3520 Training loss: 1.3582 0.1376 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3520 Training loss: 1.3580 0.1360 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3520 Training loss: 1.3578 0.1382 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3520 Training loss: 1.3576 0.1357 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3520 Training loss: 1.3575 0.1354 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3520 Training loss: 1.3571 0.1320 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3520 Training loss: 1.3571 0.1355 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3520 Training loss: 1.3570 0.1378 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3520 Training loss: 1.3570 0.1383 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3520 Training loss: 1.3567 0.1368 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3520 Training loss: 1.3566 0.1365 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3520 Training loss: 1.3566 0.1401 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3520 Training loss: 1.3563 0.1376 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3520 Training loss: 1.3558 0.1361 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3520 Training loss: 1.3556 0.1351 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3520 Training loss: 1.3553 0.1368 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3520 Training loss: 1.3552 0.1304 sec/batch\n",
      "Validation loss: 1.27283 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 1401/3520 Training loss: 1.3561 0.1389 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3520 Training loss: 1.3559 0.1361 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3520 Training loss: 1.3559 0.1327 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3520 Training loss: 1.3559 0.1356 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3520 Training loss: 1.3559 0.1368 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3520 Training loss: 1.3559 0.1371 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3520 Training loss: 1.3558 0.1362 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3520 Training loss: 1.3559 0.1319 sec/batch\n",
      "Epoch 9/20  Iteration 1409/3520 Training loss: 1.4090 0.1365 sec/batch\n",
      "Epoch 9/20  Iteration 1410/3520 Training loss: 1.3514 0.1367 sec/batch\n",
      "Epoch 9/20  Iteration 1411/3520 Training loss: 1.3478 0.1303 sec/batch\n",
      "Epoch 9/20  Iteration 1412/3520 Training loss: 1.3437 0.1322 sec/batch\n",
      "Epoch 9/20  Iteration 1413/3520 Training loss: 1.3438 0.1360 sec/batch\n",
      "Epoch 9/20  Iteration 1414/3520 Training loss: 1.3421 0.1374 sec/batch\n",
      "Epoch 9/20  Iteration 1415/3520 Training loss: 1.3405 0.1351 sec/batch\n",
      "Epoch 9/20  Iteration 1416/3520 Training loss: 1.3409 0.1457 sec/batch\n",
      "Epoch 9/20  Iteration 1417/3520 Training loss: 1.3383 0.1392 sec/batch\n",
      "Epoch 9/20  Iteration 1418/3520 Training loss: 1.3381 0.1388 sec/batch\n",
      "Epoch 9/20  Iteration 1419/3520 Training loss: 1.3380 0.1361 sec/batch\n",
      "Epoch 9/20  Iteration 1420/3520 Training loss: 1.3380 0.1361 sec/batch\n",
      "Epoch 9/20  Iteration 1421/3520 Training loss: 1.3366 0.1378 sec/batch\n",
      "Epoch 9/20  Iteration 1422/3520 Training loss: 1.3374 0.1365 sec/batch\n",
      "Epoch 9/20  Iteration 1423/3520 Training loss: 1.3365 0.1367 sec/batch\n",
      "Epoch 9/20  Iteration 1424/3520 Training loss: 1.3355 0.1388 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3520 Training loss: 1.3352 0.1324 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3520 Training loss: 1.3356 0.1362 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3520 Training loss: 1.3339 0.1375 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3520 Training loss: 1.3344 0.1367 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3520 Training loss: 1.3343 0.1379 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3520 Training loss: 1.3335 0.1338 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3520 Training loss: 1.3340 0.1375 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3520 Training loss: 1.3346 0.1375 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3520 Training loss: 1.3350 0.1365 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3520 Training loss: 1.3345 0.1358 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3520 Training loss: 1.3332 0.1365 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3520 Training loss: 1.3329 0.1323 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3520 Training loss: 1.3321 0.1373 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3520 Training loss: 1.3322 0.1391 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3520 Training loss: 1.3316 0.1368 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3520 Training loss: 1.3316 0.1374 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3520 Training loss: 1.3324 0.1364 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3520 Training loss: 1.3320 0.1379 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3520 Training loss: 1.3319 0.1358 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3520 Training loss: 1.3327 0.1331 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3520 Training loss: 1.3316 0.1362 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3520 Training loss: 1.3323 0.1368 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3520 Training loss: 1.3332 0.1376 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3520 Training loss: 1.3333 0.1367 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3520 Training loss: 1.3332 0.1378 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3520 Training loss: 1.3325 0.1363 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3520 Training loss: 1.3325 0.1368 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3520 Training loss: 1.3318 0.1362 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3520 Training loss: 1.3314 0.1353 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3520 Training loss: 1.3313 0.1332 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3520 Training loss: 1.3313 0.1346 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3520 Training loss: 1.3314 0.1358 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3520 Training loss: 1.3313 0.1363 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3520 Training loss: 1.3312 0.1361 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3520 Training loss: 1.3308 0.1327 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3520 Training loss: 1.3309 0.1376 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3520 Training loss: 1.3310 0.1382 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3520 Training loss: 1.3310 0.1355 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3520 Training loss: 1.3307 0.1371 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3520 Training loss: 1.3307 0.1359 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3520 Training loss: 1.3305 0.1355 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3520 Training loss: 1.3304 0.1355 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3520 Training loss: 1.3298 0.1339 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3520 Training loss: 1.3300 0.1360 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3520 Training loss: 1.3298 0.1333 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3520 Training loss: 1.3297 0.1372 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3520 Training loss: 1.3297 0.1380 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3520 Training loss: 1.3295 0.1384 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3520 Training loss: 1.3290 0.1352 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3520 Training loss: 1.3291 0.1369 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3520 Training loss: 1.3289 0.1379 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3520 Training loss: 1.3282 0.1384 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3520 Training loss: 1.3280 0.1380 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3520 Training loss: 1.3280 0.1317 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3520 Training loss: 1.3278 0.1340 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3520 Training loss: 1.3276 0.1369 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3520 Training loss: 1.3272 0.1455 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3520 Training loss: 1.3266 0.1380 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3520 Training loss: 1.3266 0.1361 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3520 Training loss: 1.3267 0.1369 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3520 Training loss: 1.3269 0.1351 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3520 Training loss: 1.3272 0.1364 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3520 Training loss: 1.3271 0.1369 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3520 Training loss: 1.3270 0.1363 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3520 Training loss: 1.3265 0.1368 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3520 Training loss: 1.3267 0.1359 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3520 Training loss: 1.3264 0.1372 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3520 Training loss: 1.3265 0.1331 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3520 Training loss: 1.3263 0.1344 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3520 Training loss: 1.3258 0.1361 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3520 Training loss: 1.3260 0.1364 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3520 Training loss: 1.3258 0.1337 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3520 Training loss: 1.3259 0.1359 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3520 Training loss: 1.3257 0.1379 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3520 Training loss: 1.3253 0.1381 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3520 Training loss: 1.3254 0.1342 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3520 Training loss: 1.3251 0.1360 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3520 Training loss: 1.3249 0.1373 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3520 Training loss: 1.3247 0.1360 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3520 Training loss: 1.3242 0.1382 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3520 Training loss: 1.3244 0.1393 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3520 Training loss: 1.3243 0.1313 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3520 Training loss: 1.3243 0.1381 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3520 Training loss: 1.3240 0.1370 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3520 Training loss: 1.3239 0.1374 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3520 Training loss: 1.3239 0.1344 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3520 Training loss: 1.3236 0.1356 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3520 Training loss: 1.3229 0.1364 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3520 Training loss: 1.3224 0.1378 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3520 Training loss: 1.3225 0.1380 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3520 Training loss: 1.3223 0.1369 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3520 Training loss: 1.3220 0.1365 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3520 Training loss: 1.3220 0.1307 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3520 Training loss: 1.3218 0.1360 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3520 Training loss: 1.3220 0.1365 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3520 Training loss: 1.3220 0.1369 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3520 Training loss: 1.3222 0.1383 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3520 Training loss: 1.3219 0.1362 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3520 Training loss: 1.3217 0.1385 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3520 Training loss: 1.3216 0.1371 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3520 Training loss: 1.3213 0.1369 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3520 Training loss: 1.3214 0.1374 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3520 Training loss: 1.3213 0.1316 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3520 Training loss: 1.3212 0.1386 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3520 Training loss: 1.3210 0.1321 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3520 Training loss: 1.3210 0.1301 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3520 Training loss: 1.3209 0.1388 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3520 Training loss: 1.3208 0.1378 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3520 Training loss: 1.3207 0.1414 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3520 Training loss: 1.3203 0.1379 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3520 Training loss: 1.3200 0.1357 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3520 Training loss: 1.3201 0.1362 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3520 Training loss: 1.3199 0.1358 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3520 Training loss: 1.3200 0.1353 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3520 Training loss: 1.3199 0.1364 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3520 Training loss: 1.3202 0.1364 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3520 Training loss: 1.3201 0.1397 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3520 Training loss: 1.3201 0.1365 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3520 Training loss: 1.3202 0.1381 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3520 Training loss: 1.3201 0.1354 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3520 Training loss: 1.3200 0.1370 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3520 Training loss: 1.3199 0.1374 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3520 Training loss: 1.3199 0.1349 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3520 Training loss: 1.3198 0.1350 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3520 Training loss: 1.3198 0.1385 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3520 Training loss: 1.3195 0.1376 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3520 Training loss: 1.3192 0.1381 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3520 Training loss: 1.3190 0.1380 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3520 Training loss: 1.3191 0.1363 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3520 Training loss: 1.3191 0.1366 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3520 Training loss: 1.3189 0.1377 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3520 Training loss: 1.3188 0.1381 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3520 Training loss: 1.3187 0.1369 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3520 Training loss: 1.3187 0.1369 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3520 Training loss: 1.3185 0.1357 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3520 Training loss: 1.3183 0.1360 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3520 Training loss: 1.3181 0.1392 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3520 Training loss: 1.3178 0.1349 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3520 Training loss: 1.3177 0.1375 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3520 Training loss: 1.3177 0.1377 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3520 Training loss: 1.3174 0.1324 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3520 Training loss: 1.3174 0.1355 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3520 Training loss: 1.3173 0.1380 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3520 Training loss: 1.3173 0.1370 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3520 Training loss: 1.3170 0.1363 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3520 Training loss: 1.3170 0.1320 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3520 Training loss: 1.3170 0.1366 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3520 Training loss: 1.3167 0.1387 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3520 Training loss: 1.3163 0.1383 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3520 Training loss: 1.3161 0.1365 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3520 Training loss: 1.3159 0.1321 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3520 Training loss: 1.3159 0.1351 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3520 Training loss: 1.3159 0.1349 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3520 Training loss: 1.3158 0.1292 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3520 Training loss: 1.3158 0.1383 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3520 Training loss: 1.3157 0.1384 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3520 Training loss: 1.3156 0.1364 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3520 Training loss: 1.3156 0.1355 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3520 Training loss: 1.3156 0.1358 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3520 Training loss: 1.3157 0.1381 sec/batch\n",
      "Epoch 10/20  Iteration 1585/3520 Training loss: 1.4072 0.1363 sec/batch\n",
      "Epoch 10/20  Iteration 1586/3520 Training loss: 1.3474 0.1357 sec/batch\n",
      "Epoch 10/20  Iteration 1587/3520 Training loss: 1.3404 0.1377 sec/batch\n",
      "Epoch 10/20  Iteration 1588/3520 Training loss: 1.3322 0.1372 sec/batch\n",
      "Epoch 10/20  Iteration 1589/3520 Training loss: 1.3338 0.1365 sec/batch\n",
      "Epoch 10/20  Iteration 1590/3520 Training loss: 1.3323 0.1360 sec/batch\n",
      "Epoch 10/20  Iteration 1591/3520 Training loss: 1.3285 0.1355 sec/batch\n",
      "Epoch 10/20  Iteration 1592/3520 Training loss: 1.3286 0.1355 sec/batch\n",
      "Epoch 10/20  Iteration 1593/3520 Training loss: 1.3257 0.1365 sec/batch\n",
      "Epoch 10/20  Iteration 1594/3520 Training loss: 1.3254 0.1362 sec/batch\n",
      "Epoch 10/20  Iteration 1595/3520 Training loss: 1.3253 0.1373 sec/batch\n",
      "Epoch 10/20  Iteration 1596/3520 Training loss: 1.3258 0.1357 sec/batch\n",
      "Epoch 10/20  Iteration 1597/3520 Training loss: 1.3239 0.1375 sec/batch\n",
      "Epoch 10/20  Iteration 1598/3520 Training loss: 1.3246 0.1387 sec/batch\n",
      "Epoch 10/20  Iteration 1599/3520 Training loss: 1.3236 0.1381 sec/batch\n",
      "Epoch 10/20  Iteration 1600/3520 Training loss: 1.3225 0.1379 sec/batch\n",
      "Validation loss: 1.25301 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 1601/3520 Training loss: 1.3288 0.1376 sec/batch\n",
      "Epoch 10/20  Iteration 1602/3520 Training loss: 1.3290 0.1378 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3520 Training loss: 1.3269 0.1395 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3520 Training loss: 1.3269 0.1366 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3520 Training loss: 1.3267 0.1381 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3520 Training loss: 1.3255 0.1318 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3520 Training loss: 1.3253 0.1386 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3520 Training loss: 1.3257 0.1327 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3520 Training loss: 1.3256 0.1372 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3520 Training loss: 1.3245 0.1370 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3520 Training loss: 1.3231 0.1384 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3520 Training loss: 1.3226 0.1390 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3520 Training loss: 1.3211 0.1380 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3520 Training loss: 1.3209 0.1360 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3520 Training loss: 1.3200 0.1367 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3520 Training loss: 1.3192 0.1349 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3520 Training loss: 1.3196 0.1368 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3520 Training loss: 1.3191 0.1346 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3520 Training loss: 1.3185 0.1364 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3520 Training loss: 1.3184 0.1359 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3520 Training loss: 1.3167 0.1364 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3520 Training loss: 1.3169 0.1372 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3520 Training loss: 1.3175 0.1324 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3520 Training loss: 1.3170 0.1384 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3520 Training loss: 1.3163 0.1346 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3520 Training loss: 1.3152 0.1384 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3520 Training loss: 1.3148 0.1373 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3520 Training loss: 1.3138 0.1380 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3520 Training loss: 1.3132 0.1368 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3520 Training loss: 1.3125 0.1368 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3520 Training loss: 1.3124 0.1360 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3520 Training loss: 1.3122 0.1379 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3520 Training loss: 1.3117 0.1366 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3520 Training loss: 1.3114 0.1376 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3520 Training loss: 1.3110 0.1369 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3520 Training loss: 1.3109 0.1375 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3520 Training loss: 1.3110 0.1361 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3520 Training loss: 1.3105 0.1358 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3520 Training loss: 1.3101 0.1342 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3520 Training loss: 1.3100 0.1377 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3520 Training loss: 1.3096 0.1316 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3520 Training loss: 1.3096 0.1366 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3520 Training loss: 1.3088 0.1382 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3520 Training loss: 1.3091 0.1294 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3520 Training loss: 1.3088 0.1382 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3520 Training loss: 1.3084 0.1374 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3520 Training loss: 1.3081 0.1362 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3520 Training loss: 1.3078 0.1296 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3520 Training loss: 1.3073 0.1369 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3520 Training loss: 1.3072 0.1367 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3520 Training loss: 1.3068 0.1360 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3520 Training loss: 1.3060 0.1312 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3520 Training loss: 1.3056 0.1376 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3520 Training loss: 1.3053 0.1376 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3520 Training loss: 1.3050 0.1389 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3520 Training loss: 1.3044 0.1372 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3520 Training loss: 1.3038 0.1385 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3520 Training loss: 1.3031 0.1388 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3520 Training loss: 1.3029 0.1386 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3520 Training loss: 1.3030 0.1369 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3520 Training loss: 1.3030 0.1318 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3520 Training loss: 1.3033 0.1358 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3520 Training loss: 1.3031 0.1327 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3520 Training loss: 1.3029 0.1360 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3520 Training loss: 1.3023 0.1383 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3520 Training loss: 1.3025 0.1378 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3520 Training loss: 1.3020 0.1350 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3520 Training loss: 1.3022 0.1355 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3520 Training loss: 1.3020 0.1377 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3520 Training loss: 1.3013 0.1392 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3520 Training loss: 1.3015 0.1357 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3520 Training loss: 1.3013 0.1375 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3520 Training loss: 1.3014 0.1375 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3520 Training loss: 1.3011 0.1360 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3520 Training loss: 1.3008 0.1363 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3520 Training loss: 1.3008 0.1373 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3520 Training loss: 1.3003 0.1393 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3520 Training loss: 1.3000 0.1352 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3520 Training loss: 1.2997 0.1354 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3520 Training loss: 1.2992 0.1358 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3520 Training loss: 1.2992 0.1383 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3520 Training loss: 1.2991 0.1351 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3520 Training loss: 1.2990 0.1376 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3520 Training loss: 1.2988 0.1367 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3520 Training loss: 1.2985 0.1359 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3520 Training loss: 1.2985 0.1371 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3520 Training loss: 1.2982 0.1363 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3520 Training loss: 1.2977 0.1383 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3520 Training loss: 1.2971 0.1374 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3520 Training loss: 1.2971 0.1453 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3520 Training loss: 1.2970 0.1403 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3520 Training loss: 1.2966 0.1359 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3520 Training loss: 1.2966 0.1317 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3520 Training loss: 1.2964 0.1418 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3520 Training loss: 1.2964 0.1386 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3520 Training loss: 1.2964 0.1358 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3520 Training loss: 1.2965 0.1361 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3520 Training loss: 1.2962 0.1366 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3520 Training loss: 1.2959 0.1364 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3520 Training loss: 1.2957 0.1367 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3520 Training loss: 1.2954 0.1329 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3520 Training loss: 1.2955 0.1329 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3520 Training loss: 1.2954 0.1368 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3520 Training loss: 1.2953 0.1370 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3520 Training loss: 1.2951 0.1335 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3520 Training loss: 1.2950 0.1399 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3520 Training loss: 1.2949 0.1376 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3520 Training loss: 1.2948 0.1300 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3520 Training loss: 1.2945 0.1356 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3520 Training loss: 1.2941 0.1357 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3520 Training loss: 1.2939 0.1345 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3520 Training loss: 1.2939 0.1341 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3520 Training loss: 1.2938 0.1379 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3520 Training loss: 1.2939 0.1373 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3520 Training loss: 1.2938 0.1358 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3520 Training loss: 1.2940 0.1300 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3520 Training loss: 1.2938 0.1375 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3520 Training loss: 1.2939 0.1390 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3520 Training loss: 1.2939 0.1369 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3520 Training loss: 1.2938 0.1493 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3520 Training loss: 1.2938 0.1390 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3520 Training loss: 1.2936 0.1369 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3520 Training loss: 1.2937 0.1359 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3520 Training loss: 1.2936 0.1364 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3520 Training loss: 1.2936 0.1378 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3520 Training loss: 1.2933 0.1395 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3520 Training loss: 1.2930 0.1364 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3520 Training loss: 1.2929 0.1361 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3520 Training loss: 1.2929 0.1362 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3520 Training loss: 1.2928 0.1341 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3520 Training loss: 1.2926 0.1335 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3520 Training loss: 1.2925 0.1340 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3520 Training loss: 1.2923 0.1367 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3520 Training loss: 1.2923 0.1325 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3520 Training loss: 1.2921 0.1379 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3520 Training loss: 1.2920 0.1309 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3520 Training loss: 1.2919 0.1306 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3520 Training loss: 1.2916 0.1356 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3520 Training loss: 1.2915 0.1353 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3520 Training loss: 1.2915 0.1364 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3520 Training loss: 1.2912 0.1358 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3520 Training loss: 1.2913 0.1379 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3520 Training loss: 1.2912 0.1374 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3520 Training loss: 1.2912 0.1376 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3520 Training loss: 1.2909 0.1353 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3520 Training loss: 1.2908 0.1360 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3520 Training loss: 1.2908 0.1333 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3520 Training loss: 1.2905 0.1395 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3520 Training loss: 1.2901 0.1357 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3520 Training loss: 1.2900 0.1370 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3520 Training loss: 1.2897 0.1357 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3520 Training loss: 1.2897 0.1358 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3520 Training loss: 1.2897 0.1363 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3520 Training loss: 1.2895 0.1391 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3520 Training loss: 1.2895 0.1363 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3520 Training loss: 1.2895 0.1381 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3520 Training loss: 1.2894 0.1359 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3520 Training loss: 1.2894 0.1379 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3520 Training loss: 1.2894 0.1372 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3520 Training loss: 1.2896 0.1375 sec/batch\n",
      "Epoch 11/20  Iteration 1761/3520 Training loss: 1.3647 0.1376 sec/batch\n",
      "Epoch 11/20  Iteration 1762/3520 Training loss: 1.3027 0.1370 sec/batch\n",
      "Epoch 11/20  Iteration 1763/3520 Training loss: 1.2914 0.1362 sec/batch\n",
      "Epoch 11/20  Iteration 1764/3520 Training loss: 1.2854 0.1378 sec/batch\n",
      "Epoch 11/20  Iteration 1765/3520 Training loss: 1.2847 0.1368 sec/batch\n",
      "Epoch 11/20  Iteration 1766/3520 Training loss: 1.2835 0.1364 sec/batch\n",
      "Epoch 11/20  Iteration 1767/3520 Training loss: 1.2805 0.1364 sec/batch\n",
      "Epoch 11/20  Iteration 1768/3520 Training loss: 1.2812 0.1366 sec/batch\n",
      "Epoch 11/20  Iteration 1769/3520 Training loss: 1.2770 0.1370 sec/batch\n",
      "Epoch 11/20  Iteration 1770/3520 Training loss: 1.2762 0.1360 sec/batch\n",
      "Epoch 11/20  Iteration 1771/3520 Training loss: 1.2767 0.1351 sec/batch\n",
      "Epoch 11/20  Iteration 1772/3520 Training loss: 1.2765 0.1361 sec/batch\n",
      "Epoch 11/20  Iteration 1773/3520 Training loss: 1.2750 0.1300 sec/batch\n",
      "Epoch 11/20  Iteration 1774/3520 Training loss: 1.2764 0.1389 sec/batch\n",
      "Epoch 11/20  Iteration 1775/3520 Training loss: 1.2757 0.1369 sec/batch\n",
      "Epoch 11/20  Iteration 1776/3520 Training loss: 1.2743 0.1301 sec/batch\n",
      "Epoch 11/20  Iteration 1777/3520 Training loss: 1.2738 0.1367 sec/batch\n",
      "Epoch 11/20  Iteration 1778/3520 Training loss: 1.2745 0.1381 sec/batch\n",
      "Epoch 11/20  Iteration 1779/3520 Training loss: 1.2732 0.1386 sec/batch\n",
      "Epoch 11/20  Iteration 1780/3520 Training loss: 1.2741 0.1381 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3520 Training loss: 1.2744 0.1344 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3520 Training loss: 1.2737 0.1313 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3520 Training loss: 1.2742 0.1371 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3520 Training loss: 1.2749 0.1370 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3520 Training loss: 1.2753 0.1373 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3520 Training loss: 1.2748 0.1363 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3520 Training loss: 1.2740 0.1379 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3520 Training loss: 1.2740 0.1313 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3520 Training loss: 1.2729 0.1359 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3520 Training loss: 1.2731 0.1360 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3520 Training loss: 1.2726 0.1390 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3520 Training loss: 1.2726 0.1387 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3520 Training loss: 1.2735 0.1384 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3520 Training loss: 1.2732 0.1367 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3520 Training loss: 1.2732 0.1375 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3520 Training loss: 1.2737 0.1363 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3520 Training loss: 1.2729 0.1374 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3520 Training loss: 1.2735 0.1382 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3520 Training loss: 1.2739 0.1355 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3520 Training loss: 1.2738 0.1364 sec/batch\n",
      "Validation loss: 1.21879 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 1801/3520 Training loss: 1.2779 0.1482 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3520 Training loss: 1.2771 0.1590 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3520 Training loss: 1.2771 0.1470 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3520 Training loss: 1.2764 0.1335 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3520 Training loss: 1.2760 0.1368 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3520 Training loss: 1.2756 0.1369 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3520 Training loss: 1.2754 0.1370 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3520 Training loss: 1.2756 0.1363 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3520 Training loss: 1.2754 0.1372 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3520 Training loss: 1.2753 0.1364 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3520 Training loss: 1.2750 0.1337 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3520 Training loss: 1.2749 0.1379 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3520 Training loss: 1.2749 0.1268 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3520 Training loss: 1.2748 0.1366 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3520 Training loss: 1.2744 0.1371 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3520 Training loss: 1.2744 0.1352 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3520 Training loss: 1.2739 0.1368 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3520 Training loss: 1.2740 0.1369 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3520 Training loss: 1.2736 0.1352 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3520 Training loss: 1.2738 0.1359 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3520 Training loss: 1.2737 0.1367 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3520 Training loss: 1.2736 0.1381 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3520 Training loss: 1.2736 0.1364 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3520 Training loss: 1.2734 0.1358 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3520 Training loss: 1.2728 0.1358 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3520 Training loss: 1.2727 0.1369 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3520 Training loss: 1.2724 0.1355 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3520 Training loss: 1.2716 0.1367 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3520 Training loss: 1.2713 0.1373 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3520 Training loss: 1.2711 0.1357 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3520 Training loss: 1.2710 0.1323 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3520 Training loss: 1.2707 0.1366 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3520 Training loss: 1.2703 0.1375 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3520 Training loss: 1.2698 0.1329 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3520 Training loss: 1.2695 0.1372 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3520 Training loss: 1.2696 0.1384 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3520 Training loss: 1.2699 0.1314 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3520 Training loss: 1.2703 0.1363 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3520 Training loss: 1.2701 0.1362 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3520 Training loss: 1.2699 0.1372 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3520 Training loss: 1.2696 0.1358 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3520 Training loss: 1.2697 0.1325 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3520 Training loss: 1.2694 0.1332 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3520 Training loss: 1.2696 0.1349 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3520 Training loss: 1.2693 0.1371 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3520 Training loss: 1.2687 0.1361 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3520 Training loss: 1.2689 0.1360 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3520 Training loss: 1.2687 0.1351 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3520 Training loss: 1.2689 0.1361 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3520 Training loss: 1.2686 0.1365 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3520 Training loss: 1.2683 0.1376 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3520 Training loss: 1.2684 0.1359 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3520 Training loss: 1.2680 0.1360 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3520 Training loss: 1.2677 0.1359 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3520 Training loss: 1.2676 0.1369 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3520 Training loss: 1.2671 0.1360 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3520 Training loss: 1.2672 0.1350 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3520 Training loss: 1.2670 0.1359 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3520 Training loss: 1.2670 0.1369 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3520 Training loss: 1.2668 0.1357 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3520 Training loss: 1.2666 0.1344 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3520 Training loss: 1.2667 0.1362 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3520 Training loss: 1.2664 0.1357 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3520 Training loss: 1.2658 0.1368 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3520 Training loss: 1.2654 0.1330 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3520 Training loss: 1.2654 0.1361 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3520 Training loss: 1.2653 0.1361 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3520 Training loss: 1.2649 0.1331 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3520 Training loss: 1.2650 0.1366 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3520 Training loss: 1.2648 0.1388 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3520 Training loss: 1.2648 0.1352 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3520 Training loss: 1.2648 0.1373 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3520 Training loss: 1.2649 0.1369 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3520 Training loss: 1.2647 0.1369 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3520 Training loss: 1.2646 0.1341 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3520 Training loss: 1.2645 0.1353 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3520 Training loss: 1.2643 0.1387 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3520 Training loss: 1.2643 0.1342 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3520 Training loss: 1.2642 0.1360 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3520 Training loss: 1.2640 0.1390 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3520 Training loss: 1.2640 0.1367 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3520 Training loss: 1.2639 0.1383 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3520 Training loss: 1.2637 0.1380 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3520 Training loss: 1.2636 0.1370 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3520 Training loss: 1.2634 0.1368 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3520 Training loss: 1.2631 0.1360 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3520 Training loss: 1.2629 0.1354 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3520 Training loss: 1.2629 0.1343 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3520 Training loss: 1.2628 0.1388 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3520 Training loss: 1.2629 0.1379 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3520 Training loss: 1.2628 0.1379 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3520 Training loss: 1.2631 0.1376 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3520 Training loss: 1.2630 0.1369 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3520 Training loss: 1.2631 0.1312 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3520 Training loss: 1.2631 0.1379 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3520 Training loss: 1.2630 0.1353 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3520 Training loss: 1.2630 0.1376 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3520 Training loss: 1.2628 0.1371 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3520 Training loss: 1.2629 0.1345 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3520 Training loss: 1.2629 0.1308 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3520 Training loss: 1.2629 0.1379 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3520 Training loss: 1.2628 0.1360 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3520 Training loss: 1.2624 0.1372 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3520 Training loss: 1.2624 0.1337 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3520 Training loss: 1.2624 0.1345 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3520 Training loss: 1.2624 0.1374 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3520 Training loss: 1.2623 0.1375 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3520 Training loss: 1.2622 0.1367 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3520 Training loss: 1.2621 0.1337 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3520 Training loss: 1.2621 0.1358 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3520 Training loss: 1.2620 0.1363 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3520 Training loss: 1.2618 0.1377 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3520 Training loss: 1.2618 0.1354 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3520 Training loss: 1.2616 0.1370 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3520 Training loss: 1.2615 0.1370 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3520 Training loss: 1.2615 0.1359 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3520 Training loss: 1.2613 0.1366 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3520 Training loss: 1.2613 0.1367 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3520 Training loss: 1.2613 0.1399 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3520 Training loss: 1.2614 0.1367 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3520 Training loss: 1.2612 0.1360 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3520 Training loss: 1.2612 0.1378 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3520 Training loss: 1.2611 0.1375 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3520 Training loss: 1.2609 0.1359 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3520 Training loss: 1.2606 0.1344 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3520 Training loss: 1.2604 0.1316 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3520 Training loss: 1.2601 0.1378 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3520 Training loss: 1.2601 0.1489 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3520 Training loss: 1.2602 0.1312 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3520 Training loss: 1.2601 0.1363 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3520 Training loss: 1.2601 0.1376 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3520 Training loss: 1.2601 0.1377 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3520 Training loss: 1.2600 0.1361 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3520 Training loss: 1.2601 0.1370 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3520 Training loss: 1.2600 0.1370 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3520 Training loss: 1.2603 0.1381 sec/batch\n",
      "Epoch 12/20  Iteration 1937/3520 Training loss: 1.3366 0.1320 sec/batch\n",
      "Epoch 12/20  Iteration 1938/3520 Training loss: 1.2797 0.1363 sec/batch\n",
      "Epoch 12/20  Iteration 1939/3520 Training loss: 1.2690 0.1364 sec/batch\n",
      "Epoch 12/20  Iteration 1940/3520 Training loss: 1.2607 0.1368 sec/batch\n",
      "Epoch 12/20  Iteration 1941/3520 Training loss: 1.2569 0.1366 sec/batch\n",
      "Epoch 12/20  Iteration 1942/3520 Training loss: 1.2566 0.1361 sec/batch\n",
      "Epoch 12/20  Iteration 1943/3520 Training loss: 1.2546 0.1374 sec/batch\n",
      "Epoch 12/20  Iteration 1944/3520 Training loss: 1.2542 0.1387 sec/batch\n",
      "Epoch 12/20  Iteration 1945/3520 Training loss: 1.2517 0.1383 sec/batch\n",
      "Epoch 12/20  Iteration 1946/3520 Training loss: 1.2508 0.1335 sec/batch\n",
      "Epoch 12/20  Iteration 1947/3520 Training loss: 1.2507 0.1362 sec/batch\n",
      "Epoch 12/20  Iteration 1948/3520 Training loss: 1.2503 0.1377 sec/batch\n",
      "Epoch 12/20  Iteration 1949/3520 Training loss: 1.2489 0.1370 sec/batch\n",
      "Epoch 12/20  Iteration 1950/3520 Training loss: 1.2501 0.1373 sec/batch\n",
      "Epoch 12/20  Iteration 1951/3520 Training loss: 1.2496 0.1367 sec/batch\n",
      "Epoch 12/20  Iteration 1952/3520 Training loss: 1.2489 0.1371 sec/batch\n",
      "Epoch 12/20  Iteration 1953/3520 Training loss: 1.2482 0.1380 sec/batch\n",
      "Epoch 12/20  Iteration 1954/3520 Training loss: 1.2489 0.1382 sec/batch\n",
      "Epoch 12/20  Iteration 1955/3520 Training loss: 1.2478 0.1377 sec/batch\n",
      "Epoch 12/20  Iteration 1956/3520 Training loss: 1.2485 0.1361 sec/batch\n",
      "Epoch 12/20  Iteration 1957/3520 Training loss: 1.2487 0.1374 sec/batch\n",
      "Epoch 12/20  Iteration 1958/3520 Training loss: 1.2480 0.1359 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3520 Training loss: 1.2485 0.1360 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3520 Training loss: 1.2497 0.1362 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3520 Training loss: 1.2497 0.1373 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3520 Training loss: 1.2490 0.1352 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3520 Training loss: 1.2480 0.1370 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3520 Training loss: 1.2478 0.1381 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3520 Training loss: 1.2470 0.1339 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3520 Training loss: 1.2474 0.1370 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3520 Training loss: 1.2470 0.1344 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3520 Training loss: 1.2470 0.1395 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3520 Training loss: 1.2480 0.1370 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3520 Training loss: 1.2477 0.1380 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3520 Training loss: 1.2477 0.1367 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3520 Training loss: 1.2481 0.1350 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3520 Training loss: 1.2474 0.1367 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3520 Training loss: 1.2480 0.1369 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3520 Training loss: 1.2485 0.1365 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3520 Training loss: 1.2485 0.1366 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3520 Training loss: 1.2483 0.1380 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3520 Training loss: 1.2476 0.1371 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3520 Training loss: 1.2475 0.1328 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3520 Training loss: 1.2469 0.1326 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3520 Training loss: 1.2465 0.1365 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3520 Training loss: 1.2460 0.1358 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3520 Training loss: 1.2459 0.1361 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3520 Training loss: 1.2461 0.1335 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3520 Training loss: 1.2460 0.1371 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3520 Training loss: 1.2461 0.1381 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3520 Training loss: 1.2458 0.1355 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3520 Training loss: 1.2458 0.1372 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3520 Training loss: 1.2460 0.1368 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3520 Training loss: 1.2456 0.1334 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3520 Training loss: 1.2453 0.1370 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3520 Training loss: 1.2452 0.1368 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3520 Training loss: 1.2449 0.1382 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3520 Training loss: 1.2450 0.1384 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3520 Training loss: 1.2443 0.1353 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3520 Training loss: 1.2445 0.1376 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3520 Training loss: 1.2444 0.1382 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3520 Training loss: 1.2446 0.1356 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3520 Training loss: 1.2445 0.1371 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3520 Training loss: 1.2444 0.1390 sec/batch\n",
      "Validation loss: 1.1991 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 2001/3520 Training loss: 1.2467 0.1390 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3520 Training loss: 1.2468 0.1376 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3520 Training loss: 1.2467 0.1305 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3520 Training loss: 1.2460 0.1372 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3520 Training loss: 1.2458 0.1375 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3520 Training loss: 1.2458 0.1345 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3520 Training loss: 1.2456 0.1367 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3520 Training loss: 1.2451 0.1351 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3520 Training loss: 1.2448 0.1339 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3520 Training loss: 1.2443 0.1384 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3520 Training loss: 1.2441 0.1375 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3520 Training loss: 1.2442 0.1364 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3520 Training loss: 1.2445 0.1369 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3520 Training loss: 1.2449 0.1357 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3520 Training loss: 1.2448 0.1369 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3520 Training loss: 1.2447 0.1373 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3520 Training loss: 1.2443 0.1379 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3520 Training loss: 1.2444 0.1375 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3520 Training loss: 1.2441 0.1419 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3520 Training loss: 1.2444 0.1375 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3520 Training loss: 1.2442 0.1379 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3520 Training loss: 1.2438 0.1368 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3520 Training loss: 1.2440 0.1368 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3520 Training loss: 1.2439 0.1361 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3520 Training loss: 1.2441 0.1327 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3520 Training loss: 1.2438 0.1339 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3520 Training loss: 1.2436 0.1357 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3520 Training loss: 1.2438 0.1368 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3520 Training loss: 1.2435 0.1303 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3520 Training loss: 1.2433 0.1368 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3520 Training loss: 1.2432 0.1349 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3520 Training loss: 1.2428 0.1378 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3520 Training loss: 1.2429 0.1372 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3520 Training loss: 1.2429 0.1398 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3520 Training loss: 1.2429 0.1304 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3520 Training loss: 1.2427 0.1364 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3520 Training loss: 1.2424 0.1380 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3520 Training loss: 1.2425 0.1383 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3520 Training loss: 1.2422 0.1360 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3520 Training loss: 1.2417 0.1307 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3520 Training loss: 1.2413 0.1367 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3520 Training loss: 1.2413 0.1368 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3520 Training loss: 1.2411 0.1374 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3520 Training loss: 1.2408 0.1356 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3520 Training loss: 1.2409 0.1309 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3520 Training loss: 1.2408 0.1376 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3520 Training loss: 1.2408 0.1367 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3520 Training loss: 1.2409 0.1373 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3520 Training loss: 1.2410 0.1312 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3520 Training loss: 1.2409 0.1373 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3520 Training loss: 1.2407 0.1375 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3520 Training loss: 1.2406 0.1356 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3520 Training loss: 1.2403 0.1375 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3520 Training loss: 1.2405 0.1365 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3520 Training loss: 1.2404 0.1334 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3520 Training loss: 1.2403 0.1365 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3520 Training loss: 1.2402 0.1363 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3520 Training loss: 1.2401 0.1357 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3520 Training loss: 1.2400 0.1376 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3520 Training loss: 1.2400 0.1347 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3520 Training loss: 1.2399 0.1380 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3520 Training loss: 1.2396 0.1359 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3520 Training loss: 1.2394 0.1351 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3520 Training loss: 1.2394 0.1380 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3520 Training loss: 1.2392 0.1357 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3520 Training loss: 1.2395 0.1339 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3520 Training loss: 1.2395 0.1363 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3520 Training loss: 1.2398 0.1358 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3520 Training loss: 1.2397 0.1356 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3520 Training loss: 1.2398 0.1318 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3520 Training loss: 1.2398 0.1388 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3520 Training loss: 1.2397 0.1390 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3520 Training loss: 1.2397 0.1365 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3520 Training loss: 1.2395 0.1369 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3520 Training loss: 1.2395 0.1350 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3520 Training loss: 1.2395 0.1368 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3520 Training loss: 1.2395 0.1361 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3520 Training loss: 1.2392 0.1359 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3520 Training loss: 1.2389 0.1378 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3520 Training loss: 1.2389 0.1365 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3520 Training loss: 1.2390 0.1369 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3520 Training loss: 1.2390 0.1369 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3520 Training loss: 1.2388 0.1365 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3520 Training loss: 1.2387 0.1363 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3520 Training loss: 1.2386 0.1346 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3520 Training loss: 1.2386 0.1357 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3520 Training loss: 1.2385 0.1384 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3520 Training loss: 1.2383 0.1315 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3520 Training loss: 1.2383 0.1315 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3520 Training loss: 1.2380 0.1365 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3520 Training loss: 1.2379 0.1355 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3520 Training loss: 1.2379 0.1373 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3520 Training loss: 1.2377 0.1368 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3520 Training loss: 1.2378 0.1326 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3520 Training loss: 1.2378 0.1362 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3520 Training loss: 1.2379 0.1316 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3520 Training loss: 1.2376 0.1365 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3520 Training loss: 1.2376 0.1360 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3520 Training loss: 1.2376 0.1378 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3520 Training loss: 1.2374 0.1340 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3520 Training loss: 1.2371 0.1363 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3520 Training loss: 1.2370 0.1383 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3520 Training loss: 1.2367 0.1384 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3520 Training loss: 1.2367 0.1328 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3520 Training loss: 1.2367 0.1374 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3520 Training loss: 1.2366 0.1366 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3520 Training loss: 1.2367 0.1377 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3520 Training loss: 1.2367 0.1383 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3520 Training loss: 1.2366 0.1309 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3520 Training loss: 1.2367 0.1364 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3520 Training loss: 1.2367 0.1390 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3520 Training loss: 1.2369 0.1324 sec/batch\n",
      "Epoch 13/20  Iteration 2113/3520 Training loss: 1.3133 0.1367 sec/batch\n",
      "Epoch 13/20  Iteration 2114/3520 Training loss: 1.2648 0.1371 sec/batch\n",
      "Epoch 13/20  Iteration 2115/3520 Training loss: 1.2498 0.1310 sec/batch\n",
      "Epoch 13/20  Iteration 2116/3520 Training loss: 1.2437 0.1373 sec/batch\n",
      "Epoch 13/20  Iteration 2117/3520 Training loss: 1.2443 0.1359 sec/batch\n",
      "Epoch 13/20  Iteration 2118/3520 Training loss: 1.2429 0.1372 sec/batch\n",
      "Epoch 13/20  Iteration 2119/3520 Training loss: 1.2394 0.1375 sec/batch\n",
      "Epoch 13/20  Iteration 2120/3520 Training loss: 1.2386 0.1310 sec/batch\n",
      "Epoch 13/20  Iteration 2121/3520 Training loss: 1.2364 0.1375 sec/batch\n",
      "Epoch 13/20  Iteration 2122/3520 Training loss: 1.2356 0.1370 sec/batch\n",
      "Epoch 13/20  Iteration 2123/3520 Training loss: 1.2346 0.1371 sec/batch\n",
      "Epoch 13/20  Iteration 2124/3520 Training loss: 1.2340 0.1309 sec/batch\n",
      "Epoch 13/20  Iteration 2125/3520 Training loss: 1.2335 0.1359 sec/batch\n",
      "Epoch 13/20  Iteration 2126/3520 Training loss: 1.2344 0.1309 sec/batch\n",
      "Epoch 13/20  Iteration 2127/3520 Training loss: 1.2340 0.1356 sec/batch\n",
      "Epoch 13/20  Iteration 2128/3520 Training loss: 1.2322 0.1346 sec/batch\n",
      "Epoch 13/20  Iteration 2129/3520 Training loss: 1.2316 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2130/3520 Training loss: 1.2324 0.1359 sec/batch\n",
      "Epoch 13/20  Iteration 2131/3520 Training loss: 1.2315 0.1373 sec/batch\n",
      "Epoch 13/20  Iteration 2132/3520 Training loss: 1.2320 0.1340 sec/batch\n",
      "Epoch 13/20  Iteration 2133/3520 Training loss: 1.2321 0.1384 sec/batch\n",
      "Epoch 13/20  Iteration 2134/3520 Training loss: 1.2311 0.1354 sec/batch\n",
      "Epoch 13/20  Iteration 2135/3520 Training loss: 1.2317 0.1380 sec/batch\n",
      "Epoch 13/20  Iteration 2136/3520 Training loss: 1.2328 0.1326 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3520 Training loss: 1.2330 0.1376 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3520 Training loss: 1.2325 0.1356 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3520 Training loss: 1.2316 0.1360 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3520 Training loss: 1.2316 0.1364 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3520 Training loss: 1.2306 0.1325 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3520 Training loss: 1.2307 0.1378 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3520 Training loss: 1.2300 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3520 Training loss: 1.2299 0.1374 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3520 Training loss: 1.2308 0.1410 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3520 Training loss: 1.2304 0.1506 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3520 Training loss: 1.2303 0.1369 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3520 Training loss: 1.2306 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3520 Training loss: 1.2298 0.1351 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3520 Training loss: 1.2303 0.1386 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3520 Training loss: 1.2308 0.1363 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3520 Training loss: 1.2308 0.1357 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3520 Training loss: 1.2305 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3520 Training loss: 1.2299 0.1351 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3520 Training loss: 1.2298 0.1355 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3520 Training loss: 1.2291 0.1373 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3520 Training loss: 1.2289 0.1360 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3520 Training loss: 1.2285 0.1371 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3520 Training loss: 1.2284 0.1342 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3520 Training loss: 1.2284 0.1362 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3520 Training loss: 1.2284 0.1362 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3520 Training loss: 1.2285 0.1360 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3520 Training loss: 1.2282 0.1314 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3520 Training loss: 1.2282 0.1378 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3520 Training loss: 1.2283 0.1297 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3520 Training loss: 1.2280 0.1359 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3520 Training loss: 1.2277 0.1358 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3520 Training loss: 1.2276 0.1382 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3520 Training loss: 1.2274 0.1364 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3520 Training loss: 1.2275 0.1355 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3520 Training loss: 1.2269 0.1381 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3520 Training loss: 1.2271 0.1364 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3520 Training loss: 1.2269 0.1376 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3520 Training loss: 1.2270 0.1362 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3520 Training loss: 1.2270 0.1378 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3520 Training loss: 1.2269 0.1356 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3520 Training loss: 1.2265 0.1346 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3520 Training loss: 1.2265 0.1374 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3520 Training loss: 1.2263 0.1357 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3520 Training loss: 1.2259 0.1370 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3520 Training loss: 1.2256 0.1309 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3520 Training loss: 1.2254 0.1370 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3520 Training loss: 1.2252 0.1373 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3520 Training loss: 1.2247 0.1368 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3520 Training loss: 1.2244 0.1355 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3520 Training loss: 1.2239 0.1344 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3520 Training loss: 1.2237 0.1360 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3520 Training loss: 1.2239 0.1369 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3520 Training loss: 1.2242 0.1361 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3520 Training loss: 1.2246 0.1361 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3520 Training loss: 1.2245 0.1365 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3520 Training loss: 1.2242 0.1369 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3520 Training loss: 1.2237 0.1364 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3520 Training loss: 1.2239 0.1371 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3520 Training loss: 1.2235 0.1374 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3520 Training loss: 1.2238 0.1375 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3520 Training loss: 1.2235 0.1358 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3520 Training loss: 1.2231 0.1346 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3520 Training loss: 1.2233 0.1383 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3520 Training loss: 1.2231 0.1299 sec/batch\n",
      "Validation loss: 1.18652 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 2201/3520 Training loss: 1.2254 0.1439 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3520 Training loss: 1.2253 0.1596 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3520 Training loss: 1.2252 0.1473 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3520 Training loss: 1.2253 0.1367 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3520 Training loss: 1.2251 0.1391 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3520 Training loss: 1.2249 0.1378 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3520 Training loss: 1.2248 0.1362 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3520 Training loss: 1.2244 0.1380 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3520 Training loss: 1.2244 0.1365 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3520 Training loss: 1.2244 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3520 Training loss: 1.2243 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3520 Training loss: 1.2242 0.1359 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3520 Training loss: 1.2240 0.1381 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3520 Training loss: 1.2240 0.1379 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3520 Training loss: 1.2237 0.1362 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3520 Training loss: 1.2231 0.1304 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3520 Training loss: 1.2228 0.1320 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3520 Training loss: 1.2228 0.1362 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3520 Training loss: 1.2228 0.1379 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3520 Training loss: 1.2224 0.1356 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3520 Training loss: 1.2226 0.1373 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3520 Training loss: 1.2225 0.1376 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3520 Training loss: 1.2225 0.1377 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3520 Training loss: 1.2226 0.1381 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3520 Training loss: 1.2228 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3520 Training loss: 1.2227 0.1378 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3520 Training loss: 1.2225 0.1357 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3520 Training loss: 1.2224 0.1372 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3520 Training loss: 1.2221 0.1381 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3520 Training loss: 1.2223 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3520 Training loss: 1.2222 0.1365 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3520 Training loss: 1.2221 0.1379 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3520 Training loss: 1.2220 0.1322 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3520 Training loss: 1.2219 0.1379 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3520 Training loss: 1.2218 0.1329 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3520 Training loss: 1.2217 0.1363 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3520 Training loss: 1.2215 0.1361 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3520 Training loss: 1.2211 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3520 Training loss: 1.2209 0.1372 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3520 Training loss: 1.2209 0.1361 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3520 Training loss: 1.2207 0.1354 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3520 Training loss: 1.2209 0.1363 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3520 Training loss: 1.2209 0.1347 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3520 Training loss: 1.2211 0.1378 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3520 Training loss: 1.2211 0.1364 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3520 Training loss: 1.2212 0.1372 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3520 Training loss: 1.2212 0.1376 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3520 Training loss: 1.2211 0.1378 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3520 Training loss: 1.2211 0.1379 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3520 Training loss: 1.2210 0.1377 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3520 Training loss: 1.2210 0.1309 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3520 Training loss: 1.2211 0.1368 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3520 Training loss: 1.2210 0.1401 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3520 Training loss: 1.2208 0.1313 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3520 Training loss: 1.2206 0.1357 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3520 Training loss: 1.2205 0.1384 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3520 Training loss: 1.2207 0.1376 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3520 Training loss: 1.2207 0.1379 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3520 Training loss: 1.2205 0.1352 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3520 Training loss: 1.2205 0.1362 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3520 Training loss: 1.2205 0.1381 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3520 Training loss: 1.2205 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3520 Training loss: 1.2205 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3520 Training loss: 1.2203 0.1366 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3520 Training loss: 1.2202 0.1365 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3520 Training loss: 1.2200 0.1367 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3520 Training loss: 1.2200 0.1362 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3520 Training loss: 1.2199 0.1377 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3520 Training loss: 1.2197 0.1358 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3520 Training loss: 1.2197 0.1387 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3520 Training loss: 1.2198 0.1379 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3520 Training loss: 1.2198 0.1387 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3520 Training loss: 1.2196 0.1367 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3520 Training loss: 1.2196 0.1364 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3520 Training loss: 1.2196 0.1385 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3520 Training loss: 1.2194 0.1358 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3520 Training loss: 1.2190 0.1377 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3520 Training loss: 1.2189 0.1382 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3520 Training loss: 1.2186 0.1369 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3520 Training loss: 1.2186 0.1348 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3520 Training loss: 1.2186 0.1382 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3520 Training loss: 1.2185 0.1358 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3520 Training loss: 1.2185 0.1359 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3520 Training loss: 1.2185 0.1331 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3520 Training loss: 1.2184 0.1378 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3520 Training loss: 1.2185 0.1359 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3520 Training loss: 1.2185 0.1369 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3520 Training loss: 1.2187 0.1370 sec/batch\n",
      "Epoch 14/20  Iteration 2289/3520 Training loss: 1.2833 0.1367 sec/batch\n",
      "Epoch 14/20  Iteration 2290/3520 Training loss: 1.2377 0.1360 sec/batch\n",
      "Epoch 14/20  Iteration 2291/3520 Training loss: 1.2267 0.1374 sec/batch\n",
      "Epoch 14/20  Iteration 2292/3520 Training loss: 1.2203 0.1370 sec/batch\n",
      "Epoch 14/20  Iteration 2293/3520 Training loss: 1.2203 0.1382 sec/batch\n",
      "Epoch 14/20  Iteration 2294/3520 Training loss: 1.2181 0.1362 sec/batch\n",
      "Epoch 14/20  Iteration 2295/3520 Training loss: 1.2143 0.1320 sec/batch\n",
      "Epoch 14/20  Iteration 2296/3520 Training loss: 1.2144 0.1357 sec/batch\n",
      "Epoch 14/20  Iteration 2297/3520 Training loss: 1.2121 0.1378 sec/batch\n",
      "Epoch 14/20  Iteration 2298/3520 Training loss: 1.2119 0.1400 sec/batch\n",
      "Epoch 14/20  Iteration 2299/3520 Training loss: 1.2115 0.1358 sec/batch\n",
      "Epoch 14/20  Iteration 2300/3520 Training loss: 1.2111 0.1333 sec/batch\n",
      "Epoch 14/20  Iteration 2301/3520 Training loss: 1.2093 0.1468 sec/batch\n",
      "Epoch 14/20  Iteration 2302/3520 Training loss: 1.2107 0.1362 sec/batch\n",
      "Epoch 14/20  Iteration 2303/3520 Training loss: 1.2107 0.1372 sec/batch\n",
      "Epoch 14/20  Iteration 2304/3520 Training loss: 1.2095 0.1381 sec/batch\n",
      "Epoch 14/20  Iteration 2305/3520 Training loss: 1.2088 0.1321 sec/batch\n",
      "Epoch 14/20  Iteration 2306/3520 Training loss: 1.2093 0.1366 sec/batch\n",
      "Epoch 14/20  Iteration 2307/3520 Training loss: 1.2084 0.1365 sec/batch\n",
      "Epoch 14/20  Iteration 2308/3520 Training loss: 1.2094 0.1385 sec/batch\n",
      "Epoch 14/20  Iteration 2309/3520 Training loss: 1.2095 0.1379 sec/batch\n",
      "Epoch 14/20  Iteration 2310/3520 Training loss: 1.2090 0.1374 sec/batch\n",
      "Epoch 14/20  Iteration 2311/3520 Training loss: 1.2095 0.1385 sec/batch\n",
      "Epoch 14/20  Iteration 2312/3520 Training loss: 1.2103 0.1360 sec/batch\n",
      "Epoch 14/20  Iteration 2313/3520 Training loss: 1.2105 0.1368 sec/batch\n",
      "Epoch 14/20  Iteration 2314/3520 Training loss: 1.2101 0.1384 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3520 Training loss: 1.2094 0.1378 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3520 Training loss: 1.2092 0.1366 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3520 Training loss: 1.2083 0.1335 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3520 Training loss: 1.2086 0.1368 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3520 Training loss: 1.2080 0.1374 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3520 Training loss: 1.2078 0.1377 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3520 Training loss: 1.2088 0.1357 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3520 Training loss: 1.2085 0.1371 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3520 Training loss: 1.2087 0.1372 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3520 Training loss: 1.2089 0.1357 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3520 Training loss: 1.2078 0.1279 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3520 Training loss: 1.2084 0.1336 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3520 Training loss: 1.2088 0.1359 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3520 Training loss: 1.2086 0.1363 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3520 Training loss: 1.2083 0.1326 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3520 Training loss: 1.2078 0.1337 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3520 Training loss: 1.2078 0.1376 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3520 Training loss: 1.2071 0.1327 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3520 Training loss: 1.2068 0.1362 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3520 Training loss: 1.2065 0.1368 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3520 Training loss: 1.2065 0.1385 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3520 Training loss: 1.2067 0.1364 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3520 Training loss: 1.2066 0.1316 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3520 Training loss: 1.2067 0.1362 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3520 Training loss: 1.2065 0.1362 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3520 Training loss: 1.2067 0.1397 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3520 Training loss: 1.2068 0.1360 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3520 Training loss: 1.2067 0.1380 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3520 Training loss: 1.2065 0.1358 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3520 Training loss: 1.2065 0.1366 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3520 Training loss: 1.2064 0.1366 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3520 Training loss: 1.2065 0.1351 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3520 Training loss: 1.2061 0.1372 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3520 Training loss: 1.2065 0.1354 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3520 Training loss: 1.2064 0.1360 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3520 Training loss: 1.2063 0.1361 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3520 Training loss: 1.2065 0.1382 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3520 Training loss: 1.2064 0.1360 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3520 Training loss: 1.2060 0.1371 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3520 Training loss: 1.2062 0.1362 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3520 Training loss: 1.2060 0.1372 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3520 Training loss: 1.2053 0.1379 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3520 Training loss: 1.2051 0.1359 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3520 Training loss: 1.2051 0.1369 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3520 Training loss: 1.2049 0.1352 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3520 Training loss: 1.2045 0.1365 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3520 Training loss: 1.2043 0.1355 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3520 Training loss: 1.2039 0.1351 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3520 Training loss: 1.2036 0.1370 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3520 Training loss: 1.2038 0.1381 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3520 Training loss: 1.2042 0.1381 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3520 Training loss: 1.2047 0.1368 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3520 Training loss: 1.2047 0.1381 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3520 Training loss: 1.2046 0.1369 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3520 Training loss: 1.2043 0.1359 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3520 Training loss: 1.2045 0.1342 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3520 Training loss: 1.2043 0.1358 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3520 Training loss: 1.2045 0.1368 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3520 Training loss: 1.2044 0.1385 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3520 Training loss: 1.2040 0.1385 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3520 Training loss: 1.2042 0.1347 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3520 Training loss: 1.2040 0.1374 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3520 Training loss: 1.2042 0.1367 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3520 Training loss: 1.2040 0.1376 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3520 Training loss: 1.2039 0.1366 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3520 Training loss: 1.2041 0.1369 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3520 Training loss: 1.2038 0.1383 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3520 Training loss: 1.2035 0.1381 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3520 Training loss: 1.2035 0.1352 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3520 Training loss: 1.2031 0.1368 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3520 Training loss: 1.2032 0.1377 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3520 Training loss: 1.2031 0.1356 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3520 Training loss: 1.2031 0.1371 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3520 Training loss: 1.2029 0.1361 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3520 Training loss: 1.2028 0.1402 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3520 Training loss: 1.2029 0.1369 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3520 Training loss: 1.2026 0.1374 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3520 Training loss: 1.2021 0.1376 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3520 Training loss: 1.2016 0.1366 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3520 Training loss: 1.2017 0.1370 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3520 Training loss: 1.2016 0.1317 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3520 Training loss: 1.2013 0.1369 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3520 Training loss: 1.2014 0.1368 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3520 Training loss: 1.2014 0.1361 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3520 Training loss: 1.2015 0.1329 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3520 Training loss: 1.2015 0.1357 sec/batch\n",
      "Validation loss: 1.17683 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 2401/3520 Training loss: 1.2033 0.1373 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3520 Training loss: 1.2033 0.1292 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3520 Training loss: 1.2032 0.1369 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3520 Training loss: 1.2031 0.1344 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3520 Training loss: 1.2030 0.1368 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3520 Training loss: 1.2031 0.1395 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3520 Training loss: 1.2031 0.1341 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3520 Training loss: 1.2031 0.1295 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3520 Training loss: 1.2030 0.1374 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3520 Training loss: 1.2030 0.1312 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3520 Training loss: 1.2030 0.1374 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3520 Training loss: 1.2030 0.1376 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3520 Training loss: 1.2029 0.1359 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3520 Training loss: 1.2027 0.1360 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3520 Training loss: 1.2026 0.1348 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3520 Training loss: 1.2027 0.1395 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3520 Training loss: 1.2026 0.1372 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3520 Training loss: 1.2028 0.1370 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3520 Training loss: 1.2028 0.1379 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3520 Training loss: 1.2031 0.1365 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3520 Training loss: 1.2031 0.1373 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3520 Training loss: 1.2032 0.1337 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3520 Training loss: 1.2033 0.1358 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3520 Training loss: 1.2032 0.1321 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3520 Training loss: 1.2033 0.1383 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3520 Training loss: 1.2031 0.1375 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3520 Training loss: 1.2032 0.1362 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3520 Training loss: 1.2033 0.1369 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3520 Training loss: 1.2033 0.1374 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3520 Training loss: 1.2032 0.1368 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3520 Training loss: 1.2029 0.1297 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3520 Training loss: 1.2029 0.1365 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3520 Training loss: 1.2030 0.1293 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3520 Training loss: 1.2030 0.1358 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3520 Training loss: 1.2029 0.1364 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3520 Training loss: 1.2028 0.1374 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3520 Training loss: 1.2028 0.1295 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3520 Training loss: 1.2028 0.1364 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3520 Training loss: 1.2027 0.1371 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3520 Training loss: 1.2025 0.1339 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3520 Training loss: 1.2024 0.1341 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3520 Training loss: 1.2022 0.1366 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3520 Training loss: 1.2021 0.1398 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3520 Training loss: 1.2021 0.1371 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3520 Training loss: 1.2019 0.1367 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3520 Training loss: 1.2020 0.1369 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3520 Training loss: 1.2020 0.1363 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3520 Training loss: 1.2021 0.1366 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3520 Training loss: 1.2019 0.1358 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3520 Training loss: 1.2019 0.1389 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3520 Training loss: 1.2019 0.1381 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3520 Training loss: 1.2017 0.1358 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3520 Training loss: 1.2014 0.1368 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3520 Training loss: 1.2013 0.1376 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3520 Training loss: 1.2011 0.1382 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3520 Training loss: 1.2010 0.1378 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3520 Training loss: 1.2010 0.1380 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3520 Training loss: 1.2010 0.1324 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3520 Training loss: 1.2010 0.1397 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3520 Training loss: 1.2010 0.1395 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3520 Training loss: 1.2010 0.1361 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3520 Training loss: 1.2010 0.1378 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3520 Training loss: 1.2010 0.1373 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3520 Training loss: 1.2012 0.1368 sec/batch\n",
      "Epoch 15/20  Iteration 2465/3520 Training loss: 1.2732 0.1371 sec/batch\n",
      "Epoch 15/20  Iteration 2466/3520 Training loss: 1.2189 0.1371 sec/batch\n",
      "Epoch 15/20  Iteration 2467/3520 Training loss: 1.2082 0.1323 sec/batch\n",
      "Epoch 15/20  Iteration 2468/3520 Training loss: 1.2037 0.1376 sec/batch\n",
      "Epoch 15/20  Iteration 2469/3520 Training loss: 1.2013 0.1355 sec/batch\n",
      "Epoch 15/20  Iteration 2470/3520 Training loss: 1.2006 0.1374 sec/batch\n",
      "Epoch 15/20  Iteration 2471/3520 Training loss: 1.1971 0.1366 sec/batch\n",
      "Epoch 15/20  Iteration 2472/3520 Training loss: 1.1973 0.1365 sec/batch\n",
      "Epoch 15/20  Iteration 2473/3520 Training loss: 1.1941 0.1335 sec/batch\n",
      "Epoch 15/20  Iteration 2474/3520 Training loss: 1.1930 0.1369 sec/batch\n",
      "Epoch 15/20  Iteration 2475/3520 Training loss: 1.1934 0.1362 sec/batch\n",
      "Epoch 15/20  Iteration 2476/3520 Training loss: 1.1939 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2477/3520 Training loss: 1.1925 0.1444 sec/batch\n",
      "Epoch 15/20  Iteration 2478/3520 Training loss: 1.1934 0.1361 sec/batch\n",
      "Epoch 15/20  Iteration 2479/3520 Training loss: 1.1933 0.1376 sec/batch\n",
      "Epoch 15/20  Iteration 2480/3520 Training loss: 1.1918 0.1372 sec/batch\n",
      "Epoch 15/20  Iteration 2481/3520 Training loss: 1.1913 0.1391 sec/batch\n",
      "Epoch 15/20  Iteration 2482/3520 Training loss: 1.1918 0.1379 sec/batch\n",
      "Epoch 15/20  Iteration 2483/3520 Training loss: 1.1908 0.1381 sec/batch\n",
      "Epoch 15/20  Iteration 2484/3520 Training loss: 1.1913 0.1358 sec/batch\n",
      "Epoch 15/20  Iteration 2485/3520 Training loss: 1.1914 0.1378 sec/batch\n",
      "Epoch 15/20  Iteration 2486/3520 Training loss: 1.1908 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2487/3520 Training loss: 1.1913 0.1363 sec/batch\n",
      "Epoch 15/20  Iteration 2488/3520 Training loss: 1.1928 0.1301 sec/batch\n",
      "Epoch 15/20  Iteration 2489/3520 Training loss: 1.1931 0.1309 sec/batch\n",
      "Epoch 15/20  Iteration 2490/3520 Training loss: 1.1928 0.1381 sec/batch\n",
      "Epoch 15/20  Iteration 2491/3520 Training loss: 1.1920 0.1371 sec/batch\n",
      "Epoch 15/20  Iteration 2492/3520 Training loss: 1.1920 0.1369 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3520 Training loss: 1.1909 0.1383 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3520 Training loss: 1.1914 0.1336 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3520 Training loss: 1.1908 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3520 Training loss: 1.1905 0.1369 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3520 Training loss: 1.1916 0.1370 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3520 Training loss: 1.1916 0.1382 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3520 Training loss: 1.1917 0.1387 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3520 Training loss: 1.1921 0.1363 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3520 Training loss: 1.1913 0.1331 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3520 Training loss: 1.1916 0.1321 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3520 Training loss: 1.1922 0.1307 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3520 Training loss: 1.1920 0.1377 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3520 Training loss: 1.1918 0.1348 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3520 Training loss: 1.1911 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3520 Training loss: 1.1912 0.1330 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3520 Training loss: 1.1907 0.1372 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3520 Training loss: 1.1903 0.1366 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3520 Training loss: 1.1901 0.1391 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3520 Training loss: 1.1902 0.1380 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3520 Training loss: 1.1902 0.1360 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3520 Training loss: 1.1903 0.1365 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3520 Training loss: 1.1905 0.1364 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3520 Training loss: 1.1903 0.1325 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3520 Training loss: 1.1904 0.1368 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3520 Training loss: 1.1906 0.1306 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3520 Training loss: 1.1904 0.1339 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3520 Training loss: 1.1903 0.1382 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3520 Training loss: 1.1904 0.1370 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3520 Training loss: 1.1903 0.1396 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3520 Training loss: 1.1904 0.1370 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3520 Training loss: 1.1899 0.1363 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3520 Training loss: 1.1902 0.1382 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3520 Training loss: 1.1901 0.1318 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3520 Training loss: 1.1902 0.1378 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3520 Training loss: 1.1902 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3520 Training loss: 1.1901 0.1369 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3520 Training loss: 1.1897 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3520 Training loss: 1.1900 0.1326 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3520 Training loss: 1.1899 0.1316 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3520 Training loss: 1.1894 0.1379 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3520 Training loss: 1.1893 0.1352 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3520 Training loss: 1.1894 0.1377 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3520 Training loss: 1.1895 0.1354 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3520 Training loss: 1.1892 0.1310 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3520 Training loss: 1.1888 0.1372 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3520 Training loss: 1.1882 0.1362 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3520 Training loss: 1.1882 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3520 Training loss: 1.1883 0.1366 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3520 Training loss: 1.1886 0.1397 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3520 Training loss: 1.1890 0.1387 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3520 Training loss: 1.1890 0.1390 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3520 Training loss: 1.1888 0.1356 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3520 Training loss: 1.1885 0.1356 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3520 Training loss: 1.1888 0.1365 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3520 Training loss: 1.1884 0.1377 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3520 Training loss: 1.1887 0.1355 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3520 Training loss: 1.1886 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3520 Training loss: 1.1883 0.1347 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3520 Training loss: 1.1886 0.1306 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3520 Training loss: 1.1885 0.1324 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3520 Training loss: 1.1887 0.1378 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3520 Training loss: 1.1884 0.1358 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3520 Training loss: 1.1882 0.1378 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3520 Training loss: 1.1884 0.1368 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3520 Training loss: 1.1881 0.1369 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3520 Training loss: 1.1879 0.1362 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3520 Training loss: 1.1878 0.1375 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3520 Training loss: 1.1875 0.1361 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3520 Training loss: 1.1876 0.1346 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3520 Training loss: 1.1874 0.1361 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3520 Training loss: 1.1875 0.1377 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3520 Training loss: 1.1873 0.1369 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3520 Training loss: 1.1872 0.1393 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3520 Training loss: 1.1872 0.1357 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3520 Training loss: 1.1868 0.1340 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3520 Training loss: 1.1864 0.1370 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3520 Training loss: 1.1860 0.1431 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3520 Training loss: 1.1860 0.1361 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3520 Training loss: 1.1859 0.1381 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3520 Training loss: 1.1857 0.1370 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3520 Training loss: 1.1858 0.1365 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3520 Training loss: 1.1858 0.1361 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3520 Training loss: 1.1859 0.1377 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3520 Training loss: 1.1859 0.1365 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3520 Training loss: 1.1860 0.1375 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3520 Training loss: 1.1860 0.1307 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3520 Training loss: 1.1858 0.1372 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3520 Training loss: 1.1857 0.1378 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3520 Training loss: 1.1856 0.1365 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3520 Training loss: 1.1857 0.1340 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3520 Training loss: 1.1857 0.1361 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3520 Training loss: 1.1857 0.1375 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3520 Training loss: 1.1857 0.1370 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3520 Training loss: 1.1856 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3520 Training loss: 1.1855 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3520 Training loss: 1.1854 0.1365 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3520 Training loss: 1.1853 0.1326 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3520 Training loss: 1.1850 0.1371 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3520 Training loss: 1.1848 0.1373 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3520 Training loss: 1.1848 0.1382 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3520 Training loss: 1.1848 0.1365 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3520 Training loss: 1.1850 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3520 Training loss: 1.1850 0.1358 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3520 Training loss: 1.1853 0.1366 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3520 Training loss: 1.1852 0.1378 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3520 Training loss: 1.1854 0.1363 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3520 Training loss: 1.1856 0.1329 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3520 Training loss: 1.1856 0.1372 sec/batch\n",
      "Validation loss: 1.17528 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 2601/3520 Training loss: 1.1870 0.1517 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3520 Training loss: 1.1870 0.1389 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3520 Training loss: 1.1872 0.1324 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3520 Training loss: 1.1872 0.1372 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3520 Training loss: 1.1873 0.1354 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3520 Training loss: 1.1872 0.1366 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3520 Training loss: 1.1870 0.1359 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3520 Training loss: 1.1870 0.1394 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3520 Training loss: 1.1871 0.1377 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3520 Training loss: 1.1872 0.1379 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3520 Training loss: 1.1871 0.1418 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3520 Training loss: 1.1871 0.1366 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3520 Training loss: 1.1871 0.1378 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3520 Training loss: 1.1872 0.1381 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3520 Training loss: 1.1871 0.1376 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3520 Training loss: 1.1870 0.1363 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3520 Training loss: 1.1870 0.1365 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3520 Training loss: 1.1868 0.1385 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3520 Training loss: 1.1868 0.1388 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3520 Training loss: 1.1867 0.1365 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3520 Training loss: 1.1865 0.1366 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3520 Training loss: 1.1866 0.1393 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3520 Training loss: 1.1866 0.1363 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3520 Training loss: 1.1867 0.1362 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3520 Training loss: 1.1865 0.1365 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3520 Training loss: 1.1866 0.1360 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3520 Training loss: 1.1866 0.1374 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3520 Training loss: 1.1864 0.1326 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3520 Training loss: 1.1861 0.1367 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3520 Training loss: 1.1860 0.1374 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3520 Training loss: 1.1858 0.1360 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3520 Training loss: 1.1858 0.1371 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3520 Training loss: 1.1858 0.1353 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3520 Training loss: 1.1857 0.1359 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3520 Training loss: 1.1858 0.1363 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3520 Training loss: 1.1858 0.1358 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3520 Training loss: 1.1857 0.1320 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3520 Training loss: 1.1857 0.1366 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3520 Training loss: 1.1857 0.1363 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3520 Training loss: 1.1859 0.1373 sec/batch\n",
      "Epoch 16/20  Iteration 2641/3520 Training loss: 1.2575 0.1359 sec/batch\n",
      "Epoch 16/20  Iteration 2642/3520 Training loss: 1.2060 0.1356 sec/batch\n",
      "Epoch 16/20  Iteration 2643/3520 Training loss: 1.1950 0.1340 sec/batch\n",
      "Epoch 16/20  Iteration 2644/3520 Training loss: 1.1903 0.1375 sec/batch\n",
      "Epoch 16/20  Iteration 2645/3520 Training loss: 1.1883 0.1353 sec/batch\n",
      "Epoch 16/20  Iteration 2646/3520 Training loss: 1.1848 0.1370 sec/batch\n",
      "Epoch 16/20  Iteration 2647/3520 Training loss: 1.1821 0.1355 sec/batch\n",
      "Epoch 16/20  Iteration 2648/3520 Training loss: 1.1827 0.1363 sec/batch\n",
      "Epoch 16/20  Iteration 2649/3520 Training loss: 1.1802 0.1390 sec/batch\n",
      "Epoch 16/20  Iteration 2650/3520 Training loss: 1.1791 0.1355 sec/batch\n",
      "Epoch 16/20  Iteration 2651/3520 Training loss: 1.1801 0.1316 sec/batch\n",
      "Epoch 16/20  Iteration 2652/3520 Training loss: 1.1803 0.1360 sec/batch\n",
      "Epoch 16/20  Iteration 2653/3520 Training loss: 1.1792 0.1369 sec/batch\n",
      "Epoch 16/20  Iteration 2654/3520 Training loss: 1.1808 0.1367 sec/batch\n",
      "Epoch 16/20  Iteration 2655/3520 Training loss: 1.1806 0.1361 sec/batch\n",
      "Epoch 16/20  Iteration 2656/3520 Training loss: 1.1787 0.1329 sec/batch\n",
      "Epoch 16/20  Iteration 2657/3520 Training loss: 1.1786 0.1381 sec/batch\n",
      "Epoch 16/20  Iteration 2658/3520 Training loss: 1.1789 0.1365 sec/batch\n",
      "Epoch 16/20  Iteration 2659/3520 Training loss: 1.1782 0.1357 sec/batch\n",
      "Epoch 16/20  Iteration 2660/3520 Training loss: 1.1787 0.1354 sec/batch\n",
      "Epoch 16/20  Iteration 2661/3520 Training loss: 1.1790 0.1382 sec/batch\n",
      "Epoch 16/20  Iteration 2662/3520 Training loss: 1.1784 0.1380 sec/batch\n",
      "Epoch 16/20  Iteration 2663/3520 Training loss: 1.1780 0.1369 sec/batch\n",
      "Epoch 16/20  Iteration 2664/3520 Training loss: 1.1793 0.1355 sec/batch\n",
      "Epoch 16/20  Iteration 2665/3520 Training loss: 1.1795 0.1374 sec/batch\n",
      "Epoch 16/20  Iteration 2666/3520 Training loss: 1.1796 0.1315 sec/batch\n",
      "Epoch 16/20  Iteration 2667/3520 Training loss: 1.1786 0.1354 sec/batch\n",
      "Epoch 16/20  Iteration 2668/3520 Training loss: 1.1786 0.1362 sec/batch\n",
      "Epoch 16/20  Iteration 2669/3520 Training loss: 1.1777 0.1369 sec/batch\n",
      "Epoch 16/20  Iteration 2670/3520 Training loss: 1.1782 0.1354 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3520 Training loss: 1.1776 0.1307 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3520 Training loss: 1.1776 0.1379 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3520 Training loss: 1.1783 0.1335 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3520 Training loss: 1.1780 0.1310 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3520 Training loss: 1.1778 0.1352 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3520 Training loss: 1.1782 0.1354 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3520 Training loss: 1.1774 0.1366 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3520 Training loss: 1.1777 0.1364 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3520 Training loss: 1.1784 0.1343 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3520 Training loss: 1.1785 0.1382 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3520 Training loss: 1.1783 0.1314 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3520 Training loss: 1.1778 0.1356 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3520 Training loss: 1.1778 0.1357 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3520 Training loss: 1.1775 0.1303 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3520 Training loss: 1.1771 0.1387 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3520 Training loss: 1.1768 0.1354 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3520 Training loss: 1.1769 0.1321 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3520 Training loss: 1.1770 0.1365 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3520 Training loss: 1.1771 0.1384 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3520 Training loss: 1.1772 0.1366 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3520 Training loss: 1.1770 0.1384 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3520 Training loss: 1.1772 0.1385 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3520 Training loss: 1.1773 0.1373 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3520 Training loss: 1.1772 0.1379 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3520 Training loss: 1.1771 0.1360 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3520 Training loss: 1.1770 0.1378 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3520 Training loss: 1.1769 0.1380 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3520 Training loss: 1.1770 0.1359 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3520 Training loss: 1.1766 0.1357 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3520 Training loss: 1.1768 0.1362 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3520 Training loss: 1.1768 0.1362 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3520 Training loss: 1.1769 0.1310 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3520 Training loss: 1.1769 0.1367 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3520 Training loss: 1.1768 0.1359 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3520 Training loss: 1.1763 0.1373 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3520 Training loss: 1.1764 0.1359 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3520 Training loss: 1.1764 0.1356 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3520 Training loss: 1.1759 0.1379 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3520 Training loss: 1.1756 0.1378 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3520 Training loss: 1.1756 0.1372 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3520 Training loss: 1.1755 0.1359 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3520 Training loss: 1.1751 0.1356 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3520 Training loss: 1.1749 0.1367 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3520 Training loss: 1.1745 0.1358 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3520 Training loss: 1.1745 0.1365 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3520 Training loss: 1.1745 0.1366 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3520 Training loss: 1.1748 0.1384 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3520 Training loss: 1.1752 0.1363 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3520 Training loss: 1.1751 0.1370 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3520 Training loss: 1.1749 0.1386 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3520 Training loss: 1.1747 0.1388 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3520 Training loss: 1.1749 0.1367 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3520 Training loss: 1.1747 0.1369 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3520 Training loss: 1.1750 0.1359 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3520 Training loss: 1.1748 0.1353 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3520 Training loss: 1.1744 0.1363 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3520 Training loss: 1.1745 0.1337 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3520 Training loss: 1.1745 0.1389 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3520 Training loss: 1.1746 0.1371 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3520 Training loss: 1.1744 0.1362 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3520 Training loss: 1.1743 0.1362 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3520 Training loss: 1.1744 0.1439 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3520 Training loss: 1.1741 0.1358 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3520 Training loss: 1.1739 0.1374 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3520 Training loss: 1.1738 0.1345 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3520 Training loss: 1.1735 0.1368 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3520 Training loss: 1.1737 0.1386 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3520 Training loss: 1.1736 0.1385 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3520 Training loss: 1.1736 0.1335 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3520 Training loss: 1.1734 0.1353 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3520 Training loss: 1.1732 0.1385 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3520 Training loss: 1.1732 0.1366 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3520 Training loss: 1.1729 0.1367 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3520 Training loss: 1.1724 0.1379 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3520 Training loss: 1.1720 0.1371 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3520 Training loss: 1.1721 0.1384 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3520 Training loss: 1.1719 0.1341 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3520 Training loss: 1.1716 0.1397 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3520 Training loss: 1.1717 0.1365 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3520 Training loss: 1.1717 0.1364 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3520 Training loss: 1.1717 0.1364 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3520 Training loss: 1.1717 0.1335 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3520 Training loss: 1.1719 0.1366 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3520 Training loss: 1.1719 0.1383 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3520 Training loss: 1.1717 0.1357 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3520 Training loss: 1.1717 0.1377 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3520 Training loss: 1.1715 0.1371 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3520 Training loss: 1.1717 0.1376 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3520 Training loss: 1.1716 0.1367 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3520 Training loss: 1.1716 0.1365 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3520 Training loss: 1.1716 0.1378 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3520 Training loss: 1.1716 0.1373 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3520 Training loss: 1.1716 0.1312 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3520 Training loss: 1.1715 0.1366 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3520 Training loss: 1.1714 0.1375 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3520 Training loss: 1.1711 0.1380 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3520 Training loss: 1.1710 0.1356 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3520 Training loss: 1.1710 0.1371 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3520 Training loss: 1.1710 0.1368 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3520 Training loss: 1.1712 0.1375 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3520 Training loss: 1.1713 0.1302 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3520 Training loss: 1.1715 0.1365 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3520 Training loss: 1.1715 0.1392 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3520 Training loss: 1.1717 0.1356 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3520 Training loss: 1.1717 0.1346 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3520 Training loss: 1.1717 0.1366 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3520 Training loss: 1.1718 0.1367 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3520 Training loss: 1.1716 0.1360 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3520 Training loss: 1.1718 0.1363 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3520 Training loss: 1.1719 0.1492 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3520 Training loss: 1.1719 0.1314 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3520 Training loss: 1.1718 0.1365 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3520 Training loss: 1.1715 0.1371 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3520 Training loss: 1.1715 0.1367 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3520 Training loss: 1.1716 0.1352 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3520 Training loss: 1.1717 0.1379 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3520 Training loss: 1.1716 0.1376 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3520 Training loss: 1.1715 0.1376 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3520 Training loss: 1.1716 0.1368 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3520 Training loss: 1.1718 0.1356 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3520 Training loss: 1.1717 0.1365 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3520 Training loss: 1.1715 0.1392 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3520 Training loss: 1.1715 0.1382 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3520 Training loss: 1.1713 0.1375 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3520 Training loss: 1.1713 0.1371 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3520 Training loss: 1.1713 0.1382 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3520 Training loss: 1.1711 0.1323 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3520 Training loss: 1.1713 0.1310 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3520 Training loss: 1.1713 0.1353 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3520 Training loss: 1.1714 0.1359 sec/batch\n",
      "Validation loss: 1.16098 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 2801/3520 Training loss: 1.1724 0.1388 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3520 Training loss: 1.1725 0.1310 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3520 Training loss: 1.1726 0.1368 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3520 Training loss: 1.1724 0.1358 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3520 Training loss: 1.1721 0.1378 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3520 Training loss: 1.1719 0.1373 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3520 Training loss: 1.1717 0.1358 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3520 Training loss: 1.1716 0.1364 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3520 Training loss: 1.1717 0.1331 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3520 Training loss: 1.1717 0.1359 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3520 Training loss: 1.1717 0.1364 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3520 Training loss: 1.1718 0.1383 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3520 Training loss: 1.1717 0.1375 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3520 Training loss: 1.1717 0.1385 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3520 Training loss: 1.1718 0.1364 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3520 Training loss: 1.1720 0.1365 sec/batch\n",
      "Epoch 17/20  Iteration 2817/3520 Training loss: 1.2244 0.1365 sec/batch\n",
      "Epoch 17/20  Iteration 2818/3520 Training loss: 1.1807 0.1373 sec/batch\n",
      "Epoch 17/20  Iteration 2819/3520 Training loss: 1.1735 0.1360 sec/batch\n",
      "Epoch 17/20  Iteration 2820/3520 Training loss: 1.1700 0.1359 sec/batch\n",
      "Epoch 17/20  Iteration 2821/3520 Training loss: 1.1696 0.1382 sec/batch\n",
      "Epoch 17/20  Iteration 2822/3520 Training loss: 1.1682 0.1372 sec/batch\n",
      "Epoch 17/20  Iteration 2823/3520 Training loss: 1.1661 0.1371 sec/batch\n",
      "Epoch 17/20  Iteration 2824/3520 Training loss: 1.1664 0.1382 sec/batch\n",
      "Epoch 17/20  Iteration 2825/3520 Training loss: 1.1644 0.1367 sec/batch\n",
      "Epoch 17/20  Iteration 2826/3520 Training loss: 1.1645 0.1364 sec/batch\n",
      "Epoch 17/20  Iteration 2827/3520 Training loss: 1.1650 0.1361 sec/batch\n",
      "Epoch 17/20  Iteration 2828/3520 Training loss: 1.1653 0.1328 sec/batch\n",
      "Epoch 17/20  Iteration 2829/3520 Training loss: 1.1638 0.1363 sec/batch\n",
      "Epoch 17/20  Iteration 2830/3520 Training loss: 1.1652 0.1368 sec/batch\n",
      "Epoch 17/20  Iteration 2831/3520 Training loss: 1.1654 0.1379 sec/batch\n",
      "Epoch 17/20  Iteration 2832/3520 Training loss: 1.1640 0.1342 sec/batch\n",
      "Epoch 17/20  Iteration 2833/3520 Training loss: 1.1631 0.1379 sec/batch\n",
      "Epoch 17/20  Iteration 2834/3520 Training loss: 1.1638 0.1359 sec/batch\n",
      "Epoch 17/20  Iteration 2835/3520 Training loss: 1.1631 0.1354 sec/batch\n",
      "Epoch 17/20  Iteration 2836/3520 Training loss: 1.1635 0.1368 sec/batch\n",
      "Epoch 17/20  Iteration 2837/3520 Training loss: 1.1639 0.1355 sec/batch\n",
      "Epoch 17/20  Iteration 2838/3520 Training loss: 1.1634 0.1340 sec/batch\n",
      "Epoch 17/20  Iteration 2839/3520 Training loss: 1.1632 0.1376 sec/batch\n",
      "Epoch 17/20  Iteration 2840/3520 Training loss: 1.1646 0.1307 sec/batch\n",
      "Epoch 17/20  Iteration 2841/3520 Training loss: 1.1645 0.1381 sec/batch\n",
      "Epoch 17/20  Iteration 2842/3520 Training loss: 1.1643 0.1370 sec/batch\n",
      "Epoch 17/20  Iteration 2843/3520 Training loss: 1.1638 0.1362 sec/batch\n",
      "Epoch 17/20  Iteration 2844/3520 Training loss: 1.1642 0.1370 sec/batch\n",
      "Epoch 17/20  Iteration 2845/3520 Training loss: 1.1635 0.1363 sec/batch\n",
      "Epoch 17/20  Iteration 2846/3520 Training loss: 1.1639 0.1377 sec/batch\n",
      "Epoch 17/20  Iteration 2847/3520 Training loss: 1.1632 0.1361 sec/batch\n",
      "Epoch 17/20  Iteration 2848/3520 Training loss: 1.1630 0.1358 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3520 Training loss: 1.1636 0.1382 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3520 Training loss: 1.1631 0.1368 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3520 Training loss: 1.1632 0.1374 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3520 Training loss: 1.1637 0.1375 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3520 Training loss: 1.1629 0.1340 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3520 Training loss: 1.1633 0.1326 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3520 Training loss: 1.1639 0.1378 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3520 Training loss: 1.1641 0.1366 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3520 Training loss: 1.1637 0.1370 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3520 Training loss: 1.1630 0.1347 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3520 Training loss: 1.1632 0.1363 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3520 Training loss: 1.1626 0.1379 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3520 Training loss: 1.1625 0.1359 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3520 Training loss: 1.1623 0.1361 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3520 Training loss: 1.1623 0.1367 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3520 Training loss: 1.1625 0.1360 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3520 Training loss: 1.1625 0.1361 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3520 Training loss: 1.1628 0.1368 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3520 Training loss: 1.1626 0.1315 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3520 Training loss: 1.1627 0.1362 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3520 Training loss: 1.1628 0.1365 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3520 Training loss: 1.1627 0.1388 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3520 Training loss: 1.1625 0.1411 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3520 Training loss: 1.1624 0.1362 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3520 Training loss: 1.1624 0.1365 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3520 Training loss: 1.1626 0.1384 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3520 Training loss: 1.1620 0.1360 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3520 Training loss: 1.1623 0.1360 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3520 Training loss: 1.1622 0.1379 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3520 Training loss: 1.1623 0.1367 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3520 Training loss: 1.1623 0.1327 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3520 Training loss: 1.1623 0.1381 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3520 Training loss: 1.1620 0.1380 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3520 Training loss: 1.1621 0.1377 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3520 Training loss: 1.1621 0.1285 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3520 Training loss: 1.1616 0.1367 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3520 Training loss: 1.1614 0.1350 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3520 Training loss: 1.1614 0.1364 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3520 Training loss: 1.1615 0.1363 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3520 Training loss: 1.1611 0.1364 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3520 Training loss: 1.1607 0.1381 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3520 Training loss: 1.1603 0.1365 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3520 Training loss: 1.1601 0.1375 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3520 Training loss: 1.1602 0.1353 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3520 Training loss: 1.1606 0.1378 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3520 Training loss: 1.1611 0.1361 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3520 Training loss: 1.1610 0.1373 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3520 Training loss: 1.1609 0.1356 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3520 Training loss: 1.1606 0.1370 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3520 Training loss: 1.1608 0.1388 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3520 Training loss: 1.1608 0.1426 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3520 Training loss: 1.1611 0.1380 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3520 Training loss: 1.1611 0.1394 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3520 Training loss: 1.1607 0.1377 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3520 Training loss: 1.1609 0.1358 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3520 Training loss: 1.1607 0.1364 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3520 Training loss: 1.1608 0.1361 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3520 Training loss: 1.1606 0.1385 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3520 Training loss: 1.1604 0.1375 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3520 Training loss: 1.1607 0.1371 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3520 Training loss: 1.1604 0.1368 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3520 Training loss: 1.1602 0.1367 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3520 Training loss: 1.1602 0.1346 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3520 Training loss: 1.1600 0.1366 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3520 Training loss: 1.1601 0.1355 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3520 Training loss: 1.1600 0.1357 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3520 Training loss: 1.1600 0.1365 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3520 Training loss: 1.1600 0.1385 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3520 Training loss: 1.1598 0.1375 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3520 Training loss: 1.1598 0.1344 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3520 Training loss: 1.1596 0.1368 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3520 Training loss: 1.1591 0.1445 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3520 Training loss: 1.1587 0.1382 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3520 Training loss: 1.1588 0.1365 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3520 Training loss: 1.1587 0.1381 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3520 Training loss: 1.1585 0.1359 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3520 Training loss: 1.1586 0.1310 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3520 Training loss: 1.1586 0.1378 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3520 Training loss: 1.1587 0.1367 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3520 Training loss: 1.1588 0.1393 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3520 Training loss: 1.1589 0.1295 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3520 Training loss: 1.1589 0.1349 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3520 Training loss: 1.1588 0.1372 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3520 Training loss: 1.1588 0.1347 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3520 Training loss: 1.1587 0.1377 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3520 Training loss: 1.1589 0.1365 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3520 Training loss: 1.1589 0.1357 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3520 Training loss: 1.1589 0.1367 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3520 Training loss: 1.1588 0.1357 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3520 Training loss: 1.1588 0.1386 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3520 Training loss: 1.1587 0.1369 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3520 Training loss: 1.1587 0.1371 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3520 Training loss: 1.1586 0.1359 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3520 Training loss: 1.1583 0.1367 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3520 Training loss: 1.1582 0.1372 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3520 Training loss: 1.1583 0.1350 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3520 Training loss: 1.1582 0.1366 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3520 Training loss: 1.1584 0.1351 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3520 Training loss: 1.1584 0.1366 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3520 Training loss: 1.1587 0.1369 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3520 Training loss: 1.1586 0.1365 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3520 Training loss: 1.1589 0.1357 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3520 Training loss: 1.1589 0.1306 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3520 Training loss: 1.1588 0.1380 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3520 Training loss: 1.1589 0.1360 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3520 Training loss: 1.1588 0.1313 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3520 Training loss: 1.1590 0.1413 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3520 Training loss: 1.1591 0.1372 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3520 Training loss: 1.1591 0.1363 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3520 Training loss: 1.1590 0.1382 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3520 Training loss: 1.1588 0.1378 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3520 Training loss: 1.1588 0.1372 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3520 Training loss: 1.1589 0.1375 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3520 Training loss: 1.1589 0.1366 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3520 Training loss: 1.1588 0.1357 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3520 Training loss: 1.1588 0.1363 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3520 Training loss: 1.1589 0.1360 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3520 Training loss: 1.1590 0.1324 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3520 Training loss: 1.1589 0.1388 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3520 Training loss: 1.1588 0.1362 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3520 Training loss: 1.1588 0.1370 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3520 Training loss: 1.1587 0.1345 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3520 Training loss: 1.1587 0.1322 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3520 Training loss: 1.1587 0.1360 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3520 Training loss: 1.1586 0.1376 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3520 Training loss: 1.1587 0.1365 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3520 Training loss: 1.1588 0.1358 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3520 Training loss: 1.1589 0.1374 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3520 Training loss: 1.1587 0.1364 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3520 Training loss: 1.1588 0.1381 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3520 Training loss: 1.1588 0.1363 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3520 Training loss: 1.1587 0.1333 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3520 Training loss: 1.1584 0.1375 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3520 Training loss: 1.1583 0.1357 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3520 Training loss: 1.1581 0.1360 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3520 Training loss: 1.1581 0.1258 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3520 Training loss: 1.1581 0.1354 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3520 Training loss: 1.1580 0.1399 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3520 Training loss: 1.1580 0.1369 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3520 Training loss: 1.1580 0.1376 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3520 Training loss: 1.1579 0.1341 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3520 Training loss: 1.1579 0.1385 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3520 Training loss: 1.1579 0.1374 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3520 Training loss: 1.1581 0.1370 sec/batch\n",
      "Epoch 18/20  Iteration 2993/3520 Training loss: 1.2265 0.1376 sec/batch\n",
      "Epoch 18/20  Iteration 2994/3520 Training loss: 1.1779 0.1367 sec/batch\n",
      "Epoch 18/20  Iteration 2995/3520 Training loss: 1.1658 0.1354 sec/batch\n",
      "Epoch 18/20  Iteration 2996/3520 Training loss: 1.1601 0.1401 sec/batch\n",
      "Epoch 18/20  Iteration 2997/3520 Training loss: 1.1592 0.1380 sec/batch\n",
      "Epoch 18/20  Iteration 2998/3520 Training loss: 1.1582 0.1358 sec/batch\n",
      "Epoch 18/20  Iteration 2999/3520 Training loss: 1.1552 0.1380 sec/batch\n",
      "Epoch 18/20  Iteration 3000/3520 Training loss: 1.1549 0.1312 sec/batch\n",
      "Validation loss: 1.15724 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 3001/3520 Training loss: 1.1740 0.1527 sec/batch\n",
      "Epoch 18/20  Iteration 3002/3520 Training loss: 1.1724 0.1506 sec/batch\n",
      "Epoch 18/20  Iteration 3003/3520 Training loss: 1.1714 0.1455 sec/batch\n",
      "Epoch 18/20  Iteration 3004/3520 Training loss: 1.1703 0.1332 sec/batch\n",
      "Epoch 18/20  Iteration 3005/3520 Training loss: 1.1684 0.1366 sec/batch\n",
      "Epoch 18/20  Iteration 3006/3520 Training loss: 1.1691 0.1384 sec/batch\n",
      "Epoch 18/20  Iteration 3007/3520 Training loss: 1.1678 0.1363 sec/batch\n",
      "Epoch 18/20  Iteration 3008/3520 Training loss: 1.1659 0.1379 sec/batch\n",
      "Epoch 18/20  Iteration 3009/3520 Training loss: 1.1647 0.1348 sec/batch\n",
      "Epoch 18/20  Iteration 3010/3520 Training loss: 1.1646 0.1376 sec/batch\n",
      "Epoch 18/20  Iteration 3011/3520 Training loss: 1.1633 0.1353 sec/batch\n",
      "Epoch 18/20  Iteration 3012/3520 Training loss: 1.1634 0.1379 sec/batch\n",
      "Epoch 18/20  Iteration 3013/3520 Training loss: 1.1633 0.1378 sec/batch\n",
      "Epoch 18/20  Iteration 3014/3520 Training loss: 1.1624 0.1375 sec/batch\n",
      "Epoch 18/20  Iteration 3015/3520 Training loss: 1.1623 0.1363 sec/batch\n",
      "Epoch 18/20  Iteration 3016/3520 Training loss: 1.1628 0.1359 sec/batch\n",
      "Epoch 18/20  Iteration 3017/3520 Training loss: 1.1631 0.1374 sec/batch\n",
      "Epoch 18/20  Iteration 3018/3520 Training loss: 1.1621 0.1369 sec/batch\n",
      "Epoch 18/20  Iteration 3019/3520 Training loss: 1.1612 0.1389 sec/batch\n",
      "Epoch 18/20  Iteration 3020/3520 Training loss: 1.1610 0.1358 sec/batch\n",
      "Epoch 18/20  Iteration 3021/3520 Training loss: 1.1601 0.1364 sec/batch\n",
      "Epoch 18/20  Iteration 3022/3520 Training loss: 1.1602 0.1373 sec/batch\n",
      "Epoch 18/20  Iteration 3023/3520 Training loss: 1.1592 0.1375 sec/batch\n",
      "Epoch 18/20  Iteration 3024/3520 Training loss: 1.1590 0.1370 sec/batch\n",
      "Epoch 18/20  Iteration 3025/3520 Training loss: 1.1594 0.1378 sec/batch\n",
      "Epoch 18/20  Iteration 3026/3520 Training loss: 1.1589 0.1361 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3520 Training loss: 1.1587 0.1358 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3520 Training loss: 1.1588 0.1381 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3520 Training loss: 1.1582 0.1355 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3520 Training loss: 1.1584 0.1337 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3520 Training loss: 1.1589 0.1378 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3520 Training loss: 1.1589 0.1361 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3520 Training loss: 1.1586 0.1360 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3520 Training loss: 1.1579 0.1303 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3520 Training loss: 1.1577 0.1362 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3520 Training loss: 1.1569 0.1310 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3520 Training loss: 1.1563 0.1359 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3520 Training loss: 1.1559 0.1360 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3520 Training loss: 1.1559 0.1355 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3520 Training loss: 1.1559 0.1366 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3520 Training loss: 1.1558 0.1371 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3520 Training loss: 1.1558 0.1382 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3520 Training loss: 1.1557 0.1362 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3520 Training loss: 1.1558 0.1341 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3520 Training loss: 1.1559 0.1357 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3520 Training loss: 1.1557 0.1367 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3520 Training loss: 1.1555 0.1361 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3520 Training loss: 1.1553 0.1366 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3520 Training loss: 1.1549 0.1364 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3520 Training loss: 1.1550 0.1372 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3520 Training loss: 1.1545 0.1358 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3520 Training loss: 1.1547 0.1333 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3520 Training loss: 1.1545 0.1303 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3520 Training loss: 1.1545 0.1362 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3520 Training loss: 1.1544 0.1380 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3520 Training loss: 1.1543 0.1361 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3520 Training loss: 1.1540 0.1372 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3520 Training loss: 1.1541 0.1352 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3520 Training loss: 1.1541 0.1373 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3520 Training loss: 1.1537 0.1364 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3520 Training loss: 1.1534 0.1366 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3520 Training loss: 1.1534 0.1332 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3520 Training loss: 1.1533 0.1368 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3520 Training loss: 1.1529 0.1371 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3520 Training loss: 1.1525 0.1371 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3520 Training loss: 1.1520 0.1385 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3520 Training loss: 1.1519 0.1365 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3520 Training loss: 1.1520 0.1370 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3520 Training loss: 1.1521 0.1314 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3520 Training loss: 1.1525 0.1380 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3520 Training loss: 1.1525 0.1362 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3520 Training loss: 1.1523 0.1370 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3520 Training loss: 1.1520 0.1357 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3520 Training loss: 1.1521 0.1385 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3520 Training loss: 1.1519 0.1365 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3520 Training loss: 1.1523 0.1337 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3520 Training loss: 1.1521 0.1311 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3520 Training loss: 1.1518 0.1383 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3520 Training loss: 1.1520 0.1370 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3520 Training loss: 1.1520 0.1371 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3520 Training loss: 1.1521 0.1351 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3520 Training loss: 1.1519 0.1353 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3520 Training loss: 1.1518 0.1349 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3520 Training loss: 1.1519 0.1298 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3520 Training loss: 1.1516 0.1355 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3520 Training loss: 1.1514 0.1354 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3520 Training loss: 1.1513 0.1366 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3520 Training loss: 1.1509 0.1369 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3520 Training loss: 1.1509 0.1366 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3520 Training loss: 1.1509 0.1358 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3520 Training loss: 1.1508 0.1362 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3520 Training loss: 1.1508 0.1392 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3520 Training loss: 1.1507 0.1388 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3520 Training loss: 1.1508 0.1365 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3520 Training loss: 1.1506 0.1366 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3520 Training loss: 1.1501 0.1356 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3520 Training loss: 1.1496 0.1374 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3520 Training loss: 1.1496 0.1356 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3520 Training loss: 1.1495 0.1390 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3520 Training loss: 1.1492 0.1377 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3520 Training loss: 1.1492 0.1364 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3520 Training loss: 1.1492 0.1358 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3520 Training loss: 1.1493 0.1313 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3520 Training loss: 1.1494 0.1387 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3520 Training loss: 1.1495 0.1359 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3520 Training loss: 1.1494 0.1320 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3520 Training loss: 1.1493 0.1377 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3520 Training loss: 1.1492 0.1362 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3520 Training loss: 1.1492 0.1385 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3520 Training loss: 1.1493 0.1375 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3520 Training loss: 1.1492 0.1369 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3520 Training loss: 1.1491 0.1363 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3520 Training loss: 1.1490 0.1367 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3520 Training loss: 1.1489 0.1364 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3520 Training loss: 1.1489 0.1373 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3520 Training loss: 1.1488 0.1381 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3520 Training loss: 1.1487 0.1378 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3520 Training loss: 1.1484 0.1367 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3520 Training loss: 1.1482 0.1366 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3520 Training loss: 1.1482 0.1365 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3520 Training loss: 1.1481 0.1389 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3520 Training loss: 1.1483 0.1369 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3520 Training loss: 1.1483 0.1355 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3520 Training loss: 1.1486 0.1315 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3520 Training loss: 1.1485 0.1378 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3520 Training loss: 1.1487 0.1378 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3520 Training loss: 1.1488 0.1368 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3520 Training loss: 1.1486 0.1353 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3520 Training loss: 1.1487 0.1365 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3520 Training loss: 1.1484 0.1380 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3520 Training loss: 1.1486 0.1375 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3520 Training loss: 1.1486 0.1362 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3520 Training loss: 1.1486 0.1344 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3520 Training loss: 1.1485 0.1366 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3520 Training loss: 1.1483 0.1369 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3520 Training loss: 1.1483 0.1366 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3520 Training loss: 1.1484 0.1375 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3520 Training loss: 1.1484 0.1370 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3520 Training loss: 1.1483 0.1342 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3520 Training loss: 1.1483 0.1362 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3520 Training loss: 1.1482 0.1383 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3520 Training loss: 1.1484 0.1388 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3520 Training loss: 1.1483 0.1394 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3520 Training loss: 1.1482 0.1321 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3520 Training loss: 1.1482 0.1379 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3520 Training loss: 1.1481 0.1344 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3520 Training loss: 1.1480 0.1352 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3520 Training loss: 1.1480 0.1353 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3520 Training loss: 1.1478 0.1359 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3520 Training loss: 1.1479 0.1356 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3520 Training loss: 1.1480 0.1342 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3520 Training loss: 1.1481 0.1363 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3520 Training loss: 1.1479 0.1339 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3520 Training loss: 1.1479 0.1361 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3520 Training loss: 1.1479 0.1361 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3520 Training loss: 1.1478 0.1382 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3520 Training loss: 1.1475 0.1365 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3520 Training loss: 1.1473 0.1375 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3520 Training loss: 1.1471 0.1389 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3520 Training loss: 1.1471 0.1366 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3520 Training loss: 1.1472 0.1372 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3520 Training loss: 1.1471 0.1385 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3520 Training loss: 1.1471 0.1329 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3520 Training loss: 1.1471 0.1387 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3520 Training loss: 1.1470 0.1361 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3520 Training loss: 1.1470 0.1378 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3520 Training loss: 1.1470 0.1379 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3520 Training loss: 1.1472 0.1329 sec/batch\n",
      "Epoch 19/20  Iteration 3169/3520 Training loss: 1.2160 0.1359 sec/batch\n",
      "Epoch 19/20  Iteration 3170/3520 Training loss: 1.1667 0.1383 sec/batch\n",
      "Epoch 19/20  Iteration 3171/3520 Training loss: 1.1532 0.1385 sec/batch\n",
      "Epoch 19/20  Iteration 3172/3520 Training loss: 1.1458 0.1365 sec/batch\n",
      "Epoch 19/20  Iteration 3173/3520 Training loss: 1.1437 0.1372 sec/batch\n",
      "Epoch 19/20  Iteration 3174/3520 Training loss: 1.1436 0.1395 sec/batch\n",
      "Epoch 19/20  Iteration 3175/3520 Training loss: 1.1417 0.1401 sec/batch\n",
      "Epoch 19/20  Iteration 3176/3520 Training loss: 1.1421 0.1374 sec/batch\n",
      "Epoch 19/20  Iteration 3177/3520 Training loss: 1.1402 0.1377 sec/batch\n",
      "Epoch 19/20  Iteration 3178/3520 Training loss: 1.1394 0.1382 sec/batch\n",
      "Epoch 19/20  Iteration 3179/3520 Training loss: 1.1406 0.1381 sec/batch\n",
      "Epoch 19/20  Iteration 3180/3520 Training loss: 1.1410 0.1378 sec/batch\n",
      "Epoch 19/20  Iteration 3181/3520 Training loss: 1.1405 0.1355 sec/batch\n",
      "Epoch 19/20  Iteration 3182/3520 Training loss: 1.1420 0.1369 sec/batch\n",
      "Epoch 19/20  Iteration 3183/3520 Training loss: 1.1413 0.1381 sec/batch\n",
      "Epoch 19/20  Iteration 3184/3520 Training loss: 1.1402 0.1396 sec/batch\n",
      "Epoch 19/20  Iteration 3185/3520 Training loss: 1.1403 0.1360 sec/batch\n",
      "Epoch 19/20  Iteration 3186/3520 Training loss: 1.1407 0.1363 sec/batch\n",
      "Epoch 19/20  Iteration 3187/3520 Training loss: 1.1397 0.1391 sec/batch\n",
      "Epoch 19/20  Iteration 3188/3520 Training loss: 1.1403 0.1360 sec/batch\n",
      "Epoch 19/20  Iteration 3189/3520 Training loss: 1.1410 0.1327 sec/batch\n",
      "Epoch 19/20  Iteration 3190/3520 Training loss: 1.1404 0.1356 sec/batch\n",
      "Epoch 19/20  Iteration 3191/3520 Training loss: 1.1405 0.1374 sec/batch\n",
      "Epoch 19/20  Iteration 3192/3520 Training loss: 1.1413 0.1345 sec/batch\n",
      "Epoch 19/20  Iteration 3193/3520 Training loss: 1.1415 0.1376 sec/batch\n",
      "Epoch 19/20  Iteration 3194/3520 Training loss: 1.1414 0.1394 sec/batch\n",
      "Epoch 19/20  Iteration 3195/3520 Training loss: 1.1404 0.1371 sec/batch\n",
      "Epoch 19/20  Iteration 3196/3520 Training loss: 1.1405 0.1367 sec/batch\n",
      "Epoch 19/20  Iteration 3197/3520 Training loss: 1.1397 0.1368 sec/batch\n",
      "Epoch 19/20  Iteration 3198/3520 Training loss: 1.1401 0.1393 sec/batch\n",
      "Epoch 19/20  Iteration 3199/3520 Training loss: 1.1397 0.1367 sec/batch\n",
      "Epoch 19/20  Iteration 3200/3520 Training loss: 1.1393 0.1383 sec/batch\n",
      "Validation loss: 1.15302 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 3201/3520 Training loss: 1.1465 0.1378 sec/batch\n",
      "Epoch 19/20  Iteration 3202/3520 Training loss: 1.1465 0.1383 sec/batch\n",
      "Epoch 19/20  Iteration 3203/3520 Training loss: 1.1464 0.1383 sec/batch\n",
      "Epoch 19/20  Iteration 3204/3520 Training loss: 1.1465 0.1377 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3520 Training loss: 1.1456 0.1376 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3520 Training loss: 1.1460 0.1342 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3520 Training loss: 1.1463 0.1373 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3520 Training loss: 1.1461 0.1361 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3520 Training loss: 1.1457 0.1346 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3520 Training loss: 1.1452 0.1331 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3520 Training loss: 1.1453 0.1359 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3520 Training loss: 1.1447 0.1374 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3520 Training loss: 1.1441 0.1363 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3520 Training loss: 1.1440 0.1333 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3520 Training loss: 1.1438 0.1392 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3520 Training loss: 1.1437 0.1377 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3520 Training loss: 1.1437 0.1379 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3520 Training loss: 1.1437 0.1359 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3520 Training loss: 1.1434 0.1377 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3520 Training loss: 1.1435 0.1353 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3520 Training loss: 1.1437 0.1383 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3520 Training loss: 1.1435 0.1371 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3520 Training loss: 1.1432 0.1369 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3520 Training loss: 1.1432 0.1367 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3520 Training loss: 1.1429 0.1361 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3520 Training loss: 1.1428 0.1362 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3520 Training loss: 1.1425 0.1355 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3520 Training loss: 1.1426 0.1375 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3520 Training loss: 1.1424 0.1380 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3520 Training loss: 1.1425 0.1380 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3520 Training loss: 1.1424 0.1375 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3520 Training loss: 1.1424 0.1359 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3520 Training loss: 1.1422 0.1366 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3520 Training loss: 1.1421 0.1359 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3520 Training loss: 1.1422 0.1329 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3520 Training loss: 1.1418 0.1358 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3520 Training loss: 1.1417 0.1380 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3520 Training loss: 1.1418 0.1312 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3520 Training loss: 1.1418 0.1358 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3520 Training loss: 1.1414 0.1360 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3520 Training loss: 1.1411 0.1374 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3520 Training loss: 1.1405 0.1369 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3520 Training loss: 1.1404 0.1377 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3520 Training loss: 1.1405 0.1306 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3520 Training loss: 1.1407 0.1381 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3520 Training loss: 1.1411 0.1479 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3520 Training loss: 1.1411 0.1378 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3520 Training loss: 1.1410 0.1371 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3520 Training loss: 1.1407 0.1371 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3520 Training loss: 1.1409 0.1307 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3520 Training loss: 1.1409 0.1370 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3520 Training loss: 1.1412 0.1316 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3520 Training loss: 1.1411 0.1371 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3520 Training loss: 1.1409 0.1362 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3520 Training loss: 1.1411 0.1369 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3520 Training loss: 1.1409 0.1380 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3520 Training loss: 1.1411 0.1367 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3520 Training loss: 1.1408 0.1360 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3520 Training loss: 1.1407 0.1354 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3520 Training loss: 1.1409 0.1327 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3520 Training loss: 1.1405 0.1362 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3520 Training loss: 1.1403 0.1363 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3520 Training loss: 1.1403 0.1379 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3520 Training loss: 1.1399 0.1374 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3520 Training loss: 1.1400 0.1359 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3520 Training loss: 1.1399 0.1362 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3520 Training loss: 1.1398 0.1376 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3520 Training loss: 1.1397 0.1376 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3520 Training loss: 1.1395 0.1366 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3520 Training loss: 1.1394 0.1386 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3520 Training loss: 1.1392 0.1414 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3520 Training loss: 1.1387 0.1349 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3520 Training loss: 1.1384 0.1356 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3520 Training loss: 1.1385 0.1367 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3520 Training loss: 1.1384 0.1360 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3520 Training loss: 1.1382 0.1367 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3520 Training loss: 1.1382 0.1359 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3520 Training loss: 1.1381 0.1376 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3520 Training loss: 1.1382 0.1385 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3520 Training loss: 1.1383 0.1369 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3520 Training loss: 1.1385 0.1373 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3520 Training loss: 1.1383 0.1358 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3520 Training loss: 1.1382 0.1378 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3520 Training loss: 1.1382 0.1368 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3520 Training loss: 1.1381 0.1343 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3520 Training loss: 1.1383 0.1345 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3520 Training loss: 1.1383 0.1375 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3520 Training loss: 1.1381 0.1361 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3520 Training loss: 1.1379 0.1374 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3520 Training loss: 1.1379 0.1384 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3520 Training loss: 1.1378 0.1380 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3520 Training loss: 1.1378 0.1364 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3520 Training loss: 1.1377 0.1358 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3520 Training loss: 1.1374 0.1307 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3520 Training loss: 1.1373 0.1385 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3520 Training loss: 1.1374 0.1319 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3520 Training loss: 1.1373 0.1357 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3520 Training loss: 1.1375 0.1358 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3520 Training loss: 1.1375 0.1382 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3520 Training loss: 1.1377 0.1353 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3520 Training loss: 1.1376 0.1362 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3520 Training loss: 1.1378 0.1351 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3520 Training loss: 1.1379 0.1363 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3520 Training loss: 1.1378 0.1394 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3520 Training loss: 1.1378 0.1393 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3520 Training loss: 1.1376 0.1367 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3520 Training loss: 1.1378 0.1365 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3520 Training loss: 1.1378 0.1372 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3520 Training loss: 1.1378 0.1372 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3520 Training loss: 1.1378 0.1339 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3520 Training loss: 1.1376 0.1346 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3520 Training loss: 1.1376 0.1369 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3520 Training loss: 1.1376 0.1382 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3520 Training loss: 1.1377 0.1324 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3520 Training loss: 1.1376 0.1324 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3520 Training loss: 1.1375 0.1363 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3520 Training loss: 1.1375 0.1391 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3520 Training loss: 1.1376 0.1367 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3520 Training loss: 1.1375 0.1306 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3520 Training loss: 1.1375 0.1306 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3520 Training loss: 1.1374 0.1373 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3520 Training loss: 1.1372 0.1333 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3520 Training loss: 1.1372 0.1374 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3520 Training loss: 1.1372 0.1363 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3520 Training loss: 1.1370 0.1378 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3520 Training loss: 1.1372 0.1373 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3520 Training loss: 1.1372 0.1393 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3520 Training loss: 1.1373 0.1308 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3520 Training loss: 1.1372 0.1392 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3520 Training loss: 1.1372 0.1349 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3520 Training loss: 1.1373 0.1366 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3520 Training loss: 1.1371 0.1398 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3520 Training loss: 1.1368 0.1363 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3520 Training loss: 1.1367 0.1372 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3520 Training loss: 1.1365 0.1359 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3520 Training loss: 1.1364 0.1376 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3520 Training loss: 1.1365 0.1345 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3520 Training loss: 1.1365 0.1363 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3520 Training loss: 1.1364 0.1363 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3520 Training loss: 1.1365 0.1351 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3520 Training loss: 1.1364 0.1362 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3520 Training loss: 1.1364 0.1378 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3520 Training loss: 1.1365 0.1365 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3520 Training loss: 1.1367 0.1360 sec/batch\n",
      "Epoch 20/20  Iteration 3345/3520 Training loss: 1.2051 0.1376 sec/batch\n",
      "Epoch 20/20  Iteration 3346/3520 Training loss: 1.1560 0.1327 sec/batch\n",
      "Epoch 20/20  Iteration 3347/3520 Training loss: 1.1432 0.1365 sec/batch\n",
      "Epoch 20/20  Iteration 3348/3520 Training loss: 1.1390 0.1371 sec/batch\n",
      "Epoch 20/20  Iteration 3349/3520 Training loss: 1.1379 0.1369 sec/batch\n",
      "Epoch 20/20  Iteration 3350/3520 Training loss: 1.1364 0.1314 sec/batch\n",
      "Epoch 20/20  Iteration 3351/3520 Training loss: 1.1332 0.1364 sec/batch\n",
      "Epoch 20/20  Iteration 3352/3520 Training loss: 1.1331 0.1386 sec/batch\n",
      "Epoch 20/20  Iteration 3353/3520 Training loss: 1.1306 0.1358 sec/batch\n",
      "Epoch 20/20  Iteration 3354/3520 Training loss: 1.1303 0.1377 sec/batch\n",
      "Epoch 20/20  Iteration 3355/3520 Training loss: 1.1312 0.1351 sec/batch\n",
      "Epoch 20/20  Iteration 3356/3520 Training loss: 1.1316 0.1351 sec/batch\n",
      "Epoch 20/20  Iteration 3357/3520 Training loss: 1.1308 0.1357 sec/batch\n",
      "Epoch 20/20  Iteration 3358/3520 Training loss: 1.1322 0.1365 sec/batch\n",
      "Epoch 20/20  Iteration 3359/3520 Training loss: 1.1324 0.1388 sec/batch\n",
      "Epoch 20/20  Iteration 3360/3520 Training loss: 1.1313 0.1374 sec/batch\n",
      "Epoch 20/20  Iteration 3361/3520 Training loss: 1.1306 0.1383 sec/batch\n",
      "Epoch 20/20  Iteration 3362/3520 Training loss: 1.1312 0.1364 sec/batch\n",
      "Epoch 20/20  Iteration 3363/3520 Training loss: 1.1307 0.1375 sec/batch\n",
      "Epoch 20/20  Iteration 3364/3520 Training loss: 1.1314 0.1376 sec/batch\n",
      "Epoch 20/20  Iteration 3365/3520 Training loss: 1.1317 0.1321 sec/batch\n",
      "Epoch 20/20  Iteration 3366/3520 Training loss: 1.1312 0.1379 sec/batch\n",
      "Epoch 20/20  Iteration 3367/3520 Training loss: 1.1315 0.1369 sec/batch\n",
      "Epoch 20/20  Iteration 3368/3520 Training loss: 1.1322 0.1370 sec/batch\n",
      "Epoch 20/20  Iteration 3369/3520 Training loss: 1.1324 0.1381 sec/batch\n",
      "Epoch 20/20  Iteration 3370/3520 Training loss: 1.1322 0.1402 sec/batch\n",
      "Epoch 20/20  Iteration 3371/3520 Training loss: 1.1317 0.1374 sec/batch\n",
      "Epoch 20/20  Iteration 3372/3520 Training loss: 1.1316 0.1374 sec/batch\n",
      "Epoch 20/20  Iteration 3373/3520 Training loss: 1.1311 0.1369 sec/batch\n",
      "Epoch 20/20  Iteration 3374/3520 Training loss: 1.1314 0.1365 sec/batch\n",
      "Epoch 20/20  Iteration 3375/3520 Training loss: 1.1312 0.1351 sec/batch\n",
      "Epoch 20/20  Iteration 3376/3520 Training loss: 1.1311 0.1388 sec/batch\n",
      "Epoch 20/20  Iteration 3377/3520 Training loss: 1.1321 0.1408 sec/batch\n",
      "Epoch 20/20  Iteration 3378/3520 Training loss: 1.1315 0.1389 sec/batch\n",
      "Epoch 20/20  Iteration 3379/3520 Training loss: 1.1315 0.1362 sec/batch\n",
      "Epoch 20/20  Iteration 3380/3520 Training loss: 1.1319 0.1368 sec/batch\n",
      "Epoch 20/20  Iteration 3381/3520 Training loss: 1.1312 0.1360 sec/batch\n",
      "Epoch 20/20  Iteration 3382/3520 Training loss: 1.1316 0.1371 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3520 Training loss: 1.1319 0.1387 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3520 Training loss: 1.1318 0.1388 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3520 Training loss: 1.1315 0.1360 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3520 Training loss: 1.1312 0.1377 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3520 Training loss: 1.1314 0.1373 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3520 Training loss: 1.1308 0.1377 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3520 Training loss: 1.1305 0.1366 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3520 Training loss: 1.1303 0.1377 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3520 Training loss: 1.1303 0.1354 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3520 Training loss: 1.1303 0.1367 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3520 Training loss: 1.1304 0.1358 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3520 Training loss: 1.1307 0.1370 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3520 Training loss: 1.1306 0.1392 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3520 Training loss: 1.1308 0.1361 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3520 Training loss: 1.1310 0.1369 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3520 Training loss: 1.1309 0.1374 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3520 Training loss: 1.1308 0.1355 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3520 Training loss: 1.1306 0.1314 sec/batch\n",
      "Validation loss: 1.15105 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 3401/3520 Training loss: 1.1343 0.1512 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3520 Training loss: 1.1345 0.1560 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3520 Training loss: 1.1342 0.1441 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3520 Training loss: 1.1342 0.1345 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3520 Training loss: 1.1341 0.1339 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3520 Training loss: 1.1341 0.1370 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3520 Training loss: 1.1343 0.1353 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3520 Training loss: 1.1342 0.1359 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3520 Training loss: 1.1339 0.1407 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3520 Training loss: 1.1341 0.1362 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3520 Training loss: 1.1341 0.1374 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3520 Training loss: 1.1335 0.1362 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3520 Training loss: 1.1333 0.1376 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3520 Training loss: 1.1335 0.1391 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3520 Training loss: 1.1334 0.1368 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3520 Training loss: 1.1328 0.1380 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3520 Training loss: 1.1326 0.1357 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3520 Training loss: 1.1321 0.1364 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3520 Training loss: 1.1320 0.1383 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3520 Training loss: 1.1321 0.1358 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3520 Training loss: 1.1323 0.1364 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3520 Training loss: 1.1327 0.1372 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3520 Training loss: 1.1325 0.1361 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3520 Training loss: 1.1324 0.1371 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3520 Training loss: 1.1321 0.1368 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3520 Training loss: 1.1323 0.1329 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3520 Training loss: 1.1321 0.1359 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3520 Training loss: 1.1325 0.1384 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3520 Training loss: 1.1324 0.1353 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3520 Training loss: 1.1320 0.1368 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3520 Training loss: 1.1322 0.1364 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3520 Training loss: 1.1320 0.1370 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3520 Training loss: 1.1322 0.1383 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3520 Training loss: 1.1320 0.1365 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3520 Training loss: 1.1318 0.1303 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3520 Training loss: 1.1318 0.1379 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3520 Training loss: 1.1315 0.1354 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3520 Training loss: 1.1314 0.1359 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3520 Training loss: 1.1313 0.1364 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3520 Training loss: 1.1309 0.1365 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3520 Training loss: 1.1310 0.1396 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3520 Training loss: 1.1310 0.1455 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3520 Training loss: 1.1309 0.1375 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3520 Training loss: 1.1309 0.1328 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3520 Training loss: 1.1307 0.1368 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3520 Training loss: 1.1307 0.1342 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3520 Training loss: 1.1305 0.1369 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3520 Training loss: 1.1300 0.1322 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3520 Training loss: 1.1296 0.1365 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3520 Training loss: 1.1296 0.1391 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3520 Training loss: 1.1296 0.1390 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3520 Training loss: 1.1294 0.1388 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3520 Training loss: 1.1294 0.1360 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3520 Training loss: 1.1293 0.1378 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3520 Training loss: 1.1293 0.1389 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3520 Training loss: 1.1294 0.1364 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3520 Training loss: 1.1296 0.1369 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3520 Training loss: 1.1296 0.1359 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3520 Training loss: 1.1294 0.1374 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3520 Training loss: 1.1293 0.1380 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3520 Training loss: 1.1292 0.1359 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3520 Training loss: 1.1293 0.1364 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3520 Training loss: 1.1293 0.1381 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3520 Training loss: 1.1291 0.1366 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3520 Training loss: 1.1290 0.1371 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3520 Training loss: 1.1290 0.1319 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3520 Training loss: 1.1290 0.1366 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3520 Training loss: 1.1290 0.1385 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3520 Training loss: 1.1289 0.1322 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3520 Training loss: 1.1285 0.1357 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3520 Training loss: 1.1285 0.1375 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3520 Training loss: 1.1285 0.1361 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3520 Training loss: 1.1285 0.1387 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3520 Training loss: 1.1286 0.1362 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3520 Training loss: 1.1286 0.1370 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3520 Training loss: 1.1289 0.1375 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3520 Training loss: 1.1288 0.1386 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3520 Training loss: 1.1290 0.1362 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3520 Training loss: 1.1291 0.1370 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3520 Training loss: 1.1291 0.1355 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3520 Training loss: 1.1291 0.1322 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3520 Training loss: 1.1289 0.1362 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3520 Training loss: 1.1291 0.1388 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3520 Training loss: 1.1292 0.1364 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3520 Training loss: 1.1290 0.1365 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3520 Training loss: 1.1289 0.1331 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3520 Training loss: 1.1288 0.1319 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3520 Training loss: 1.1288 0.1376 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3520 Training loss: 1.1289 0.1370 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3520 Training loss: 1.1290 0.1376 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3520 Training loss: 1.1290 0.1368 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3520 Training loss: 1.1289 0.1400 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3520 Training loss: 1.1290 0.1378 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3520 Training loss: 1.1292 0.1382 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3520 Training loss: 1.1291 0.1371 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3520 Training loss: 1.1290 0.1376 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3520 Training loss: 1.1290 0.1368 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3520 Training loss: 1.1288 0.1373 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3520 Training loss: 1.1288 0.1369 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3520 Training loss: 1.1288 0.1364 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3520 Training loss: 1.1286 0.1394 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3520 Training loss: 1.1287 0.1362 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3520 Training loss: 1.1288 0.1361 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3520 Training loss: 1.1290 0.1395 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3520 Training loss: 1.1288 0.1383 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3520 Training loss: 1.1288 0.1376 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3520 Training loss: 1.1289 0.1349 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3520 Training loss: 1.1287 0.1326 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3520 Training loss: 1.1285 0.1382 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3520 Training loss: 1.1283 0.1356 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3520 Training loss: 1.1281 0.1310 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3520 Training loss: 1.1281 0.1356 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3520 Training loss: 1.1281 0.1389 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3520 Training loss: 1.1280 0.1360 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3520 Training loss: 1.1279 0.1369 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3520 Training loss: 1.1280 0.1355 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3520 Training loss: 1.1279 0.1378 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3520 Training loss: 1.1279 0.1463 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3520 Training loss: 1.1280 0.1370 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3520 Training loss: 1.1282 0.1324 sec/batch\n",
      "Validation loss: 1.15202 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 1/3520 Training loss: 4.3469 1.4695 sec/batch\n",
      "Epoch 1/20  Iteration 2/3520 Training loss: 4.2865 0.2939 sec/batch\n",
      "Epoch 1/20  Iteration 3/3520 Training loss: 4.7172 0.2931 sec/batch\n",
      "Epoch 1/20  Iteration 4/3520 Training loss: 4.5214 0.2941 sec/batch\n",
      "Epoch 1/20  Iteration 5/3520 Training loss: 4.3651 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 6/3520 Training loss: 4.2342 0.2944 sec/batch\n",
      "Epoch 1/20  Iteration 7/3520 Training loss: 4.1201 0.2924 sec/batch\n",
      "Epoch 1/20  Iteration 8/3520 Training loss: 4.0255 0.2947 sec/batch\n",
      "Epoch 1/20  Iteration 9/3520 Training loss: 3.9481 0.2927 sec/batch\n",
      "Epoch 1/20  Iteration 10/3520 Training loss: 3.8838 0.2954 sec/batch\n",
      "Epoch 1/20  Iteration 11/3520 Training loss: 3.8286 0.2934 sec/batch\n",
      "Epoch 1/20  Iteration 12/3520 Training loss: 3.7820 0.2938 sec/batch\n",
      "Epoch 1/20  Iteration 13/3520 Training loss: 3.7428 0.2933 sec/batch\n",
      "Epoch 1/20  Iteration 14/3520 Training loss: 3.7059 0.2932 sec/batch\n",
      "Epoch 1/20  Iteration 15/3520 Training loss: 3.6724 0.2931 sec/batch\n",
      "Epoch 1/20  Iteration 16/3520 Training loss: 3.6437 0.2929 sec/batch\n",
      "Epoch 1/20  Iteration 17/3520 Training loss: 3.6177 0.2938 sec/batch\n",
      "Epoch 1/20  Iteration 18/3520 Training loss: 3.5950 0.2944 sec/batch\n",
      "Epoch 1/20  Iteration 19/3520 Training loss: 3.5735 0.2935 sec/batch\n",
      "Epoch 1/20  Iteration 20/3520 Training loss: 3.5547 0.2934 sec/batch\n",
      "Epoch 1/20  Iteration 21/3520 Training loss: 3.5367 0.2925 sec/batch\n",
      "Epoch 1/20  Iteration 22/3520 Training loss: 3.5209 0.2928 sec/batch\n",
      "Epoch 1/20  Iteration 23/3520 Training loss: 3.5062 0.2942 sec/batch\n",
      "Epoch 1/20  Iteration 24/3520 Training loss: 3.4924 0.2949 sec/batch\n",
      "Epoch 1/20  Iteration 25/3520 Training loss: 3.4798 0.2945 sec/batch\n",
      "Epoch 1/20  Iteration 26/3520 Training loss: 3.4675 0.2934 sec/batch\n",
      "Epoch 1/20  Iteration 27/3520 Training loss: 3.4562 0.2942 sec/batch\n",
      "Epoch 1/20  Iteration 28/3520 Training loss: 3.4448 0.2940 sec/batch\n",
      "Epoch 1/20  Iteration 29/3520 Training loss: 3.4345 0.2940 sec/batch\n",
      "Epoch 1/20  Iteration 30/3520 Training loss: 3.4249 0.2936 sec/batch\n",
      "Epoch 1/20  Iteration 31/3520 Training loss: 3.4155 0.2936 sec/batch\n",
      "Epoch 1/20  Iteration 32/3520 Training loss: 3.4067 0.2950 sec/batch\n",
      "Epoch 1/20  Iteration 33/3520 Training loss: 3.3984 0.2938 sec/batch\n",
      "Epoch 1/20  Iteration 34/3520 Training loss: 3.3911 0.2951 sec/batch\n",
      "Epoch 1/20  Iteration 35/3520 Training loss: 3.3836 0.2933 sec/batch\n",
      "Epoch 1/20  Iteration 36/3520 Training loss: 3.3766 0.2930 sec/batch\n",
      "Epoch 1/20  Iteration 37/3520 Training loss: 3.3701 0.2933 sec/batch\n",
      "Epoch 1/20  Iteration 38/3520 Training loss: 3.3639 0.2936 sec/batch\n",
      "Epoch 1/20  Iteration 39/3520 Training loss: 3.3581 0.2933 sec/batch\n",
      "Epoch 1/20  Iteration 40/3520 Training loss: 3.3522 0.2933 sec/batch\n",
      "Epoch 1/20  Iteration 41/3520 Training loss: 3.3468 0.2934 sec/batch\n",
      "Epoch 1/20  Iteration 42/3520 Training loss: 3.3413 0.2931 sec/batch\n",
      "Epoch 1/20  Iteration 43/3520 Training loss: 3.3362 0.2929 sec/batch\n",
      "Epoch 1/20  Iteration 44/3520 Training loss: 3.3307 0.2933 sec/batch\n",
      "Epoch 1/20  Iteration 45/3520 Training loss: 3.3255 0.2940 sec/batch\n",
      "Epoch 1/20  Iteration 46/3520 Training loss: 3.3204 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 47/3520 Training loss: 3.3156 0.2940 sec/batch\n",
      "Epoch 1/20  Iteration 48/3520 Training loss: 3.3109 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 49/3520 Training loss: 3.3064 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 50/3520 Training loss: 3.3022 0.2934 sec/batch\n",
      "Epoch 1/20  Iteration 51/3520 Training loss: 3.2982 0.2971 sec/batch\n",
      "Epoch 1/20  Iteration 52/3520 Training loss: 3.2943 0.2927 sec/batch\n",
      "Epoch 1/20  Iteration 53/3520 Training loss: 3.2902 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 54/3520 Training loss: 3.2866 0.2942 sec/batch\n",
      "Epoch 1/20  Iteration 55/3520 Training loss: 3.2826 0.2949 sec/batch\n",
      "Epoch 1/20  Iteration 56/3520 Training loss: 3.2788 0.2977 sec/batch\n",
      "Epoch 1/20  Iteration 57/3520 Training loss: 3.2747 0.2944 sec/batch\n",
      "Epoch 1/20  Iteration 58/3520 Training loss: 3.2706 0.2944 sec/batch\n",
      "Epoch 1/20  Iteration 59/3520 Training loss: 3.2664 0.2943 sec/batch\n",
      "Epoch 1/20  Iteration 60/3520 Training loss: 3.2795 0.2946 sec/batch\n",
      "Epoch 1/20  Iteration 61/3520 Training loss: 3.3576 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 62/3520 Training loss: 3.3671 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 63/3520 Training loss: 3.3703 0.2943 sec/batch\n",
      "Epoch 1/20  Iteration 64/3520 Training loss: 3.3709 0.2937 sec/batch\n",
      "Epoch 1/20  Iteration 65/3520 Training loss: 3.3724 0.2943 sec/batch\n",
      "Epoch 1/20  Iteration 66/3520 Training loss: 3.3713 0.2952 sec/batch\n",
      "Epoch 1/20  Iteration 67/3520 Training loss: 3.3691 0.3039 sec/batch\n",
      "Epoch 1/20  Iteration 68/3520 Training loss: 3.3671 0.2951 sec/batch\n",
      "Epoch 1/20  Iteration 69/3520 Training loss: 3.3651 0.2938 sec/batch\n",
      "Epoch 1/20  Iteration 70/3520 Training loss: 3.3626 0.2958 sec/batch\n",
      "Epoch 1/20  Iteration 71/3520 Training loss: 3.3599 0.2936 sec/batch\n",
      "Epoch 1/20  Iteration 72/3520 Training loss: 3.3575 0.2936 sec/batch\n",
      "Epoch 1/20  Iteration 73/3520 Training loss: 3.3550 0.2943 sec/batch\n",
      "Epoch 1/20  Iteration 74/3520 Training loss: 3.3524 0.2946 sec/batch\n",
      "Epoch 1/20  Iteration 75/3520 Training loss: 3.3496 0.2956 sec/batch\n",
      "Epoch 1/20  Iteration 76/3520 Training loss: 3.3468 0.2939 sec/batch\n",
      "Epoch 1/20  Iteration 77/3520 Training loss: 3.3439 0.2935 sec/batch\n",
      "Epoch 1/20  Iteration 78/3520 Training loss: 3.3410 0.2945 sec/batch\n",
      "Epoch 1/20  Iteration 79/3520 Training loss: 3.3382 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 80/3520 Training loss: 3.3354 0.2944 sec/batch\n",
      "Epoch 1/20  Iteration 81/3520 Training loss: 3.3327 0.2940 sec/batch\n",
      "Epoch 1/20  Iteration 82/3520 Training loss: 3.3299 0.2943 sec/batch\n",
      "Epoch 1/20  Iteration 83/3520 Training loss: 3.3271 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 84/3520 Training loss: 3.3248 0.2941 sec/batch\n",
      "Epoch 1/20  Iteration 85/3520 Training loss: 3.3222 0.2949 sec/batch\n",
      "Epoch 1/20  Iteration 86/3520 Training loss: 3.3197 0.2950 sec/batch\n",
      "Epoch 1/20  Iteration 87/3520 Training loss: 3.3172 0.2950 sec/batch\n",
      "Epoch 1/20  Iteration 88/3520 Training loss: 3.3147 0.2945 sec/batch\n",
      "Epoch 1/20  Iteration 89/3520 Training loss: 3.3122 0.2954 sec/batch\n",
      "Epoch 1/20  Iteration 90/3520 Training loss: 3.3098 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 91/3520 Training loss: 3.3074 0.2935 sec/batch\n",
      "Epoch 1/20  Iteration 92/3520 Training loss: 3.3051 0.2947 sec/batch\n",
      "Epoch 1/20  Iteration 93/3520 Training loss: 3.3027 0.2942 sec/batch\n",
      "Epoch 1/20  Iteration 94/3520 Training loss: 3.3003 0.2952 sec/batch\n",
      "Epoch 1/20  Iteration 95/3520 Training loss: 3.2978 0.2951 sec/batch\n",
      "Epoch 1/20  Iteration 96/3520 Training loss: 3.2955 0.2942 sec/batch\n",
      "Epoch 1/20  Iteration 97/3520 Training loss: 3.2935 0.2939 sec/batch\n",
      "Epoch 1/20  Iteration 98/3520 Training loss: 3.2912 0.2947 sec/batch\n",
      "Epoch 1/20  Iteration 99/3520 Training loss: 3.2888 0.2954 sec/batch\n",
      "Epoch 1/20  Iteration 100/3520 Training loss: 3.2862 0.2940 sec/batch\n",
      "Epoch 1/20  Iteration 101/3520 Training loss: 3.2838 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 102/3520 Training loss: 3.2814 0.2987 sec/batch\n",
      "Epoch 1/20  Iteration 103/3520 Training loss: 3.2790 0.2954 sec/batch\n",
      "Epoch 1/20  Iteration 104/3520 Training loss: 3.2765 0.2944 sec/batch\n",
      "Epoch 1/20  Iteration 105/3520 Training loss: 3.2741 0.2936 sec/batch\n",
      "Epoch 1/20  Iteration 106/3520 Training loss: 3.2716 0.2930 sec/batch\n",
      "Epoch 1/20  Iteration 107/3520 Training loss: 3.2688 0.2947 sec/batch\n",
      "Epoch 1/20  Iteration 108/3520 Training loss: 3.2659 0.2949 sec/batch\n",
      "Epoch 1/20  Iteration 109/3520 Training loss: 3.2633 0.2942 sec/batch\n",
      "Epoch 1/20  Iteration 110/3520 Training loss: 3.2604 0.2949 sec/batch\n",
      "Epoch 1/20  Iteration 111/3520 Training loss: 3.2573 0.2945 sec/batch\n",
      "Epoch 1/20  Iteration 112/3520 Training loss: 3.2545 0.2950 sec/batch\n",
      "Epoch 1/20  Iteration 113/3520 Training loss: 3.2516 0.2952 sec/batch\n",
      "Epoch 1/20  Iteration 114/3520 Training loss: 3.2484 0.2943 sec/batch\n",
      "Epoch 1/20  Iteration 115/3520 Training loss: 3.2453 0.2953 sec/batch\n",
      "Epoch 1/20  Iteration 116/3520 Training loss: 3.2422 0.2945 sec/batch\n",
      "Epoch 1/20  Iteration 117/3520 Training loss: 3.2392 0.2950 sec/batch\n",
      "Epoch 1/20  Iteration 118/3520 Training loss: 3.2359 0.2938 sec/batch\n",
      "Epoch 1/20  Iteration 119/3520 Training loss: 3.2328 0.2940 sec/batch\n",
      "Epoch 1/20  Iteration 120/3520 Training loss: 3.2296 0.2936 sec/batch\n",
      "Epoch 1/20  Iteration 121/3520 Training loss: 3.2268 0.2939 sec/batch\n",
      "Epoch 1/20  Iteration 122/3520 Training loss: 3.2236 0.2928 sec/batch\n",
      "Epoch 1/20  Iteration 123/3520 Training loss: 3.2204 0.2934 sec/batch\n",
      "Epoch 1/20  Iteration 124/3520 Training loss: 3.2172 0.3042 sec/batch\n",
      "Epoch 1/20  Iteration 125/3520 Training loss: 3.2141 0.2943 sec/batch\n",
      "Epoch 1/20  Iteration 126/3520 Training loss: 3.2107 0.2931 sec/batch\n",
      "Epoch 1/20  Iteration 127/3520 Training loss: 3.2072 0.2938 sec/batch\n",
      "Epoch 1/20  Iteration 128/3520 Training loss: 3.2040 0.2931 sec/batch\n",
      "Epoch 1/20  Iteration 129/3520 Training loss: 3.2001 0.2944 sec/batch\n",
      "Epoch 1/20  Iteration 130/3520 Training loss: 3.1967 0.2936 sec/batch\n",
      "Epoch 1/20  Iteration 131/3520 Training loss: 3.1932 0.2935 sec/batch\n",
      "Epoch 1/20  Iteration 132/3520 Training loss: 3.1896 0.2934 sec/batch\n",
      "Epoch 1/20  Iteration 133/3520 Training loss: 3.1863 0.2925 sec/batch\n",
      "Epoch 1/20  Iteration 134/3520 Training loss: 3.1827 0.2938 sec/batch\n",
      "Epoch 1/20  Iteration 135/3520 Training loss: 3.1795 0.2936 sec/batch\n",
      "Epoch 1/20  Iteration 136/3520 Training loss: 3.1762 0.2947 sec/batch\n",
      "Epoch 1/20  Iteration 137/3520 Training loss: 3.1727 0.2933 sec/batch\n",
      "Epoch 1/20  Iteration 138/3520 Training loss: 3.1692 0.3053 sec/batch\n",
      "Epoch 1/20  Iteration 139/3520 Training loss: 3.1656 0.2930 sec/batch\n",
      "Epoch 1/20  Iteration 140/3520 Training loss: 3.1622 0.2937 sec/batch\n",
      "Epoch 1/20  Iteration 141/3520 Training loss: 3.1587 0.2945 sec/batch\n",
      "Epoch 1/20  Iteration 142/3520 Training loss: 3.1553 0.2934 sec/batch\n",
      "Epoch 1/20  Iteration 143/3520 Training loss: 3.1518 0.2948 sec/batch\n",
      "Epoch 1/20  Iteration 144/3520 Training loss: 3.1482 0.2938 sec/batch\n",
      "Epoch 1/20  Iteration 145/3520 Training loss: 3.1447 0.2946 sec/batch\n",
      "Epoch 1/20  Iteration 146/3520 Training loss: 3.1411 0.2935 sec/batch\n",
      "Epoch 1/20  Iteration 147/3520 Training loss: 3.1375 0.2942 sec/batch\n",
      "Epoch 1/20  Iteration 148/3520 Training loss: 3.1342 0.2939 sec/batch\n",
      "Epoch 1/20  Iteration 149/3520 Training loss: 3.1308 0.2947 sec/batch\n",
      "Epoch 1/20  Iteration 150/3520 Training loss: 3.1274 0.2939 sec/batch\n",
      "Epoch 1/20  Iteration 151/3520 Training loss: 3.1239 0.2933 sec/batch\n",
      "Epoch 1/20  Iteration 152/3520 Training loss: 3.1204 0.2941 sec/batch\n",
      "Epoch 1/20  Iteration 153/3520 Training loss: 3.1170 0.2949 sec/batch\n",
      "Epoch 1/20  Iteration 154/3520 Training loss: 3.1137 0.2932 sec/batch\n",
      "Epoch 1/20  Iteration 155/3520 Training loss: 3.1102 0.2941 sec/batch\n",
      "Epoch 1/20  Iteration 156/3520 Training loss: 3.1067 0.2937 sec/batch\n",
      "Epoch 1/20  Iteration 157/3520 Training loss: 3.1032 0.2933 sec/batch\n",
      "Epoch 1/20  Iteration 158/3520 Training loss: 3.0998 0.2947 sec/batch\n",
      "Epoch 1/20  Iteration 159/3520 Training loss: 3.0964 0.2946 sec/batch\n",
      "Epoch 1/20  Iteration 160/3520 Training loss: 3.0930 0.2976 sec/batch\n",
      "Epoch 1/20  Iteration 161/3520 Training loss: 3.0897 0.2932 sec/batch\n",
      "Epoch 1/20  Iteration 162/3520 Training loss: 3.0862 0.2934 sec/batch\n",
      "Epoch 1/20  Iteration 163/3520 Training loss: 3.0827 0.2932 sec/batch\n",
      "Epoch 1/20  Iteration 164/3520 Training loss: 3.0791 0.2934 sec/batch\n",
      "Epoch 1/20  Iteration 165/3520 Training loss: 3.0757 0.2946 sec/batch\n",
      "Epoch 1/20  Iteration 166/3520 Training loss: 3.0723 0.2942 sec/batch\n",
      "Epoch 1/20  Iteration 167/3520 Training loss: 3.0689 0.2938 sec/batch\n",
      "Epoch 1/20  Iteration 168/3520 Training loss: 3.0657 0.2940 sec/batch\n",
      "Epoch 1/20  Iteration 169/3520 Training loss: 3.0624 0.2955 sec/batch\n",
      "Epoch 1/20  Iteration 170/3520 Training loss: 3.0590 0.2934 sec/batch\n",
      "Epoch 1/20  Iteration 171/3520 Training loss: 3.0556 0.2937 sec/batch\n",
      "Epoch 1/20  Iteration 172/3520 Training loss: 3.0523 0.2943 sec/batch\n",
      "Epoch 1/20  Iteration 173/3520 Training loss: 3.0489 0.2951 sec/batch\n",
      "Epoch 1/20  Iteration 174/3520 Training loss: 3.0457 0.2955 sec/batch\n",
      "Epoch 1/20  Iteration 175/3520 Training loss: 3.0425 0.2950 sec/batch\n",
      "Epoch 1/20  Iteration 176/3520 Training loss: 3.0395 0.2935 sec/batch\n",
      "Epoch 2/20  Iteration 177/3520 Training loss: 2.5184 0.2941 sec/batch\n",
      "Epoch 2/20  Iteration 178/3520 Training loss: 2.4822 0.2942 sec/batch\n",
      "Epoch 2/20  Iteration 179/3520 Training loss: 2.4769 0.2935 sec/batch\n",
      "Epoch 2/20  Iteration 180/3520 Training loss: 2.4721 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 181/3520 Training loss: 2.4703 0.2937 sec/batch\n",
      "Epoch 2/20  Iteration 182/3520 Training loss: 2.4688 0.2941 sec/batch\n",
      "Epoch 2/20  Iteration 183/3520 Training loss: 2.4687 0.2958 sec/batch\n",
      "Epoch 2/20  Iteration 184/3520 Training loss: 2.4696 0.2952 sec/batch\n",
      "Epoch 2/20  Iteration 185/3520 Training loss: 2.4680 0.3066 sec/batch\n",
      "Epoch 2/20  Iteration 186/3520 Training loss: 2.4682 0.2947 sec/batch\n",
      "Epoch 2/20  Iteration 187/3520 Training loss: 2.4646 0.2939 sec/batch\n",
      "Epoch 2/20  Iteration 188/3520 Training loss: 2.4633 0.2956 sec/batch\n",
      "Epoch 2/20  Iteration 189/3520 Training loss: 2.4605 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 190/3520 Training loss: 2.4577 0.2951 sec/batch\n",
      "Epoch 2/20  Iteration 191/3520 Training loss: 2.4536 0.2937 sec/batch\n",
      "Epoch 2/20  Iteration 192/3520 Training loss: 2.4516 0.2939 sec/batch\n",
      "Epoch 2/20  Iteration 193/3520 Training loss: 2.4492 0.2929 sec/batch\n",
      "Epoch 2/20  Iteration 194/3520 Training loss: 2.4486 0.2944 sec/batch\n",
      "Epoch 2/20  Iteration 195/3520 Training loss: 2.4442 0.2936 sec/batch\n",
      "Epoch 2/20  Iteration 196/3520 Training loss: 2.4430 0.2939 sec/batch\n",
      "Epoch 2/20  Iteration 197/3520 Training loss: 2.4415 0.2936 sec/batch\n",
      "Epoch 2/20  Iteration 198/3520 Training loss: 2.4403 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 199/3520 Training loss: 2.4399 0.2945 sec/batch\n",
      "Epoch 2/20  Iteration 200/3520 Training loss: 2.4385 0.2944 sec/batch\n",
      "Validation loss: 2.2948 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 201/3520 Training loss: 2.4373 0.2973 sec/batch\n",
      "Epoch 2/20  Iteration 202/3520 Training loss: 2.4342 0.2965 sec/batch\n",
      "Epoch 2/20  Iteration 203/3520 Training loss: 2.4313 0.2939 sec/batch\n",
      "Epoch 2/20  Iteration 204/3520 Training loss: 2.4286 0.2941 sec/batch\n",
      "Epoch 2/20  Iteration 205/3520 Training loss: 2.4259 0.2953 sec/batch\n",
      "Epoch 2/20  Iteration 206/3520 Training loss: 2.4236 0.2942 sec/batch\n",
      "Epoch 2/20  Iteration 207/3520 Training loss: 2.4213 0.2947 sec/batch\n",
      "Epoch 2/20  Iteration 208/3520 Training loss: 2.4192 0.2947 sec/batch\n",
      "Epoch 2/20  Iteration 209/3520 Training loss: 2.4169 0.2951 sec/batch\n",
      "Epoch 2/20  Iteration 210/3520 Training loss: 2.4177 0.2942 sec/batch\n",
      "Epoch 2/20  Iteration 211/3520 Training loss: 2.4190 0.2955 sec/batch\n",
      "Epoch 2/20  Iteration 212/3520 Training loss: 2.4198 0.2952 sec/batch\n",
      "Epoch 2/20  Iteration 213/3520 Training loss: 2.4197 0.2958 sec/batch\n",
      "Epoch 2/20  Iteration 214/3520 Training loss: 2.4193 0.2947 sec/batch\n",
      "Epoch 2/20  Iteration 215/3520 Training loss: 2.4187 0.2961 sec/batch\n",
      "Epoch 2/20  Iteration 216/3520 Training loss: 2.4174 0.2950 sec/batch\n",
      "Epoch 2/20  Iteration 217/3520 Training loss: 2.4160 0.2942 sec/batch\n",
      "Epoch 2/20  Iteration 218/3520 Training loss: 2.4133 0.2939 sec/batch\n",
      "Epoch 2/20  Iteration 219/3520 Training loss: 2.4116 0.2951 sec/batch\n",
      "Epoch 2/20  Iteration 220/3520 Training loss: 2.4093 0.2942 sec/batch\n",
      "Epoch 2/20  Iteration 221/3520 Training loss: 2.4077 0.2950 sec/batch\n",
      "Epoch 2/20  Iteration 222/3520 Training loss: 2.4053 0.3010 sec/batch\n",
      "Epoch 2/20  Iteration 223/3520 Training loss: 2.4027 0.2946 sec/batch\n",
      "Epoch 2/20  Iteration 224/3520 Training loss: 2.4007 0.2942 sec/batch\n",
      "Epoch 2/20  Iteration 225/3520 Training loss: 2.3988 0.2937 sec/batch\n",
      "Epoch 2/20  Iteration 226/3520 Training loss: 2.3972 0.2958 sec/batch\n",
      "Epoch 2/20  Iteration 227/3520 Training loss: 2.3955 0.2954 sec/batch\n",
      "Epoch 2/20  Iteration 228/3520 Training loss: 2.3939 0.2967 sec/batch\n",
      "Epoch 2/20  Iteration 229/3520 Training loss: 2.3922 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 230/3520 Training loss: 2.3908 0.2937 sec/batch\n",
      "Epoch 2/20  Iteration 231/3520 Training loss: 2.3882 0.2951 sec/batch\n",
      "Epoch 2/20  Iteration 232/3520 Training loss: 2.3860 0.2950 sec/batch\n",
      "Epoch 2/20  Iteration 233/3520 Training loss: 2.3836 0.2946 sec/batch\n",
      "Epoch 2/20  Iteration 234/3520 Training loss: 2.3817 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 235/3520 Training loss: 2.3800 0.2953 sec/batch\n",
      "Epoch 2/20  Iteration 236/3520 Training loss: 2.3785 0.2952 sec/batch\n",
      "Epoch 2/20  Iteration 237/3520 Training loss: 2.3768 0.2941 sec/batch\n",
      "Epoch 2/20  Iteration 238/3520 Training loss: 2.3752 0.2947 sec/batch\n",
      "Epoch 2/20  Iteration 239/3520 Training loss: 2.3733 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 240/3520 Training loss: 2.3712 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 241/3520 Training loss: 2.3691 0.2939 sec/batch\n",
      "Epoch 2/20  Iteration 242/3520 Training loss: 2.3671 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 243/3520 Training loss: 2.3652 0.2928 sec/batch\n",
      "Epoch 2/20  Iteration 244/3520 Training loss: 2.3633 0.2952 sec/batch\n",
      "Epoch 2/20  Iteration 245/3520 Training loss: 2.3613 0.2940 sec/batch\n",
      "Epoch 2/20  Iteration 246/3520 Training loss: 2.3597 0.2944 sec/batch\n",
      "Epoch 2/20  Iteration 247/3520 Training loss: 2.3580 0.2941 sec/batch\n",
      "Epoch 2/20  Iteration 248/3520 Training loss: 2.3562 0.2946 sec/batch\n",
      "Epoch 2/20  Iteration 249/3520 Training loss: 2.3543 0.2945 sec/batch\n",
      "Epoch 2/20  Iteration 250/3520 Training loss: 2.3523 0.2940 sec/batch\n",
      "Epoch 2/20  Iteration 251/3520 Training loss: 2.3502 0.2945 sec/batch\n",
      "Epoch 2/20  Iteration 252/3520 Training loss: 2.3485 0.2951 sec/batch\n",
      "Epoch 2/20  Iteration 253/3520 Training loss: 2.3467 0.2951 sec/batch\n",
      "Epoch 2/20  Iteration 254/3520 Training loss: 2.3447 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 255/3520 Training loss: 2.3428 0.2947 sec/batch\n",
      "Epoch 2/20  Iteration 256/3520 Training loss: 2.3411 0.2950 sec/batch\n",
      "Epoch 2/20  Iteration 257/3520 Training loss: 2.3389 0.2955 sec/batch\n",
      "Epoch 2/20  Iteration 258/3520 Training loss: 2.3373 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 259/3520 Training loss: 2.3352 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 260/3520 Training loss: 2.3337 0.2953 sec/batch\n",
      "Epoch 2/20  Iteration 261/3520 Training loss: 2.3320 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 262/3520 Training loss: 2.3301 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 263/3520 Training loss: 2.3286 0.2943 sec/batch\n",
      "Epoch 2/20  Iteration 264/3520 Training loss: 2.3269 0.2950 sec/batch\n",
      "Epoch 2/20  Iteration 265/3520 Training loss: 2.3254 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 266/3520 Training loss: 2.3239 0.2980 sec/batch\n",
      "Epoch 2/20  Iteration 267/3520 Training loss: 2.3224 0.2975 sec/batch\n",
      "Epoch 2/20  Iteration 268/3520 Training loss: 2.3210 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 269/3520 Training loss: 2.3195 0.2952 sec/batch\n",
      "Epoch 2/20  Iteration 270/3520 Training loss: 2.3178 0.2950 sec/batch\n",
      "Epoch 2/20  Iteration 271/3520 Training loss: 2.3161 0.2951 sec/batch\n",
      "Epoch 2/20  Iteration 272/3520 Training loss: 2.3142 0.2952 sec/batch\n",
      "Epoch 2/20  Iteration 273/3520 Training loss: 2.3131 0.2944 sec/batch\n",
      "Epoch 2/20  Iteration 274/3520 Training loss: 2.3119 0.2977 sec/batch\n",
      "Epoch 2/20  Iteration 275/3520 Training loss: 2.3104 0.3032 sec/batch\n",
      "Epoch 2/20  Iteration 276/3520 Training loss: 2.3085 0.2952 sec/batch\n",
      "Epoch 2/20  Iteration 277/3520 Training loss: 2.3071 0.2944 sec/batch\n",
      "Epoch 2/20  Iteration 278/3520 Training loss: 2.3058 0.2950 sec/batch\n",
      "Epoch 2/20  Iteration 279/3520 Training loss: 2.3042 0.2951 sec/batch\n",
      "Epoch 2/20  Iteration 280/3520 Training loss: 2.3024 0.2940 sec/batch\n",
      "Epoch 2/20  Iteration 281/3520 Training loss: 2.3009 0.2935 sec/batch\n",
      "Epoch 2/20  Iteration 282/3520 Training loss: 2.2995 0.2941 sec/batch\n",
      "Epoch 2/20  Iteration 283/3520 Training loss: 2.2978 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 284/3520 Training loss: 2.2959 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 285/3520 Training loss: 2.2944 0.2934 sec/batch\n",
      "Epoch 2/20  Iteration 286/3520 Training loss: 2.2928 0.2943 sec/batch\n",
      "Epoch 2/20  Iteration 287/3520 Training loss: 2.2910 0.2954 sec/batch\n",
      "Epoch 2/20  Iteration 288/3520 Training loss: 2.2895 0.2939 sec/batch\n",
      "Epoch 2/20  Iteration 289/3520 Training loss: 2.2881 0.2950 sec/batch\n",
      "Epoch 2/20  Iteration 290/3520 Training loss: 2.2865 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 291/3520 Training loss: 2.2849 0.2939 sec/batch\n",
      "Epoch 2/20  Iteration 292/3520 Training loss: 2.2835 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 293/3520 Training loss: 2.2820 0.2949 sec/batch\n",
      "Epoch 2/20  Iteration 294/3520 Training loss: 2.2805 0.2934 sec/batch\n",
      "Epoch 2/20  Iteration 295/3520 Training loss: 2.2790 0.2935 sec/batch\n",
      "Epoch 2/20  Iteration 296/3520 Training loss: 2.2775 0.2939 sec/batch\n",
      "Epoch 2/20  Iteration 297/3520 Training loss: 2.2761 0.2934 sec/batch\n",
      "Epoch 2/20  Iteration 298/3520 Training loss: 2.2747 0.2932 sec/batch\n",
      "Epoch 2/20  Iteration 299/3520 Training loss: 2.2730 0.2929 sec/batch\n",
      "Epoch 2/20  Iteration 300/3520 Training loss: 2.2715 0.2929 sec/batch\n",
      "Epoch 2/20  Iteration 301/3520 Training loss: 2.2702 0.2933 sec/batch\n",
      "Epoch 2/20  Iteration 302/3520 Training loss: 2.2687 0.2941 sec/batch\n",
      "Epoch 2/20  Iteration 303/3520 Training loss: 2.2669 0.2935 sec/batch\n",
      "Epoch 2/20  Iteration 304/3520 Training loss: 2.2655 0.2935 sec/batch\n",
      "Epoch 2/20  Iteration 305/3520 Training loss: 2.2639 0.2956 sec/batch\n",
      "Epoch 2/20  Iteration 306/3520 Training loss: 2.2626 0.2935 sec/batch\n",
      "Epoch 2/20  Iteration 307/3520 Training loss: 2.2611 0.2938 sec/batch\n",
      "Epoch 2/20  Iteration 308/3520 Training loss: 2.2599 0.2938 sec/batch\n",
      "Epoch 2/20  Iteration 309/3520 Training loss: 2.2587 0.2950 sec/batch\n",
      "Epoch 2/20  Iteration 310/3520 Training loss: 2.2573 0.2942 sec/batch\n",
      "Epoch 2/20  Iteration 311/3520 Training loss: 2.2560 0.2943 sec/batch\n",
      "Epoch 2/20  Iteration 312/3520 Training loss: 2.2548 0.2930 sec/batch\n",
      "Epoch 2/20  Iteration 313/3520 Training loss: 2.2536 0.2943 sec/batch\n",
      "Epoch 2/20  Iteration 314/3520 Training loss: 2.2521 0.2944 sec/batch\n",
      "Epoch 2/20  Iteration 315/3520 Training loss: 2.2508 0.2935 sec/batch\n",
      "Epoch 2/20  Iteration 316/3520 Training loss: 2.2496 0.2935 sec/batch\n",
      "Epoch 2/20  Iteration 317/3520 Training loss: 2.2484 0.2932 sec/batch\n",
      "Epoch 2/20  Iteration 318/3520 Training loss: 2.2471 0.2981 sec/batch\n",
      "Epoch 2/20  Iteration 319/3520 Training loss: 2.2457 0.2937 sec/batch\n",
      "Epoch 2/20  Iteration 320/3520 Training loss: 2.2444 0.2940 sec/batch\n",
      "Epoch 2/20  Iteration 321/3520 Training loss: 2.2430 0.2938 sec/batch\n",
      "Epoch 2/20  Iteration 322/3520 Training loss: 2.2418 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 323/3520 Training loss: 2.2405 0.2944 sec/batch\n",
      "Epoch 2/20  Iteration 324/3520 Training loss: 2.2395 0.2938 sec/batch\n",
      "Epoch 2/20  Iteration 325/3520 Training loss: 2.2383 0.2935 sec/batch\n",
      "Epoch 2/20  Iteration 326/3520 Training loss: 2.2373 0.2939 sec/batch\n",
      "Epoch 2/20  Iteration 327/3520 Training loss: 2.2360 0.2945 sec/batch\n",
      "Epoch 2/20  Iteration 328/3520 Training loss: 2.2347 0.2951 sec/batch\n",
      "Epoch 2/20  Iteration 329/3520 Training loss: 2.2334 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 330/3520 Training loss: 2.2321 0.2931 sec/batch\n",
      "Epoch 2/20  Iteration 331/3520 Training loss: 2.2308 0.2936 sec/batch\n",
      "Epoch 2/20  Iteration 332/3520 Training loss: 2.2294 0.2945 sec/batch\n",
      "Epoch 2/20  Iteration 333/3520 Training loss: 2.2281 0.2941 sec/batch\n",
      "Epoch 2/20  Iteration 334/3520 Training loss: 2.2267 0.2942 sec/batch\n",
      "Epoch 2/20  Iteration 335/3520 Training loss: 2.2254 0.2946 sec/batch\n",
      "Epoch 2/20  Iteration 336/3520 Training loss: 2.2243 0.2936 sec/batch\n",
      "Epoch 2/20  Iteration 337/3520 Training loss: 2.2230 0.2945 sec/batch\n",
      "Epoch 2/20  Iteration 338/3520 Training loss: 2.2217 0.3013 sec/batch\n",
      "Epoch 2/20  Iteration 339/3520 Training loss: 2.2206 0.2957 sec/batch\n",
      "Epoch 2/20  Iteration 340/3520 Training loss: 2.2192 0.2954 sec/batch\n",
      "Epoch 2/20  Iteration 341/3520 Training loss: 2.2178 0.2932 sec/batch\n",
      "Epoch 2/20  Iteration 342/3520 Training loss: 2.2164 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 343/3520 Training loss: 2.2151 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 344/3520 Training loss: 2.2138 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 345/3520 Training loss: 2.2126 0.2933 sec/batch\n",
      "Epoch 2/20  Iteration 346/3520 Training loss: 2.2113 0.2930 sec/batch\n",
      "Epoch 2/20  Iteration 347/3520 Training loss: 2.2101 0.2950 sec/batch\n",
      "Epoch 2/20  Iteration 348/3520 Training loss: 2.2088 0.2942 sec/batch\n",
      "Epoch 2/20  Iteration 349/3520 Training loss: 2.2075 0.2935 sec/batch\n",
      "Epoch 2/20  Iteration 350/3520 Training loss: 2.2064 0.2937 sec/batch\n",
      "Epoch 2/20  Iteration 351/3520 Training loss: 2.2051 0.2948 sec/batch\n",
      "Epoch 2/20  Iteration 352/3520 Training loss: 2.2040 0.2947 sec/batch\n",
      "Epoch 3/20  Iteration 353/3520 Training loss: 2.0156 0.2944 sec/batch\n",
      "Epoch 3/20  Iteration 354/3520 Training loss: 1.9762 0.2955 sec/batch\n",
      "Epoch 3/20  Iteration 355/3520 Training loss: 1.9808 0.2970 sec/batch\n",
      "Epoch 3/20  Iteration 356/3520 Training loss: 1.9820 0.2949 sec/batch\n",
      "Epoch 3/20  Iteration 357/3520 Training loss: 1.9845 0.2942 sec/batch\n",
      "Epoch 3/20  Iteration 358/3520 Training loss: 1.9849 0.2950 sec/batch\n",
      "Epoch 3/20  Iteration 359/3520 Training loss: 1.9839 0.2948 sec/batch\n",
      "Epoch 3/20  Iteration 360/3520 Training loss: 1.9867 0.2947 sec/batch\n",
      "Epoch 3/20  Iteration 361/3520 Training loss: 1.9834 0.2949 sec/batch\n",
      "Epoch 3/20  Iteration 362/3520 Training loss: 1.9840 0.2958 sec/batch\n",
      "Epoch 3/20  Iteration 363/3520 Training loss: 1.9826 0.2934 sec/batch\n",
      "Epoch 3/20  Iteration 364/3520 Training loss: 1.9817 0.2928 sec/batch\n",
      "Epoch 3/20  Iteration 365/3520 Training loss: 1.9786 0.2929 sec/batch\n",
      "Epoch 3/20  Iteration 366/3520 Training loss: 1.9786 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 367/3520 Training loss: 1.9763 0.2932 sec/batch\n",
      "Epoch 3/20  Iteration 368/3520 Training loss: 1.9751 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 369/3520 Training loss: 1.9747 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 370/3520 Training loss: 1.9743 0.2944 sec/batch\n",
      "Epoch 3/20  Iteration 371/3520 Training loss: 1.9719 0.2938 sec/batch\n",
      "Epoch 3/20  Iteration 372/3520 Training loss: 1.9719 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 373/3520 Training loss: 1.9716 0.2938 sec/batch\n",
      "Epoch 3/20  Iteration 374/3520 Training loss: 1.9707 0.2939 sec/batch\n",
      "Epoch 3/20  Iteration 375/3520 Training loss: 1.9708 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 376/3520 Training loss: 1.9701 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 377/3520 Training loss: 1.9696 0.2945 sec/batch\n",
      "Epoch 3/20  Iteration 378/3520 Training loss: 1.9679 0.2932 sec/batch\n",
      "Epoch 3/20  Iteration 379/3520 Training loss: 1.9655 0.2929 sec/batch\n",
      "Epoch 3/20  Iteration 380/3520 Training loss: 1.9644 0.2951 sec/batch\n",
      "Epoch 3/20  Iteration 381/3520 Training loss: 1.9629 0.2948 sec/batch\n",
      "Epoch 3/20  Iteration 382/3520 Training loss: 1.9616 0.2949 sec/batch\n",
      "Epoch 3/20  Iteration 383/3520 Training loss: 1.9610 0.2944 sec/batch\n",
      "Epoch 3/20  Iteration 384/3520 Training loss: 1.9599 0.2938 sec/batch\n",
      "Epoch 3/20  Iteration 385/3520 Training loss: 1.9594 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 386/3520 Training loss: 1.9589 0.2961 sec/batch\n",
      "Epoch 3/20  Iteration 387/3520 Training loss: 1.9577 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 388/3520 Training loss: 1.9571 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 389/3520 Training loss: 1.9555 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 390/3520 Training loss: 1.9555 0.2947 sec/batch\n",
      "Epoch 3/20  Iteration 391/3520 Training loss: 1.9559 0.3067 sec/batch\n",
      "Epoch 3/20  Iteration 392/3520 Training loss: 1.9551 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 393/3520 Training loss: 1.9545 0.2941 sec/batch\n",
      "Epoch 3/20  Iteration 394/3520 Training loss: 1.9525 0.2941 sec/batch\n",
      "Epoch 3/20  Iteration 395/3520 Training loss: 1.9520 0.2934 sec/batch\n",
      "Epoch 3/20  Iteration 396/3520 Training loss: 1.9505 0.2947 sec/batch\n",
      "Epoch 3/20  Iteration 397/3520 Training loss: 1.9496 0.2928 sec/batch\n",
      "Epoch 3/20  Iteration 398/3520 Training loss: 1.9483 0.2931 sec/batch\n",
      "Epoch 3/20  Iteration 399/3520 Training loss: 1.9473 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 400/3520 Training loss: 1.9464 0.2940 sec/batch\n",
      "Validation loss: 1.79137 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 401/3520 Training loss: 1.9465 0.2972 sec/batch\n",
      "Epoch 3/20  Iteration 402/3520 Training loss: 1.9459 0.2963 sec/batch\n",
      "Epoch 3/20  Iteration 403/3520 Training loss: 1.9449 0.2943 sec/batch\n",
      "Epoch 3/20  Iteration 404/3520 Training loss: 1.9444 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 405/3520 Training loss: 1.9441 0.2949 sec/batch\n",
      "Epoch 3/20  Iteration 406/3520 Training loss: 1.9435 0.2948 sec/batch\n",
      "Epoch 3/20  Iteration 407/3520 Training loss: 1.9422 0.3048 sec/batch\n",
      "Epoch 3/20  Iteration 408/3520 Training loss: 1.9409 0.2957 sec/batch\n",
      "Epoch 3/20  Iteration 409/3520 Training loss: 1.9398 0.2943 sec/batch\n",
      "Epoch 3/20  Iteration 410/3520 Training loss: 1.9389 0.2944 sec/batch\n",
      "Epoch 3/20  Iteration 411/3520 Training loss: 1.9377 0.2943 sec/batch\n",
      "Epoch 3/20  Iteration 412/3520 Training loss: 1.9375 0.2946 sec/batch\n",
      "Epoch 3/20  Iteration 413/3520 Training loss: 1.9365 0.2932 sec/batch\n",
      "Epoch 3/20  Iteration 414/3520 Training loss: 1.9359 0.2929 sec/batch\n",
      "Epoch 3/20  Iteration 415/3520 Training loss: 1.9350 0.2942 sec/batch\n",
      "Epoch 3/20  Iteration 416/3520 Training loss: 1.9343 0.2939 sec/batch\n",
      "Epoch 3/20  Iteration 417/3520 Training loss: 1.9332 0.2932 sec/batch\n",
      "Epoch 3/20  Iteration 418/3520 Training loss: 1.9323 0.2942 sec/batch\n",
      "Epoch 3/20  Iteration 419/3520 Training loss: 1.9315 0.2932 sec/batch\n",
      "Epoch 3/20  Iteration 420/3520 Training loss: 1.9303 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 421/3520 Training loss: 1.9295 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 422/3520 Training loss: 1.9290 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 423/3520 Training loss: 1.9281 0.2934 sec/batch\n",
      "Epoch 3/20  Iteration 424/3520 Training loss: 1.9273 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 425/3520 Training loss: 1.9263 0.2939 sec/batch\n",
      "Epoch 3/20  Iteration 426/3520 Training loss: 1.9251 0.2943 sec/batch\n",
      "Epoch 3/20  Iteration 427/3520 Training loss: 1.9243 0.2946 sec/batch\n",
      "Epoch 3/20  Iteration 428/3520 Training loss: 1.9238 0.2940 sec/batch\n",
      "Epoch 3/20  Iteration 429/3520 Training loss: 1.9231 0.2944 sec/batch\n",
      "Epoch 3/20  Iteration 430/3520 Training loss: 1.9225 0.2939 sec/batch\n",
      "Epoch 3/20  Iteration 431/3520 Training loss: 1.9215 0.2958 sec/batch\n",
      "Epoch 3/20  Iteration 432/3520 Training loss: 1.9208 0.2945 sec/batch\n",
      "Epoch 3/20  Iteration 433/3520 Training loss: 1.9195 0.2940 sec/batch\n",
      "Epoch 3/20  Iteration 434/3520 Training loss: 1.9189 0.2949 sec/batch\n",
      "Epoch 3/20  Iteration 435/3520 Training loss: 1.9180 0.2931 sec/batch\n",
      "Epoch 3/20  Iteration 436/3520 Training loss: 1.9176 0.2945 sec/batch\n",
      "Epoch 3/20  Iteration 437/3520 Training loss: 1.9169 0.2951 sec/batch\n",
      "Epoch 3/20  Iteration 438/3520 Training loss: 1.9159 0.2950 sec/batch\n",
      "Epoch 3/20  Iteration 439/3520 Training loss: 1.9154 0.2956 sec/batch\n",
      "Epoch 3/20  Iteration 440/3520 Training loss: 1.9146 0.2947 sec/batch\n",
      "Epoch 3/20  Iteration 441/3520 Training loss: 1.9140 0.2939 sec/batch\n",
      "Epoch 3/20  Iteration 442/3520 Training loss: 1.9132 0.2948 sec/batch\n",
      "Epoch 3/20  Iteration 443/3520 Training loss: 1.9127 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 444/3520 Training loss: 1.9123 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 445/3520 Training loss: 1.9115 0.2942 sec/batch\n",
      "Epoch 3/20  Iteration 446/3520 Training loss: 1.9108 0.2938 sec/batch\n",
      "Epoch 3/20  Iteration 447/3520 Training loss: 1.9100 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 448/3520 Training loss: 1.9089 0.2940 sec/batch\n",
      "Epoch 3/20  Iteration 449/3520 Training loss: 1.9085 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 450/3520 Training loss: 1.9079 0.2930 sec/batch\n",
      "Epoch 3/20  Iteration 451/3520 Training loss: 1.9074 0.2927 sec/batch\n",
      "Epoch 3/20  Iteration 452/3520 Training loss: 1.9064 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 453/3520 Training loss: 1.9057 0.2949 sec/batch\n",
      "Epoch 3/20  Iteration 454/3520 Training loss: 1.9052 0.2944 sec/batch\n",
      "Epoch 3/20  Iteration 455/3520 Training loss: 1.9042 0.2934 sec/batch\n",
      "Epoch 3/20  Iteration 456/3520 Training loss: 1.9032 0.2942 sec/batch\n",
      "Epoch 3/20  Iteration 457/3520 Training loss: 1.9023 0.2939 sec/batch\n",
      "Epoch 3/20  Iteration 458/3520 Training loss: 1.9018 0.2928 sec/batch\n",
      "Epoch 3/20  Iteration 459/3520 Training loss: 1.9009 0.3047 sec/batch\n",
      "Epoch 3/20  Iteration 460/3520 Training loss: 1.8998 0.2948 sec/batch\n",
      "Epoch 3/20  Iteration 461/3520 Training loss: 1.8992 0.2951 sec/batch\n",
      "Epoch 3/20  Iteration 462/3520 Training loss: 1.8984 0.2950 sec/batch\n",
      "Epoch 3/20  Iteration 463/3520 Training loss: 1.8976 0.2940 sec/batch\n",
      "Epoch 3/20  Iteration 464/3520 Training loss: 1.8970 0.2940 sec/batch\n",
      "Epoch 3/20  Iteration 465/3520 Training loss: 1.8966 0.2954 sec/batch\n",
      "Epoch 3/20  Iteration 466/3520 Training loss: 1.8959 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 467/3520 Training loss: 1.8950 0.2933 sec/batch\n",
      "Epoch 3/20  Iteration 468/3520 Training loss: 1.8943 0.2934 sec/batch\n",
      "Epoch 3/20  Iteration 469/3520 Training loss: 1.8935 0.2931 sec/batch\n",
      "Epoch 3/20  Iteration 470/3520 Training loss: 1.8929 0.2933 sec/batch\n",
      "Epoch 3/20  Iteration 471/3520 Training loss: 1.8921 0.2932 sec/batch\n",
      "Epoch 3/20  Iteration 472/3520 Training loss: 1.8914 0.2947 sec/batch\n",
      "Epoch 3/20  Iteration 473/3520 Training loss: 1.8908 0.2948 sec/batch\n",
      "Epoch 3/20  Iteration 474/3520 Training loss: 1.8902 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 475/3520 Training loss: 1.8893 0.2944 sec/batch\n",
      "Epoch 3/20  Iteration 476/3520 Training loss: 1.8885 0.2944 sec/batch\n",
      "Epoch 3/20  Iteration 477/3520 Training loss: 1.8878 0.2949 sec/batch\n",
      "Epoch 3/20  Iteration 478/3520 Training loss: 1.8869 0.2938 sec/batch\n",
      "Epoch 3/20  Iteration 479/3520 Training loss: 1.8859 0.2948 sec/batch\n",
      "Epoch 3/20  Iteration 480/3520 Training loss: 1.8852 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 481/3520 Training loss: 1.8844 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 482/3520 Training loss: 1.8839 0.2943 sec/batch\n",
      "Epoch 3/20  Iteration 483/3520 Training loss: 1.8832 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 484/3520 Training loss: 1.8827 0.2940 sec/batch\n",
      "Epoch 3/20  Iteration 485/3520 Training loss: 1.8822 0.2951 sec/batch\n",
      "Epoch 3/20  Iteration 486/3520 Training loss: 1.8815 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 487/3520 Training loss: 1.8809 0.2934 sec/batch\n",
      "Epoch 3/20  Iteration 488/3520 Training loss: 1.8803 0.2933 sec/batch\n",
      "Epoch 3/20  Iteration 489/3520 Training loss: 1.8797 0.2938 sec/batch\n",
      "Epoch 3/20  Iteration 490/3520 Training loss: 1.8790 0.2930 sec/batch\n",
      "Epoch 3/20  Iteration 491/3520 Training loss: 1.8785 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 492/3520 Training loss: 1.8780 0.2947 sec/batch\n",
      "Epoch 3/20  Iteration 493/3520 Training loss: 1.8773 0.2944 sec/batch\n",
      "Epoch 3/20  Iteration 494/3520 Training loss: 1.8766 0.2941 sec/batch\n",
      "Epoch 3/20  Iteration 495/3520 Training loss: 1.8758 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 496/3520 Training loss: 1.8751 0.2951 sec/batch\n",
      "Epoch 3/20  Iteration 497/3520 Training loss: 1.8744 0.2933 sec/batch\n",
      "Epoch 3/20  Iteration 498/3520 Training loss: 1.8738 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 499/3520 Training loss: 1.8730 0.2927 sec/batch\n",
      "Epoch 3/20  Iteration 500/3520 Training loss: 1.8725 0.2934 sec/batch\n",
      "Epoch 3/20  Iteration 501/3520 Training loss: 1.8718 0.2946 sec/batch\n",
      "Epoch 3/20  Iteration 502/3520 Training loss: 1.8714 0.2940 sec/batch\n",
      "Epoch 3/20  Iteration 503/3520 Training loss: 1.8707 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 504/3520 Training loss: 1.8700 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 505/3520 Training loss: 1.8693 0.2938 sec/batch\n",
      "Epoch 3/20  Iteration 506/3520 Training loss: 1.8685 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 507/3520 Training loss: 1.8679 0.2939 sec/batch\n",
      "Epoch 3/20  Iteration 508/3520 Training loss: 1.8672 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 509/3520 Training loss: 1.8664 0.2933 sec/batch\n",
      "Epoch 3/20  Iteration 510/3520 Training loss: 1.8658 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 511/3520 Training loss: 1.8652 0.2941 sec/batch\n",
      "Epoch 3/20  Iteration 512/3520 Training loss: 1.8647 0.2934 sec/batch\n",
      "Epoch 3/20  Iteration 513/3520 Training loss: 1.8640 0.2930 sec/batch\n",
      "Epoch 3/20  Iteration 514/3520 Training loss: 1.8634 0.2944 sec/batch\n",
      "Epoch 3/20  Iteration 515/3520 Training loss: 1.8629 0.2932 sec/batch\n",
      "Epoch 3/20  Iteration 516/3520 Training loss: 1.8621 0.2946 sec/batch\n",
      "Epoch 3/20  Iteration 517/3520 Training loss: 1.8612 0.2938 sec/batch\n",
      "Epoch 3/20  Iteration 518/3520 Training loss: 1.8605 0.2937 sec/batch\n",
      "Epoch 3/20  Iteration 519/3520 Training loss: 1.8598 0.2938 sec/batch\n",
      "Epoch 3/20  Iteration 520/3520 Training loss: 1.8592 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 521/3520 Training loss: 1.8586 0.2935 sec/batch\n",
      "Epoch 3/20  Iteration 522/3520 Training loss: 1.8580 0.2938 sec/batch\n",
      "Epoch 3/20  Iteration 523/3520 Training loss: 1.8575 0.2944 sec/batch\n",
      "Epoch 3/20  Iteration 524/3520 Training loss: 1.8568 0.2938 sec/batch\n",
      "Epoch 3/20  Iteration 525/3520 Training loss: 1.8561 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 526/3520 Training loss: 1.8556 0.2936 sec/batch\n",
      "Epoch 3/20  Iteration 527/3520 Training loss: 1.8549 0.2931 sec/batch\n",
      "Epoch 3/20  Iteration 528/3520 Training loss: 1.8545 0.2932 sec/batch\n",
      "Epoch 4/20  Iteration 529/3520 Training loss: 1.7549 0.2931 sec/batch\n",
      "Epoch 4/20  Iteration 530/3520 Training loss: 1.7195 0.2954 sec/batch\n",
      "Epoch 4/20  Iteration 531/3520 Training loss: 1.7263 0.2955 sec/batch\n",
      "Epoch 4/20  Iteration 532/3520 Training loss: 1.7290 0.2948 sec/batch\n",
      "Epoch 4/20  Iteration 533/3520 Training loss: 1.7326 0.2953 sec/batch\n",
      "Epoch 4/20  Iteration 534/3520 Training loss: 1.7341 0.2992 sec/batch\n",
      "Epoch 4/20  Iteration 535/3520 Training loss: 1.7341 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 536/3520 Training loss: 1.7373 0.2938 sec/batch\n",
      "Epoch 4/20  Iteration 537/3520 Training loss: 1.7345 0.2945 sec/batch\n",
      "Epoch 4/20  Iteration 538/3520 Training loss: 1.7353 0.2947 sec/batch\n",
      "Epoch 4/20  Iteration 539/3520 Training loss: 1.7362 0.2946 sec/batch\n",
      "Epoch 4/20  Iteration 540/3520 Training loss: 1.7374 0.2945 sec/batch\n",
      "Epoch 4/20  Iteration 541/3520 Training loss: 1.7356 0.2940 sec/batch\n",
      "Epoch 4/20  Iteration 542/3520 Training loss: 1.7359 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 543/3520 Training loss: 1.7345 0.2944 sec/batch\n",
      "Epoch 4/20  Iteration 544/3520 Training loss: 1.7341 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 545/3520 Training loss: 1.7338 0.2943 sec/batch\n",
      "Epoch 4/20  Iteration 546/3520 Training loss: 1.7335 0.2926 sec/batch\n",
      "Epoch 4/20  Iteration 547/3520 Training loss: 1.7317 0.2951 sec/batch\n",
      "Epoch 4/20  Iteration 548/3520 Training loss: 1.7331 0.2953 sec/batch\n",
      "Epoch 4/20  Iteration 549/3520 Training loss: 1.7331 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 550/3520 Training loss: 1.7322 0.2945 sec/batch\n",
      "Epoch 4/20  Iteration 551/3520 Training loss: 1.7331 0.2948 sec/batch\n",
      "Epoch 4/20  Iteration 552/3520 Training loss: 1.7323 0.2935 sec/batch\n",
      "Epoch 4/20  Iteration 553/3520 Training loss: 1.7322 0.2936 sec/batch\n",
      "Epoch 4/20  Iteration 554/3520 Training loss: 1.7306 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 555/3520 Training loss: 1.7291 0.2943 sec/batch\n",
      "Epoch 4/20  Iteration 556/3520 Training loss: 1.7287 0.2936 sec/batch\n",
      "Epoch 4/20  Iteration 557/3520 Training loss: 1.7274 0.2942 sec/batch\n",
      "Epoch 4/20  Iteration 558/3520 Training loss: 1.7265 0.2940 sec/batch\n",
      "Epoch 4/20  Iteration 559/3520 Training loss: 1.7260 0.2949 sec/batch\n",
      "Epoch 4/20  Iteration 560/3520 Training loss: 1.7253 0.2939 sec/batch\n",
      "Epoch 4/20  Iteration 561/3520 Training loss: 1.7255 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 562/3520 Training loss: 1.7253 0.2935 sec/batch\n",
      "Epoch 4/20  Iteration 563/3520 Training loss: 1.7248 0.2947 sec/batch\n",
      "Epoch 4/20  Iteration 564/3520 Training loss: 1.7249 0.2950 sec/batch\n",
      "Epoch 4/20  Iteration 565/3520 Training loss: 1.7238 0.2951 sec/batch\n",
      "Epoch 4/20  Iteration 566/3520 Training loss: 1.7244 0.2948 sec/batch\n",
      "Epoch 4/20  Iteration 567/3520 Training loss: 1.7253 0.2947 sec/batch\n",
      "Epoch 4/20  Iteration 568/3520 Training loss: 1.7250 0.2936 sec/batch\n",
      "Epoch 4/20  Iteration 569/3520 Training loss: 1.7245 0.2950 sec/batch\n",
      "Epoch 4/20  Iteration 570/3520 Training loss: 1.7228 0.2960 sec/batch\n",
      "Epoch 4/20  Iteration 571/3520 Training loss: 1.7228 0.2948 sec/batch\n",
      "Epoch 4/20  Iteration 572/3520 Training loss: 1.7216 0.2941 sec/batch\n",
      "Epoch 4/20  Iteration 573/3520 Training loss: 1.7210 0.2948 sec/batch\n",
      "Epoch 4/20  Iteration 574/3520 Training loss: 1.7202 0.2941 sec/batch\n",
      "Epoch 4/20  Iteration 575/3520 Training loss: 1.7197 0.2949 sec/batch\n",
      "Epoch 4/20  Iteration 576/3520 Training loss: 1.7194 0.2941 sec/batch\n",
      "Epoch 4/20  Iteration 577/3520 Training loss: 1.7192 0.2943 sec/batch\n",
      "Epoch 4/20  Iteration 578/3520 Training loss: 1.7188 0.2935 sec/batch\n",
      "Epoch 4/20  Iteration 579/3520 Training loss: 1.7185 0.2932 sec/batch\n",
      "Epoch 4/20  Iteration 580/3520 Training loss: 1.7182 0.2946 sec/batch\n",
      "Epoch 4/20  Iteration 581/3520 Training loss: 1.7183 0.2945 sec/batch\n",
      "Epoch 4/20  Iteration 582/3520 Training loss: 1.7179 0.2935 sec/batch\n",
      "Epoch 4/20  Iteration 583/3520 Training loss: 1.7169 0.2933 sec/batch\n",
      "Epoch 4/20  Iteration 584/3520 Training loss: 1.7161 0.2946 sec/batch\n",
      "Epoch 4/20  Iteration 585/3520 Training loss: 1.7154 0.2943 sec/batch\n",
      "Epoch 4/20  Iteration 586/3520 Training loss: 1.7147 0.2936 sec/batch\n",
      "Epoch 4/20  Iteration 587/3520 Training loss: 1.7139 0.2944 sec/batch\n",
      "Epoch 4/20  Iteration 588/3520 Training loss: 1.7143 0.2941 sec/batch\n",
      "Epoch 4/20  Iteration 589/3520 Training loss: 1.7135 0.2936 sec/batch\n",
      "Epoch 4/20  Iteration 590/3520 Training loss: 1.7132 0.2945 sec/batch\n",
      "Epoch 4/20  Iteration 591/3520 Training loss: 1.7124 0.2953 sec/batch\n",
      "Epoch 4/20  Iteration 592/3520 Training loss: 1.7121 0.2964 sec/batch\n",
      "Epoch 4/20  Iteration 593/3520 Training loss: 1.7111 0.2944 sec/batch\n",
      "Epoch 4/20  Iteration 594/3520 Training loss: 1.7106 0.3053 sec/batch\n",
      "Epoch 4/20  Iteration 595/3520 Training loss: 1.7100 0.2941 sec/batch\n",
      "Epoch 4/20  Iteration 596/3520 Training loss: 1.7091 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 597/3520 Training loss: 1.7084 0.2944 sec/batch\n",
      "Epoch 4/20  Iteration 598/3520 Training loss: 1.7082 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 599/3520 Training loss: 1.7076 0.2935 sec/batch\n",
      "Epoch 4/20  Iteration 600/3520 Training loss: 1.7071 0.2938 sec/batch\n",
      "Validation loss: 1.54666 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 601/3520 Training loss: 1.7072 0.3064 sec/batch\n",
      "Epoch 4/20  Iteration 602/3520 Training loss: 1.7064 0.2974 sec/batch\n",
      "Epoch 4/20  Iteration 603/3520 Training loss: 1.7060 0.2940 sec/batch\n",
      "Epoch 4/20  Iteration 604/3520 Training loss: 1.7058 0.2933 sec/batch\n",
      "Epoch 4/20  Iteration 605/3520 Training loss: 1.7056 0.2938 sec/batch\n",
      "Epoch 4/20  Iteration 606/3520 Training loss: 1.7056 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 607/3520 Training loss: 1.7049 0.2940 sec/batch\n",
      "Epoch 4/20  Iteration 608/3520 Training loss: 1.7046 0.2936 sec/batch\n",
      "Epoch 4/20  Iteration 609/3520 Training loss: 1.7036 0.2936 sec/batch\n",
      "Epoch 4/20  Iteration 610/3520 Training loss: 1.7035 0.2936 sec/batch\n",
      "Epoch 4/20  Iteration 611/3520 Training loss: 1.7028 0.2933 sec/batch\n",
      "Epoch 4/20  Iteration 612/3520 Training loss: 1.7026 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 613/3520 Training loss: 1.7021 0.2942 sec/batch\n",
      "Epoch 4/20  Iteration 614/3520 Training loss: 1.7014 0.2943 sec/batch\n",
      "Epoch 4/20  Iteration 615/3520 Training loss: 1.7013 0.2952 sec/batch\n",
      "Epoch 4/20  Iteration 616/3520 Training loss: 1.7009 0.2953 sec/batch\n",
      "Epoch 4/20  Iteration 617/3520 Training loss: 1.7007 0.2953 sec/batch\n",
      "Epoch 4/20  Iteration 618/3520 Training loss: 1.7001 0.2950 sec/batch\n",
      "Epoch 4/20  Iteration 619/3520 Training loss: 1.6998 0.2953 sec/batch\n",
      "Epoch 4/20  Iteration 620/3520 Training loss: 1.6996 0.2951 sec/batch\n",
      "Epoch 4/20  Iteration 621/3520 Training loss: 1.6992 0.2942 sec/batch\n",
      "Epoch 4/20  Iteration 622/3520 Training loss: 1.6985 0.2942 sec/batch\n",
      "Epoch 4/20  Iteration 623/3520 Training loss: 1.6980 0.2946 sec/batch\n",
      "Epoch 4/20  Iteration 624/3520 Training loss: 1.6973 0.2935 sec/batch\n",
      "Epoch 4/20  Iteration 625/3520 Training loss: 1.6973 0.2948 sec/batch\n",
      "Epoch 4/20  Iteration 626/3520 Training loss: 1.6969 0.2938 sec/batch\n",
      "Epoch 4/20  Iteration 627/3520 Training loss: 1.6967 0.3026 sec/batch\n",
      "Epoch 4/20  Iteration 628/3520 Training loss: 1.6960 0.2951 sec/batch\n",
      "Epoch 4/20  Iteration 629/3520 Training loss: 1.6955 0.2951 sec/batch\n",
      "Epoch 4/20  Iteration 630/3520 Training loss: 1.6951 0.2951 sec/batch\n",
      "Epoch 4/20  Iteration 631/3520 Training loss: 1.6945 0.2948 sec/batch\n",
      "Epoch 4/20  Iteration 632/3520 Training loss: 1.6937 0.2941 sec/batch\n",
      "Epoch 4/20  Iteration 633/3520 Training loss: 1.6930 0.2953 sec/batch\n",
      "Epoch 4/20  Iteration 634/3520 Training loss: 1.6928 0.2941 sec/batch\n",
      "Epoch 4/20  Iteration 635/3520 Training loss: 1.6922 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 636/3520 Training loss: 1.6914 0.2939 sec/batch\n",
      "Epoch 4/20  Iteration 637/3520 Training loss: 1.6910 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 638/3520 Training loss: 1.6906 0.2942 sec/batch\n",
      "Epoch 4/20  Iteration 639/3520 Training loss: 1.6901 0.2938 sec/batch\n",
      "Epoch 4/20  Iteration 640/3520 Training loss: 1.6898 0.2957 sec/batch\n",
      "Epoch 4/20  Iteration 641/3520 Training loss: 1.6895 0.3079 sec/batch\n",
      "Epoch 4/20  Iteration 642/3520 Training loss: 1.6891 0.2943 sec/batch\n",
      "Epoch 4/20  Iteration 643/3520 Training loss: 1.6885 0.2947 sec/batch\n",
      "Epoch 4/20  Iteration 644/3520 Training loss: 1.6881 0.2944 sec/batch\n",
      "Epoch 4/20  Iteration 645/3520 Training loss: 1.6875 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 646/3520 Training loss: 1.6873 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 647/3520 Training loss: 1.6868 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 648/3520 Training loss: 1.6864 0.2939 sec/batch\n",
      "Epoch 4/20  Iteration 649/3520 Training loss: 1.6860 0.2951 sec/batch\n",
      "Epoch 4/20  Iteration 650/3520 Training loss: 1.6856 0.2944 sec/batch\n",
      "Epoch 4/20  Iteration 651/3520 Training loss: 1.6852 0.2949 sec/batch\n",
      "Epoch 4/20  Iteration 652/3520 Training loss: 1.6848 0.2938 sec/batch\n",
      "Epoch 4/20  Iteration 653/3520 Training loss: 1.6844 0.2941 sec/batch\n",
      "Epoch 4/20  Iteration 654/3520 Training loss: 1.6837 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 655/3520 Training loss: 1.6830 0.2944 sec/batch\n",
      "Epoch 4/20  Iteration 656/3520 Training loss: 1.6826 0.2944 sec/batch\n",
      "Epoch 4/20  Iteration 657/3520 Training loss: 1.6822 0.2933 sec/batch\n",
      "Epoch 4/20  Iteration 658/3520 Training loss: 1.6820 0.2936 sec/batch\n",
      "Epoch 4/20  Iteration 659/3520 Training loss: 1.6815 0.2946 sec/batch\n",
      "Epoch 4/20  Iteration 660/3520 Training loss: 1.6814 0.2936 sec/batch\n",
      "Epoch 4/20  Iteration 661/3520 Training loss: 1.6811 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 662/3520 Training loss: 1.6807 0.2943 sec/batch\n",
      "Epoch 4/20  Iteration 663/3520 Training loss: 1.6804 0.2941 sec/batch\n",
      "Epoch 4/20  Iteration 664/3520 Training loss: 1.6800 0.3046 sec/batch\n",
      "Epoch 4/20  Iteration 665/3520 Training loss: 1.6797 0.2942 sec/batch\n",
      "Epoch 4/20  Iteration 666/3520 Training loss: 1.6793 0.2945 sec/batch\n",
      "Epoch 4/20  Iteration 667/3520 Training loss: 1.6791 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 668/3520 Training loss: 1.6788 0.2948 sec/batch\n",
      "Epoch 4/20  Iteration 669/3520 Training loss: 1.6785 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 670/3520 Training loss: 1.6781 0.2951 sec/batch\n",
      "Epoch 4/20  Iteration 671/3520 Training loss: 1.6775 0.2991 sec/batch\n",
      "Epoch 4/20  Iteration 672/3520 Training loss: 1.6771 0.2938 sec/batch\n",
      "Epoch 4/20  Iteration 673/3520 Training loss: 1.6768 0.2950 sec/batch\n",
      "Epoch 4/20  Iteration 674/3520 Training loss: 1.6764 0.2930 sec/batch\n",
      "Epoch 4/20  Iteration 675/3520 Training loss: 1.6760 0.2935 sec/batch\n",
      "Epoch 4/20  Iteration 676/3520 Training loss: 1.6757 0.2938 sec/batch\n",
      "Epoch 4/20  Iteration 677/3520 Training loss: 1.6753 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 678/3520 Training loss: 1.6751 0.2946 sec/batch\n",
      "Epoch 4/20  Iteration 679/3520 Training loss: 1.6747 0.2950 sec/batch\n",
      "Epoch 4/20  Iteration 680/3520 Training loss: 1.6742 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 681/3520 Training loss: 1.6738 0.2945 sec/batch\n",
      "Epoch 4/20  Iteration 682/3520 Training loss: 1.6732 0.2931 sec/batch\n",
      "Epoch 4/20  Iteration 683/3520 Training loss: 1.6728 0.2935 sec/batch\n",
      "Epoch 4/20  Iteration 684/3520 Training loss: 1.6724 0.2957 sec/batch\n",
      "Epoch 4/20  Iteration 685/3520 Training loss: 1.6718 0.2976 sec/batch\n",
      "Epoch 4/20  Iteration 686/3520 Training loss: 1.6714 0.2938 sec/batch\n",
      "Epoch 4/20  Iteration 687/3520 Training loss: 1.6712 0.2938 sec/batch\n",
      "Epoch 4/20  Iteration 688/3520 Training loss: 1.6709 0.2946 sec/batch\n",
      "Epoch 4/20  Iteration 689/3520 Training loss: 1.6703 0.2942 sec/batch\n",
      "Epoch 4/20  Iteration 690/3520 Training loss: 1.6699 0.2954 sec/batch\n",
      "Epoch 4/20  Iteration 691/3520 Training loss: 1.6696 0.2942 sec/batch\n",
      "Epoch 4/20  Iteration 692/3520 Training loss: 1.6691 0.2953 sec/batch\n",
      "Epoch 4/20  Iteration 693/3520 Training loss: 1.6684 0.2945 sec/batch\n",
      "Epoch 4/20  Iteration 694/3520 Training loss: 1.6679 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 695/3520 Training loss: 1.6674 0.2926 sec/batch\n",
      "Epoch 4/20  Iteration 696/3520 Training loss: 1.6671 0.2935 sec/batch\n",
      "Epoch 4/20  Iteration 697/3520 Training loss: 1.6668 0.2944 sec/batch\n",
      "Epoch 4/20  Iteration 698/3520 Training loss: 1.6664 0.2940 sec/batch\n",
      "Epoch 4/20  Iteration 699/3520 Training loss: 1.6661 0.2937 sec/batch\n",
      "Epoch 4/20  Iteration 700/3520 Training loss: 1.6657 0.2942 sec/batch\n",
      "Epoch 4/20  Iteration 701/3520 Training loss: 1.6653 0.2958 sec/batch\n",
      "Epoch 4/20  Iteration 702/3520 Training loss: 1.6651 0.2934 sec/batch\n",
      "Epoch 4/20  Iteration 703/3520 Training loss: 1.6647 0.2942 sec/batch\n",
      "Epoch 4/20  Iteration 704/3520 Training loss: 1.6646 0.2956 sec/batch\n",
      "Epoch 5/20  Iteration 705/3520 Training loss: 1.6243 0.2946 sec/batch\n",
      "Epoch 5/20  Iteration 706/3520 Training loss: 1.5812 0.2946 sec/batch\n",
      "Epoch 5/20  Iteration 707/3520 Training loss: 1.5868 0.2949 sec/batch\n",
      "Epoch 5/20  Iteration 708/3520 Training loss: 1.5895 0.2940 sec/batch\n",
      "Epoch 5/20  Iteration 709/3520 Training loss: 1.5908 0.2942 sec/batch\n",
      "Epoch 5/20  Iteration 710/3520 Training loss: 1.5916 0.2938 sec/batch\n",
      "Epoch 5/20  Iteration 711/3520 Training loss: 1.5903 0.2924 sec/batch\n",
      "Epoch 5/20  Iteration 712/3520 Training loss: 1.5930 0.2933 sec/batch\n",
      "Epoch 5/20  Iteration 713/3520 Training loss: 1.5903 0.2935 sec/batch\n",
      "Epoch 5/20  Iteration 714/3520 Training loss: 1.5908 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 715/3520 Training loss: 1.5927 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 716/3520 Training loss: 1.5930 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 717/3520 Training loss: 1.5913 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 718/3520 Training loss: 1.5918 0.2941 sec/batch\n",
      "Epoch 5/20  Iteration 719/3520 Training loss: 1.5906 0.2944 sec/batch\n",
      "Epoch 5/20  Iteration 720/3520 Training loss: 1.5907 0.2946 sec/batch\n",
      "Epoch 5/20  Iteration 721/3520 Training loss: 1.5902 0.2939 sec/batch\n",
      "Epoch 5/20  Iteration 722/3520 Training loss: 1.5906 0.3032 sec/batch\n",
      "Epoch 5/20  Iteration 723/3520 Training loss: 1.5894 0.2948 sec/batch\n",
      "Epoch 5/20  Iteration 724/3520 Training loss: 1.5906 0.2945 sec/batch\n",
      "Epoch 5/20  Iteration 725/3520 Training loss: 1.5908 0.2934 sec/batch\n",
      "Epoch 5/20  Iteration 726/3520 Training loss: 1.5896 0.2935 sec/batch\n",
      "Epoch 5/20  Iteration 727/3520 Training loss: 1.5907 0.2937 sec/batch\n",
      "Epoch 5/20  Iteration 728/3520 Training loss: 1.5908 0.2934 sec/batch\n",
      "Epoch 5/20  Iteration 729/3520 Training loss: 1.5913 0.2945 sec/batch\n",
      "Epoch 5/20  Iteration 730/3520 Training loss: 1.5903 0.2945 sec/batch\n",
      "Epoch 5/20  Iteration 731/3520 Training loss: 1.5890 0.2939 sec/batch\n",
      "Epoch 5/20  Iteration 732/3520 Training loss: 1.5887 0.2950 sec/batch\n",
      "Epoch 5/20  Iteration 733/3520 Training loss: 1.5875 0.2941 sec/batch\n",
      "Epoch 5/20  Iteration 734/3520 Training loss: 1.5873 0.2948 sec/batch\n",
      "Epoch 5/20  Iteration 735/3520 Training loss: 1.5869 0.2952 sec/batch\n",
      "Epoch 5/20  Iteration 736/3520 Training loss: 1.5865 0.2942 sec/batch\n",
      "Epoch 5/20  Iteration 737/3520 Training loss: 1.5869 0.2944 sec/batch\n",
      "Epoch 5/20  Iteration 738/3520 Training loss: 1.5863 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 739/3520 Training loss: 1.5859 0.2956 sec/batch\n",
      "Epoch 5/20  Iteration 740/3520 Training loss: 1.5863 0.2953 sec/batch\n",
      "Epoch 5/20  Iteration 741/3520 Training loss: 1.5852 0.2952 sec/batch\n",
      "Epoch 5/20  Iteration 742/3520 Training loss: 1.5859 0.2955 sec/batch\n",
      "Epoch 5/20  Iteration 743/3520 Training loss: 1.5871 0.2961 sec/batch\n",
      "Epoch 5/20  Iteration 744/3520 Training loss: 1.5869 0.2944 sec/batch\n",
      "Epoch 5/20  Iteration 745/3520 Training loss: 1.5865 0.2942 sec/batch\n",
      "Epoch 5/20  Iteration 746/3520 Training loss: 1.5854 0.2941 sec/batch\n",
      "Epoch 5/20  Iteration 747/3520 Training loss: 1.5855 0.2982 sec/batch\n",
      "Epoch 5/20  Iteration 748/3520 Training loss: 1.5844 0.2939 sec/batch\n",
      "Epoch 5/20  Iteration 749/3520 Training loss: 1.5839 0.2937 sec/batch\n",
      "Epoch 5/20  Iteration 750/3520 Training loss: 1.5833 0.2950 sec/batch\n",
      "Epoch 5/20  Iteration 751/3520 Training loss: 1.5831 0.2944 sec/batch\n",
      "Epoch 5/20  Iteration 752/3520 Training loss: 1.5829 0.2945 sec/batch\n",
      "Epoch 5/20  Iteration 753/3520 Training loss: 1.5828 0.2937 sec/batch\n",
      "Epoch 5/20  Iteration 754/3520 Training loss: 1.5824 0.2939 sec/batch\n",
      "Epoch 5/20  Iteration 755/3520 Training loss: 1.5823 0.2946 sec/batch\n",
      "Epoch 5/20  Iteration 756/3520 Training loss: 1.5821 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 757/3520 Training loss: 1.5821 0.2951 sec/batch\n",
      "Epoch 5/20  Iteration 758/3520 Training loss: 1.5818 0.2938 sec/batch\n",
      "Epoch 5/20  Iteration 759/3520 Training loss: 1.5811 0.3019 sec/batch\n",
      "Epoch 5/20  Iteration 760/3520 Training loss: 1.5807 0.2960 sec/batch\n",
      "Epoch 5/20  Iteration 761/3520 Training loss: 1.5801 0.2953 sec/batch\n",
      "Epoch 5/20  Iteration 762/3520 Training loss: 1.5798 0.2945 sec/batch\n",
      "Epoch 5/20  Iteration 763/3520 Training loss: 1.5792 0.2946 sec/batch\n",
      "Epoch 5/20  Iteration 764/3520 Training loss: 1.5796 0.2954 sec/batch\n",
      "Epoch 5/20  Iteration 765/3520 Training loss: 1.5791 0.2927 sec/batch\n",
      "Epoch 5/20  Iteration 766/3520 Training loss: 1.5788 0.2945 sec/batch\n",
      "Epoch 5/20  Iteration 767/3520 Training loss: 1.5784 0.2967 sec/batch\n",
      "Epoch 5/20  Iteration 768/3520 Training loss: 1.5781 0.2944 sec/batch\n",
      "Epoch 5/20  Iteration 769/3520 Training loss: 1.5773 0.2976 sec/batch\n",
      "Epoch 5/20  Iteration 770/3520 Training loss: 1.5771 0.2952 sec/batch\n",
      "Epoch 5/20  Iteration 771/3520 Training loss: 1.5767 0.2946 sec/batch\n",
      "Epoch 5/20  Iteration 772/3520 Training loss: 1.5760 0.2934 sec/batch\n",
      "Epoch 5/20  Iteration 773/3520 Training loss: 1.5756 0.2938 sec/batch\n",
      "Epoch 5/20  Iteration 774/3520 Training loss: 1.5755 0.2933 sec/batch\n",
      "Epoch 5/20  Iteration 775/3520 Training loss: 1.5750 0.2941 sec/batch\n",
      "Epoch 5/20  Iteration 776/3520 Training loss: 1.5747 0.2949 sec/batch\n",
      "Epoch 5/20  Iteration 777/3520 Training loss: 1.5742 0.2962 sec/batch\n",
      "Epoch 5/20  Iteration 778/3520 Training loss: 1.5736 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 779/3520 Training loss: 1.5732 0.2940 sec/batch\n",
      "Epoch 5/20  Iteration 780/3520 Training loss: 1.5732 0.2932 sec/batch\n",
      "Epoch 5/20  Iteration 781/3520 Training loss: 1.5734 0.2928 sec/batch\n",
      "Epoch 5/20  Iteration 782/3520 Training loss: 1.5736 0.2946 sec/batch\n",
      "Epoch 5/20  Iteration 783/3520 Training loss: 1.5731 0.2935 sec/batch\n",
      "Epoch 5/20  Iteration 784/3520 Training loss: 1.5728 0.2949 sec/batch\n",
      "Epoch 5/20  Iteration 785/3520 Training loss: 1.5721 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 786/3520 Training loss: 1.5722 0.2946 sec/batch\n",
      "Epoch 5/20  Iteration 787/3520 Training loss: 1.5719 0.2944 sec/batch\n",
      "Epoch 5/20  Iteration 788/3520 Training loss: 1.5718 0.2954 sec/batch\n",
      "Epoch 5/20  Iteration 789/3520 Training loss: 1.5714 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 790/3520 Training loss: 1.5707 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 791/3520 Training loss: 1.5710 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 792/3520 Training loss: 1.5708 0.2949 sec/batch\n",
      "Epoch 5/20  Iteration 793/3520 Training loss: 1.5707 0.2949 sec/batch\n",
      "Epoch 5/20  Iteration 794/3520 Training loss: 1.5705 0.2938 sec/batch\n",
      "Epoch 5/20  Iteration 795/3520 Training loss: 1.5703 0.2951 sec/batch\n",
      "Epoch 5/20  Iteration 796/3520 Training loss: 1.5703 0.2938 sec/batch\n",
      "Epoch 5/20  Iteration 797/3520 Training loss: 1.5699 0.2940 sec/batch\n",
      "Epoch 5/20  Iteration 798/3520 Training loss: 1.5694 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 799/3520 Training loss: 1.5691 0.2932 sec/batch\n",
      "Epoch 5/20  Iteration 800/3520 Training loss: 1.5684 0.2938 sec/batch\n",
      "Validation loss: 1.41632 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 801/3520 Training loss: 1.5694 0.2963 sec/batch\n",
      "Epoch 5/20  Iteration 802/3520 Training loss: 1.5693 0.2955 sec/batch\n",
      "Epoch 5/20  Iteration 803/3520 Training loss: 1.5693 0.2953 sec/batch\n",
      "Epoch 5/20  Iteration 804/3520 Training loss: 1.5687 0.2935 sec/batch\n",
      "Epoch 5/20  Iteration 805/3520 Training loss: 1.5685 0.2934 sec/batch\n",
      "Epoch 5/20  Iteration 806/3520 Training loss: 1.5685 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 807/3520 Training loss: 1.5679 0.2932 sec/batch\n",
      "Epoch 5/20  Iteration 808/3520 Training loss: 1.5672 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 809/3520 Training loss: 1.5667 0.2942 sec/batch\n",
      "Epoch 5/20  Iteration 810/3520 Training loss: 1.5666 0.2939 sec/batch\n",
      "Epoch 5/20  Iteration 811/3520 Training loss: 1.5663 0.2974 sec/batch\n",
      "Epoch 5/20  Iteration 812/3520 Training loss: 1.5658 0.2941 sec/batch\n",
      "Epoch 5/20  Iteration 813/3520 Training loss: 1.5656 0.2937 sec/batch\n",
      "Epoch 5/20  Iteration 814/3520 Training loss: 1.5652 0.2933 sec/batch\n",
      "Epoch 5/20  Iteration 815/3520 Training loss: 1.5650 0.2937 sec/batch\n",
      "Epoch 5/20  Iteration 816/3520 Training loss: 1.5649 0.2939 sec/batch\n",
      "Epoch 5/20  Iteration 817/3520 Training loss: 1.5647 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 818/3520 Training loss: 1.5644 0.2954 sec/batch\n",
      "Epoch 5/20  Iteration 819/3520 Training loss: 1.5640 0.2951 sec/batch\n",
      "Epoch 5/20  Iteration 820/3520 Training loss: 1.5637 0.2944 sec/batch\n",
      "Epoch 5/20  Iteration 821/3520 Training loss: 1.5632 0.2944 sec/batch\n",
      "Epoch 5/20  Iteration 822/3520 Training loss: 1.5632 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 823/3520 Training loss: 1.5628 0.2933 sec/batch\n",
      "Epoch 5/20  Iteration 824/3520 Training loss: 1.5625 0.2937 sec/batch\n",
      "Epoch 5/20  Iteration 825/3520 Training loss: 1.5624 0.2945 sec/batch\n",
      "Epoch 5/20  Iteration 826/3520 Training loss: 1.5621 0.2933 sec/batch\n",
      "Epoch 5/20  Iteration 827/3520 Training loss: 1.5617 0.2965 sec/batch\n",
      "Epoch 5/20  Iteration 828/3520 Training loss: 1.5614 0.2930 sec/batch\n",
      "Epoch 5/20  Iteration 829/3520 Training loss: 1.5612 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 830/3520 Training loss: 1.5606 0.2929 sec/batch\n",
      "Epoch 5/20  Iteration 831/3520 Training loss: 1.5602 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 832/3520 Training loss: 1.5599 0.3050 sec/batch\n",
      "Epoch 5/20  Iteration 833/3520 Training loss: 1.5596 0.2952 sec/batch\n",
      "Epoch 5/20  Iteration 834/3520 Training loss: 1.5596 0.2942 sec/batch\n",
      "Epoch 5/20  Iteration 835/3520 Training loss: 1.5592 0.2949 sec/batch\n",
      "Epoch 5/20  Iteration 836/3520 Training loss: 1.5592 0.2942 sec/batch\n",
      "Epoch 5/20  Iteration 837/3520 Training loss: 1.5590 0.2949 sec/batch\n",
      "Epoch 5/20  Iteration 838/3520 Training loss: 1.5589 0.2934 sec/batch\n",
      "Epoch 5/20  Iteration 839/3520 Training loss: 1.5587 0.2941 sec/batch\n",
      "Epoch 5/20  Iteration 840/3520 Training loss: 1.5584 0.2946 sec/batch\n",
      "Epoch 5/20  Iteration 841/3520 Training loss: 1.5582 0.2930 sec/batch\n",
      "Epoch 5/20  Iteration 842/3520 Training loss: 1.5579 0.2935 sec/batch\n",
      "Epoch 5/20  Iteration 843/3520 Training loss: 1.5579 0.2939 sec/batch\n",
      "Epoch 5/20  Iteration 844/3520 Training loss: 1.5577 0.2942 sec/batch\n",
      "Epoch 5/20  Iteration 845/3520 Training loss: 1.5575 0.2934 sec/batch\n",
      "Epoch 5/20  Iteration 846/3520 Training loss: 1.5572 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 847/3520 Training loss: 1.5567 0.2934 sec/batch\n",
      "Epoch 5/20  Iteration 848/3520 Training loss: 1.5564 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 849/3520 Training loss: 1.5562 0.2939 sec/batch\n",
      "Epoch 5/20  Iteration 850/3520 Training loss: 1.5560 0.2936 sec/batch\n",
      "Epoch 5/20  Iteration 851/3520 Training loss: 1.5557 0.2946 sec/batch\n",
      "Epoch 5/20  Iteration 852/3520 Training loss: 1.5555 0.2938 sec/batch\n",
      "Epoch 5/20  Iteration 853/3520 Training loss: 1.5552 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 854/3520 Training loss: 1.5551 0.2935 sec/batch\n",
      "Epoch 5/20  Iteration 855/3520 Training loss: 1.5548 0.2944 sec/batch\n",
      "Epoch 5/20  Iteration 856/3520 Training loss: 1.5545 0.2930 sec/batch\n",
      "Epoch 5/20  Iteration 857/3520 Training loss: 1.5543 0.2937 sec/batch\n",
      "Epoch 5/20  Iteration 858/3520 Training loss: 1.5539 0.2934 sec/batch\n",
      "Epoch 5/20  Iteration 859/3520 Training loss: 1.5535 0.2931 sec/batch\n",
      "Epoch 5/20  Iteration 860/3520 Training loss: 1.5533 0.2934 sec/batch\n",
      "Epoch 5/20  Iteration 861/3520 Training loss: 1.5529 0.2939 sec/batch\n",
      "Epoch 5/20  Iteration 862/3520 Training loss: 1.5527 0.2938 sec/batch\n",
      "Epoch 5/20  Iteration 863/3520 Training loss: 1.5526 0.2954 sec/batch\n",
      "Epoch 5/20  Iteration 864/3520 Training loss: 1.5525 0.2949 sec/batch\n",
      "Epoch 5/20  Iteration 865/3520 Training loss: 1.5522 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 866/3520 Training loss: 1.5520 0.2937 sec/batch\n",
      "Epoch 5/20  Iteration 867/3520 Training loss: 1.5519 0.2937 sec/batch\n",
      "Epoch 5/20  Iteration 868/3520 Training loss: 1.5514 0.2948 sec/batch\n",
      "Epoch 5/20  Iteration 869/3520 Training loss: 1.5508 0.2931 sec/batch\n",
      "Epoch 5/20  Iteration 870/3520 Training loss: 1.5504 0.2935 sec/batch\n",
      "Epoch 5/20  Iteration 871/3520 Training loss: 1.5500 0.2938 sec/batch\n",
      "Epoch 5/20  Iteration 872/3520 Training loss: 1.5497 0.2939 sec/batch\n",
      "Epoch 5/20  Iteration 873/3520 Training loss: 1.5497 0.2933 sec/batch\n",
      "Epoch 5/20  Iteration 874/3520 Training loss: 1.5494 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 875/3520 Training loss: 1.5492 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 876/3520 Training loss: 1.5490 0.2932 sec/batch\n",
      "Epoch 5/20  Iteration 877/3520 Training loss: 1.5488 0.2947 sec/batch\n",
      "Epoch 5/20  Iteration 878/3520 Training loss: 1.5486 0.2943 sec/batch\n",
      "Epoch 5/20  Iteration 879/3520 Training loss: 1.5484 0.2940 sec/batch\n",
      "Epoch 5/20  Iteration 880/3520 Training loss: 1.5484 0.2948 sec/batch\n",
      "Epoch 6/20  Iteration 881/3520 Training loss: 1.5421 0.2934 sec/batch\n",
      "Epoch 6/20  Iteration 882/3520 Training loss: 1.4994 0.2948 sec/batch\n",
      "Epoch 6/20  Iteration 883/3520 Training loss: 1.5018 0.2942 sec/batch\n",
      "Epoch 6/20  Iteration 884/3520 Training loss: 1.5005 0.2950 sec/batch\n",
      "Epoch 6/20  Iteration 885/3520 Training loss: 1.5036 0.2940 sec/batch\n",
      "Epoch 6/20  Iteration 886/3520 Training loss: 1.5044 0.2948 sec/batch\n",
      "Epoch 6/20  Iteration 887/3520 Training loss: 1.5007 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 888/3520 Training loss: 1.5018 0.2940 sec/batch\n",
      "Epoch 6/20  Iteration 889/3520 Training loss: 1.4991 0.2943 sec/batch\n",
      "Epoch 6/20  Iteration 890/3520 Training loss: 1.4990 0.2940 sec/batch\n",
      "Epoch 6/20  Iteration 891/3520 Training loss: 1.5009 0.2936 sec/batch\n",
      "Epoch 6/20  Iteration 892/3520 Training loss: 1.5005 0.2950 sec/batch\n",
      "Epoch 6/20  Iteration 893/3520 Training loss: 1.4994 0.2932 sec/batch\n",
      "Epoch 6/20  Iteration 894/3520 Training loss: 1.5009 0.2947 sec/batch\n",
      "Epoch 6/20  Iteration 895/3520 Training loss: 1.4991 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 896/3520 Training loss: 1.4985 0.2937 sec/batch\n",
      "Epoch 6/20  Iteration 897/3520 Training loss: 1.4984 0.2937 sec/batch\n",
      "Epoch 6/20  Iteration 898/3520 Training loss: 1.4989 0.2932 sec/batch\n",
      "Epoch 6/20  Iteration 899/3520 Training loss: 1.4975 0.2972 sec/batch\n",
      "Epoch 6/20  Iteration 900/3520 Training loss: 1.4989 0.2941 sec/batch\n",
      "Epoch 6/20  Iteration 901/3520 Training loss: 1.4989 0.2934 sec/batch\n",
      "Epoch 6/20  Iteration 902/3520 Training loss: 1.4981 0.2935 sec/batch\n",
      "Epoch 6/20  Iteration 903/3520 Training loss: 1.4987 0.2942 sec/batch\n",
      "Epoch 6/20  Iteration 904/3520 Training loss: 1.4992 0.2934 sec/batch\n",
      "Epoch 6/20  Iteration 905/3520 Training loss: 1.4997 0.2931 sec/batch\n",
      "Epoch 6/20  Iteration 906/3520 Training loss: 1.4986 0.2931 sec/batch\n",
      "Epoch 6/20  Iteration 907/3520 Training loss: 1.4978 0.2935 sec/batch\n",
      "Epoch 6/20  Iteration 908/3520 Training loss: 1.4976 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 909/3520 Training loss: 1.4969 0.2936 sec/batch\n",
      "Epoch 6/20  Iteration 910/3520 Training loss: 1.4969 0.2949 sec/batch\n",
      "Epoch 6/20  Iteration 911/3520 Training loss: 1.4962 0.2949 sec/batch\n",
      "Epoch 6/20  Iteration 912/3520 Training loss: 1.4956 0.2941 sec/batch\n",
      "Epoch 6/20  Iteration 913/3520 Training loss: 1.4961 0.2938 sec/batch\n",
      "Epoch 6/20  Iteration 914/3520 Training loss: 1.4958 0.2945 sec/batch\n",
      "Epoch 6/20  Iteration 915/3520 Training loss: 1.4953 0.2937 sec/batch\n",
      "Epoch 6/20  Iteration 916/3520 Training loss: 1.4957 0.2939 sec/batch\n",
      "Epoch 6/20  Iteration 917/3520 Training loss: 1.4948 0.2950 sec/batch\n",
      "Epoch 6/20  Iteration 918/3520 Training loss: 1.4953 0.2948 sec/batch\n",
      "Epoch 6/20  Iteration 919/3520 Training loss: 1.4962 0.2936 sec/batch\n",
      "Epoch 6/20  Iteration 920/3520 Training loss: 1.4959 0.2942 sec/batch\n",
      "Epoch 6/20  Iteration 921/3520 Training loss: 1.4955 0.2945 sec/batch\n",
      "Epoch 6/20  Iteration 922/3520 Training loss: 1.4944 0.2940 sec/batch\n",
      "Epoch 6/20  Iteration 923/3520 Training loss: 1.4946 0.3061 sec/batch\n",
      "Epoch 6/20  Iteration 924/3520 Training loss: 1.4936 0.2943 sec/batch\n",
      "Epoch 6/20  Iteration 925/3520 Training loss: 1.4933 0.2951 sec/batch\n",
      "Epoch 6/20  Iteration 926/3520 Training loss: 1.4927 0.3051 sec/batch\n",
      "Epoch 6/20  Iteration 927/3520 Training loss: 1.4925 0.2939 sec/batch\n",
      "Epoch 6/20  Iteration 928/3520 Training loss: 1.4923 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 929/3520 Training loss: 1.4921 0.2938 sec/batch\n",
      "Epoch 6/20  Iteration 930/3520 Training loss: 1.4919 0.2937 sec/batch\n",
      "Epoch 6/20  Iteration 931/3520 Training loss: 1.4917 0.2945 sec/batch\n",
      "Epoch 6/20  Iteration 932/3520 Training loss: 1.4917 0.2941 sec/batch\n",
      "Epoch 6/20  Iteration 933/3520 Training loss: 1.4919 0.2941 sec/batch\n",
      "Epoch 6/20  Iteration 934/3520 Training loss: 1.4918 0.2935 sec/batch\n",
      "Epoch 6/20  Iteration 935/3520 Training loss: 1.4912 0.2953 sec/batch\n",
      "Epoch 6/20  Iteration 936/3520 Training loss: 1.4908 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 937/3520 Training loss: 1.4904 0.2938 sec/batch\n",
      "Epoch 6/20  Iteration 938/3520 Training loss: 1.4903 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 939/3520 Training loss: 1.4898 0.2935 sec/batch\n",
      "Epoch 6/20  Iteration 940/3520 Training loss: 1.4902 0.2934 sec/batch\n",
      "Epoch 6/20  Iteration 941/3520 Training loss: 1.4899 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 942/3520 Training loss: 1.4897 0.2936 sec/batch\n",
      "Epoch 6/20  Iteration 943/3520 Training loss: 1.4893 0.2940 sec/batch\n",
      "Epoch 6/20  Iteration 944/3520 Training loss: 1.4892 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 945/3520 Training loss: 1.4886 0.2936 sec/batch\n",
      "Epoch 6/20  Iteration 946/3520 Training loss: 1.4884 0.2957 sec/batch\n",
      "Epoch 6/20  Iteration 947/3520 Training loss: 1.4882 0.2940 sec/batch\n",
      "Epoch 6/20  Iteration 948/3520 Training loss: 1.4876 0.2945 sec/batch\n",
      "Epoch 6/20  Iteration 949/3520 Training loss: 1.4872 0.2955 sec/batch\n",
      "Epoch 6/20  Iteration 950/3520 Training loss: 1.4873 0.2950 sec/batch\n",
      "Epoch 6/20  Iteration 951/3520 Training loss: 1.4869 0.2942 sec/batch\n",
      "Epoch 6/20  Iteration 952/3520 Training loss: 1.4865 0.2944 sec/batch\n",
      "Epoch 6/20  Iteration 953/3520 Training loss: 1.4862 0.2945 sec/batch\n",
      "Epoch 6/20  Iteration 954/3520 Training loss: 1.4856 0.2939 sec/batch\n",
      "Epoch 6/20  Iteration 955/3520 Training loss: 1.4855 0.2940 sec/batch\n",
      "Epoch 6/20  Iteration 956/3520 Training loss: 1.4857 0.2928 sec/batch\n",
      "Epoch 6/20  Iteration 957/3520 Training loss: 1.4858 0.2939 sec/batch\n",
      "Epoch 6/20  Iteration 958/3520 Training loss: 1.4861 0.2944 sec/batch\n",
      "Epoch 6/20  Iteration 959/3520 Training loss: 1.4858 0.2946 sec/batch\n",
      "Epoch 6/20  Iteration 960/3520 Training loss: 1.4856 0.2939 sec/batch\n",
      "Epoch 6/20  Iteration 961/3520 Training loss: 1.4849 0.2948 sec/batch\n",
      "Epoch 6/20  Iteration 962/3520 Training loss: 1.4851 0.2952 sec/batch\n",
      "Epoch 6/20  Iteration 963/3520 Training loss: 1.4848 0.2954 sec/batch\n",
      "Epoch 6/20  Iteration 964/3520 Training loss: 1.4847 0.2950 sec/batch\n",
      "Epoch 6/20  Iteration 965/3520 Training loss: 1.4843 0.2957 sec/batch\n",
      "Epoch 6/20  Iteration 966/3520 Training loss: 1.4837 0.2939 sec/batch\n",
      "Epoch 6/20  Iteration 967/3520 Training loss: 1.4841 0.2948 sec/batch\n",
      "Epoch 6/20  Iteration 968/3520 Training loss: 1.4839 0.2931 sec/batch\n",
      "Epoch 6/20  Iteration 969/3520 Training loss: 1.4841 0.2931 sec/batch\n",
      "Epoch 6/20  Iteration 970/3520 Training loss: 1.4838 0.2927 sec/batch\n",
      "Epoch 6/20  Iteration 971/3520 Training loss: 1.4835 0.2955 sec/batch\n",
      "Epoch 6/20  Iteration 972/3520 Training loss: 1.4836 0.2957 sec/batch\n",
      "Epoch 6/20  Iteration 973/3520 Training loss: 1.4833 0.2949 sec/batch\n",
      "Epoch 6/20  Iteration 974/3520 Training loss: 1.4830 0.2942 sec/batch\n",
      "Epoch 6/20  Iteration 975/3520 Training loss: 1.4828 0.2944 sec/batch\n",
      "Epoch 6/20  Iteration 976/3520 Training loss: 1.4823 0.2940 sec/batch\n",
      "Epoch 6/20  Iteration 977/3520 Training loss: 1.4825 0.2940 sec/batch\n",
      "Epoch 6/20  Iteration 978/3520 Training loss: 1.4825 0.2935 sec/batch\n",
      "Epoch 6/20  Iteration 979/3520 Training loss: 1.4824 0.2944 sec/batch\n",
      "Epoch 6/20  Iteration 980/3520 Training loss: 1.4822 0.2941 sec/batch\n",
      "Epoch 6/20  Iteration 981/3520 Training loss: 1.4819 0.2937 sec/batch\n",
      "Epoch 6/20  Iteration 982/3520 Training loss: 1.4818 0.2947 sec/batch\n",
      "Epoch 6/20  Iteration 983/3520 Training loss: 1.4815 0.2946 sec/batch\n",
      "Epoch 6/20  Iteration 984/3520 Training loss: 1.4808 0.2932 sec/batch\n",
      "Epoch 6/20  Iteration 985/3520 Training loss: 1.4803 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 986/3520 Training loss: 1.4804 0.2930 sec/batch\n",
      "Epoch 6/20  Iteration 987/3520 Training loss: 1.4802 0.2930 sec/batch\n",
      "Epoch 6/20  Iteration 988/3520 Training loss: 1.4797 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 989/3520 Training loss: 1.4796 0.2937 sec/batch\n",
      "Epoch 6/20  Iteration 990/3520 Training loss: 1.4793 0.2935 sec/batch\n",
      "Epoch 6/20  Iteration 991/3520 Training loss: 1.4792 0.2938 sec/batch\n",
      "Epoch 6/20  Iteration 992/3520 Training loss: 1.4791 0.2934 sec/batch\n",
      "Epoch 6/20  Iteration 993/3520 Training loss: 1.4792 0.2946 sec/batch\n",
      "Epoch 6/20  Iteration 994/3520 Training loss: 1.4789 0.2951 sec/batch\n",
      "Epoch 6/20  Iteration 995/3520 Training loss: 1.4786 0.2946 sec/batch\n",
      "Epoch 6/20  Iteration 996/3520 Training loss: 1.4783 0.2940 sec/batch\n",
      "Epoch 6/20  Iteration 997/3520 Training loss: 1.4780 0.2937 sec/batch\n",
      "Epoch 6/20  Iteration 998/3520 Training loss: 1.4780 0.2937 sec/batch\n",
      "Epoch 6/20  Iteration 999/3520 Training loss: 1.4778 0.2946 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3520 Training loss: 1.4777 0.2936 sec/batch\n",
      "Validation loss: 1.33605 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1001/3520 Training loss: 1.4783 0.2966 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3520 Training loss: 1.4781 0.2966 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3520 Training loss: 1.4778 0.2942 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3520 Training loss: 1.4777 0.2952 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3520 Training loss: 1.4776 0.2947 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3520 Training loss: 1.4772 0.2936 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3520 Training loss: 1.4768 0.2939 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3520 Training loss: 1.4766 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3520 Training loss: 1.4764 0.2933 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3520 Training loss: 1.4764 0.2935 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3520 Training loss: 1.4762 0.2929 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3520 Training loss: 1.4764 0.2951 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3520 Training loss: 1.4763 0.2950 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3520 Training loss: 1.4763 0.2943 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3520 Training loss: 1.4763 0.2952 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3520 Training loss: 1.4761 0.2960 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3520 Training loss: 1.4761 0.2948 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3520 Training loss: 1.4759 0.2948 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3520 Training loss: 1.4759 0.2946 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3520 Training loss: 1.4759 0.2946 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3520 Training loss: 1.4757 0.2948 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3520 Training loss: 1.4755 0.2939 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3520 Training loss: 1.4751 0.2938 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3520 Training loss: 1.4749 0.2932 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3520 Training loss: 1.4748 0.2941 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3520 Training loss: 1.4747 0.2925 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3520 Training loss: 1.4744 0.2947 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3520 Training loss: 1.4743 0.2956 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3520 Training loss: 1.4741 0.2931 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3520 Training loss: 1.4741 0.2936 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3520 Training loss: 1.4738 0.2940 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3520 Training loss: 1.4735 0.2944 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3520 Training loss: 1.4733 0.2958 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3520 Training loss: 1.4731 0.2934 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3520 Training loss: 1.4728 0.3040 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3520 Training loss: 1.4726 0.2936 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3520 Training loss: 1.4724 0.2939 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3520 Training loss: 1.4722 0.2947 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3520 Training loss: 1.4721 0.2948 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3520 Training loss: 1.4721 0.2937 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3520 Training loss: 1.4718 0.2934 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3520 Training loss: 1.4716 0.2935 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3520 Training loss: 1.4715 0.2935 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3520 Training loss: 1.4712 0.2937 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3520 Training loss: 1.4707 0.2945 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3520 Training loss: 1.4704 0.2944 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3520 Training loss: 1.4701 0.2947 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3520 Training loss: 1.4700 0.2936 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3520 Training loss: 1.4699 0.2932 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3520 Training loss: 1.4697 0.2934 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3520 Training loss: 1.4696 0.2942 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3520 Training loss: 1.4694 0.2937 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3520 Training loss: 1.4692 0.2938 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3520 Training loss: 1.4691 0.2939 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3520 Training loss: 1.4689 0.2945 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3520 Training loss: 1.4690 0.2933 sec/batch\n",
      "Epoch 7/20  Iteration 1057/3520 Training loss: 1.4722 0.2928 sec/batch\n",
      "Epoch 7/20  Iteration 1058/3520 Training loss: 1.4305 0.2922 sec/batch\n",
      "Epoch 7/20  Iteration 1059/3520 Training loss: 1.4329 0.2931 sec/batch\n",
      "Epoch 7/20  Iteration 1060/3520 Training loss: 1.4320 0.2937 sec/batch\n",
      "Epoch 7/20  Iteration 1061/3520 Training loss: 1.4340 0.2931 sec/batch\n",
      "Epoch 7/20  Iteration 1062/3520 Training loss: 1.4347 0.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1063/3520 Training loss: 1.4333 0.2937 sec/batch\n",
      "Epoch 7/20  Iteration 1064/3520 Training loss: 1.4338 0.2945 sec/batch\n",
      "Epoch 7/20  Iteration 1065/3520 Training loss: 1.4307 0.2943 sec/batch\n",
      "Epoch 7/20  Iteration 1066/3520 Training loss: 1.4310 0.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1067/3520 Training loss: 1.4318 0.2940 sec/batch\n",
      "Epoch 7/20  Iteration 1068/3520 Training loss: 1.4326 0.2944 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3520 Training loss: 1.4313 0.2953 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3520 Training loss: 1.4329 0.2949 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3520 Training loss: 1.4319 0.2948 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3520 Training loss: 1.4313 0.3037 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3520 Training loss: 1.4309 0.2933 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3520 Training loss: 1.4314 0.2935 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3520 Training loss: 1.4306 0.2930 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3520 Training loss: 1.4317 0.2951 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3520 Training loss: 1.4322 0.2945 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3520 Training loss: 1.4313 0.2938 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3520 Training loss: 1.4324 0.2944 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3520 Training loss: 1.4330 0.2934 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3520 Training loss: 1.4336 0.2937 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3520 Training loss: 1.4332 0.2945 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3520 Training loss: 1.4322 0.2944 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3520 Training loss: 1.4322 0.2937 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3520 Training loss: 1.4317 0.2933 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3520 Training loss: 1.4318 0.2939 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3520 Training loss: 1.4315 0.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3520 Training loss: 1.4312 0.2949 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3520 Training loss: 1.4318 0.2935 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3520 Training loss: 1.4315 0.2947 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3520 Training loss: 1.4313 0.2946 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3520 Training loss: 1.4317 0.2942 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3520 Training loss: 1.4308 0.2942 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3520 Training loss: 1.4316 0.2935 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3520 Training loss: 1.4326 0.2934 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3520 Training loss: 1.4322 0.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3520 Training loss: 1.4319 0.2938 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3520 Training loss: 1.4311 0.2947 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3520 Training loss: 1.4313 0.2942 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3520 Training loss: 1.4304 0.2942 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3520 Training loss: 1.4300 0.2937 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3520 Training loss: 1.4296 0.2946 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3520 Training loss: 1.4294 0.2932 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3520 Training loss: 1.4294 0.2945 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3520 Training loss: 1.4293 0.2946 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3520 Training loss: 1.4293 0.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3520 Training loss: 1.4292 0.2946 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3520 Training loss: 1.4292 0.2948 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3520 Training loss: 1.4293 0.2945 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3520 Training loss: 1.4292 0.2945 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3520 Training loss: 1.4289 0.2939 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3520 Training loss: 1.4289 0.2942 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3520 Training loss: 1.4284 0.2944 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3520 Training loss: 1.4282 0.2942 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3520 Training loss: 1.4275 0.2950 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3520 Training loss: 1.4279 0.2959 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3520 Training loss: 1.4275 0.2955 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3520 Training loss: 1.4274 0.2951 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3520 Training loss: 1.4274 0.2949 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3520 Training loss: 1.4273 0.2952 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3520 Training loss: 1.4267 0.2946 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3520 Training loss: 1.4264 0.2940 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3520 Training loss: 1.4262 0.2929 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3520 Training loss: 1.4255 0.2938 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3520 Training loss: 1.4250 0.2934 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3520 Training loss: 1.4251 0.2947 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3520 Training loss: 1.4247 0.3045 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3520 Training loss: 1.4244 0.2943 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3520 Training loss: 1.4241 0.2943 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3520 Training loss: 1.4235 0.2932 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3520 Training loss: 1.4233 0.2956 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3520 Training loss: 1.4236 0.2937 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3520 Training loss: 1.4238 0.2941 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3520 Training loss: 1.4242 0.2949 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3520 Training loss: 1.4240 0.2942 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3520 Training loss: 1.4239 0.2949 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3520 Training loss: 1.4234 0.2951 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3520 Training loss: 1.4237 0.2942 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3520 Training loss: 1.4234 0.2940 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3520 Training loss: 1.4235 0.2950 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3520 Training loss: 1.4233 0.2929 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3520 Training loss: 1.4227 0.2943 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3520 Training loss: 1.4230 0.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3520 Training loss: 1.4229 0.2941 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3520 Training loss: 1.4231 0.2971 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3520 Training loss: 1.4229 0.2946 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3520 Training loss: 1.4227 0.2950 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3520 Training loss: 1.4229 0.2941 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3520 Training loss: 1.4226 0.2932 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3520 Training loss: 1.4223 0.2940 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3520 Training loss: 1.4221 0.2947 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3520 Training loss: 1.4217 0.2952 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3520 Training loss: 1.4219 0.2939 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3520 Training loss: 1.4219 0.2949 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3520 Training loss: 1.4219 0.2939 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3520 Training loss: 1.4217 0.2938 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3520 Training loss: 1.4213 0.2938 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3520 Training loss: 1.4213 0.2930 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3520 Training loss: 1.4210 0.2932 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3520 Training loss: 1.4204 0.2947 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3520 Training loss: 1.4200 0.2953 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3520 Training loss: 1.4201 0.2937 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3520 Training loss: 1.4199 0.2938 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3520 Training loss: 1.4194 0.2950 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3520 Training loss: 1.4194 0.2945 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3520 Training loss: 1.4192 0.2937 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3520 Training loss: 1.4191 0.2938 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3520 Training loss: 1.4191 0.2932 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3520 Training loss: 1.4191 0.2935 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3520 Training loss: 1.4189 0.2939 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3520 Training loss: 1.4186 0.2931 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3520 Training loss: 1.4184 0.2935 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3520 Training loss: 1.4182 0.2931 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3520 Training loss: 1.4182 0.2950 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3520 Training loss: 1.4181 0.2947 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3520 Training loss: 1.4180 0.2944 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3520 Training loss: 1.4180 0.2940 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3520 Training loss: 1.4179 0.2933 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3520 Training loss: 1.4177 0.2940 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3520 Training loss: 1.4176 0.2934 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3520 Training loss: 1.4175 0.2948 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3520 Training loss: 1.4171 0.2935 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3520 Training loss: 1.4168 0.3059 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3520 Training loss: 1.4167 0.2937 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3520 Training loss: 1.4166 0.2940 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3520 Training loss: 1.4166 0.2938 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3520 Training loss: 1.4165 0.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3520 Training loss: 1.4166 0.2933 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3520 Training loss: 1.4165 0.2954 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3520 Training loss: 1.4164 0.2968 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3520 Training loss: 1.4164 0.2927 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3520 Training loss: 1.4163 0.2935 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3520 Training loss: 1.4162 0.2943 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3520 Training loss: 1.4160 0.2930 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3520 Training loss: 1.4161 0.2945 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3520 Training loss: 1.4161 0.2934 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3520 Training loss: 1.4160 0.2934 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3520 Training loss: 1.4158 0.2951 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3520 Training loss: 1.4155 0.2942 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3520 Training loss: 1.4153 0.2938 sec/batch\n",
      "Validation loss: 1.28413 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1201/3520 Training loss: 1.4160 0.2971 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3520 Training loss: 1.4160 0.2964 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3520 Training loss: 1.4158 0.2948 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3520 Training loss: 1.4157 0.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3520 Training loss: 1.4157 0.2938 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3520 Training loss: 1.4157 0.2943 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3520 Training loss: 1.4155 0.3061 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3520 Training loss: 1.4153 0.2932 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3520 Training loss: 1.4151 0.2935 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3520 Training loss: 1.4149 0.2934 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3520 Training loss: 1.4147 0.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3520 Training loss: 1.4146 0.2947 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3520 Training loss: 1.4143 0.2956 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3520 Training loss: 1.4142 0.2956 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3520 Training loss: 1.4142 0.2940 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3520 Training loss: 1.4142 0.2935 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3520 Training loss: 1.4139 0.2944 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3520 Training loss: 1.4138 0.2967 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3520 Training loss: 1.4138 0.2950 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3520 Training loss: 1.4135 0.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3520 Training loss: 1.4131 0.2937 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3520 Training loss: 1.4129 0.2929 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3520 Training loss: 1.4126 0.2936 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3520 Training loss: 1.4125 0.2947 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3520 Training loss: 1.4125 0.2947 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3520 Training loss: 1.4123 0.2933 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3520 Training loss: 1.4123 0.2932 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3520 Training loss: 1.4122 0.2939 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3520 Training loss: 1.4121 0.2933 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3520 Training loss: 1.4121 0.2934 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3520 Training loss: 1.4119 0.2937 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3520 Training loss: 1.4121 0.2936 sec/batch\n",
      "Epoch 8/20  Iteration 1233/3520 Training loss: 1.4311 0.2939 sec/batch\n",
      "Epoch 8/20  Iteration 1234/3520 Training loss: 1.3872 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1235/3520 Training loss: 1.3896 0.2935 sec/batch\n",
      "Epoch 8/20  Iteration 1236/3520 Training loss: 1.3878 0.2933 sec/batch\n",
      "Epoch 8/20  Iteration 1237/3520 Training loss: 1.3890 0.2945 sec/batch\n",
      "Epoch 8/20  Iteration 1238/3520 Training loss: 1.3897 0.2946 sec/batch\n",
      "Epoch 8/20  Iteration 1239/3520 Training loss: 1.3872 0.2934 sec/batch\n",
      "Epoch 8/20  Iteration 1240/3520 Training loss: 1.3886 0.2946 sec/batch\n",
      "Epoch 8/20  Iteration 1241/3520 Training loss: 1.3853 0.2947 sec/batch\n",
      "Epoch 8/20  Iteration 1242/3520 Training loss: 1.3848 0.2939 sec/batch\n",
      "Epoch 8/20  Iteration 1243/3520 Training loss: 1.3849 0.2933 sec/batch\n",
      "Epoch 8/20  Iteration 1244/3520 Training loss: 1.3851 0.2947 sec/batch\n",
      "Epoch 8/20  Iteration 1245/3520 Training loss: 1.3837 0.2934 sec/batch\n",
      "Epoch 8/20  Iteration 1246/3520 Training loss: 1.3843 0.2939 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3520 Training loss: 1.3837 0.2939 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3520 Training loss: 1.3833 0.2936 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3520 Training loss: 1.3826 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3520 Training loss: 1.3831 0.2937 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3520 Training loss: 1.3821 0.2936 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3520 Training loss: 1.3833 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3520 Training loss: 1.3840 0.2933 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3520 Training loss: 1.3832 0.2937 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3520 Training loss: 1.3842 0.2951 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3520 Training loss: 1.3851 0.2953 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3520 Training loss: 1.3858 0.2934 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3520 Training loss: 1.3854 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3520 Training loss: 1.3846 0.2935 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3520 Training loss: 1.3846 0.2947 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3520 Training loss: 1.3835 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3520 Training loss: 1.3839 0.2936 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3520 Training loss: 1.3834 0.2943 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3520 Training loss: 1.3834 0.2946 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3520 Training loss: 1.3839 0.3080 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3520 Training loss: 1.3836 0.2945 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3520 Training loss: 1.3833 0.2930 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3520 Training loss: 1.3838 0.2943 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3520 Training loss: 1.3826 0.2946 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3520 Training loss: 1.3834 0.2932 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3520 Training loss: 1.3843 0.2943 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3520 Training loss: 1.3845 0.2936 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3520 Training loss: 1.3841 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3520 Training loss: 1.3835 0.2933 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3520 Training loss: 1.3834 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3520 Training loss: 1.3828 0.2945 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3520 Training loss: 1.3823 0.2939 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3520 Training loss: 1.3821 0.2949 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3520 Training loss: 1.3820 0.2949 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3520 Training loss: 1.3821 0.2934 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3520 Training loss: 1.3821 0.2934 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3520 Training loss: 1.3819 0.3060 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3520 Training loss: 1.3817 0.2942 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3520 Training loss: 1.3817 0.2924 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3520 Training loss: 1.3817 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3520 Training loss: 1.3816 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3520 Training loss: 1.3812 0.2953 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3520 Training loss: 1.3811 0.2931 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3520 Training loss: 1.3809 0.2948 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3520 Training loss: 1.3808 0.2948 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3520 Training loss: 1.3801 0.2937 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3520 Training loss: 1.3805 0.2933 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3520 Training loss: 1.3803 0.2950 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3520 Training loss: 1.3802 0.2953 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3520 Training loss: 1.3802 0.2941 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3520 Training loss: 1.3801 0.2952 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3520 Training loss: 1.3796 0.2951 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3520 Training loss: 1.3796 0.2949 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3520 Training loss: 1.3794 0.2942 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3520 Training loss: 1.3788 0.2950 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3520 Training loss: 1.3784 0.2929 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3520 Training loss: 1.3784 0.2948 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3520 Training loss: 1.3781 0.2946 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3520 Training loss: 1.3778 0.2946 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3520 Training loss: 1.3775 0.2947 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3520 Training loss: 1.3770 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3520 Training loss: 1.3767 0.2945 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3520 Training loss: 1.3769 0.2937 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3520 Training loss: 1.3771 0.2941 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3520 Training loss: 1.3775 0.2943 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3520 Training loss: 1.3774 0.2954 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3520 Training loss: 1.3773 0.2954 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3520 Training loss: 1.3768 0.2936 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3520 Training loss: 1.3771 0.2937 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3520 Training loss: 1.3769 0.2961 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3520 Training loss: 1.3770 0.2932 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3520 Training loss: 1.3769 0.2933 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3520 Training loss: 1.3764 0.2934 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3520 Training loss: 1.3768 0.2935 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3520 Training loss: 1.3767 0.3031 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3520 Training loss: 1.3768 0.2945 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3520 Training loss: 1.3766 0.2934 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3520 Training loss: 1.3765 0.2948 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3520 Training loss: 1.3766 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3520 Training loss: 1.3762 0.2933 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3520 Training loss: 1.3760 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3520 Training loss: 1.3758 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3520 Training loss: 1.3754 0.2946 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3520 Training loss: 1.3756 0.2944 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3520 Training loss: 1.3755 0.2935 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3520 Training loss: 1.3755 0.2948 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3520 Training loss: 1.3751 0.2943 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3520 Training loss: 1.3749 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3520 Training loss: 1.3749 0.2927 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3520 Training loss: 1.3745 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3520 Training loss: 1.3739 0.2941 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3520 Training loss: 1.3735 0.2953 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3520 Training loss: 1.3735 0.2961 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3520 Training loss: 1.3734 0.2947 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3520 Training loss: 1.3730 0.2945 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3520 Training loss: 1.3730 0.3028 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3520 Training loss: 1.3729 0.2933 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3520 Training loss: 1.3728 0.2936 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3520 Training loss: 1.3729 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3520 Training loss: 1.3729 0.2941 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3520 Training loss: 1.3726 0.2947 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3520 Training loss: 1.3725 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3520 Training loss: 1.3723 0.3059 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3520 Training loss: 1.3722 0.2944 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3520 Training loss: 1.3724 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3520 Training loss: 1.3721 0.2941 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3520 Training loss: 1.3720 0.2933 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3520 Training loss: 1.3720 0.2934 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3520 Training loss: 1.3719 0.2952 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3520 Training loss: 1.3719 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3520 Training loss: 1.3719 0.2943 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3520 Training loss: 1.3717 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3520 Training loss: 1.3714 0.2934 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3520 Training loss: 1.3712 0.2929 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3520 Training loss: 1.3711 0.2939 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3520 Training loss: 1.3710 0.2956 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3520 Training loss: 1.3710 0.2945 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3520 Training loss: 1.3709 0.2947 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3520 Training loss: 1.3712 0.2950 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3520 Training loss: 1.3711 0.2934 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3520 Training loss: 1.3711 0.2949 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3520 Training loss: 1.3712 0.3024 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3520 Training loss: 1.3711 0.2932 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3520 Training loss: 1.3712 0.2934 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3520 Training loss: 1.3709 0.2925 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3520 Training loss: 1.3710 0.2935 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3520 Training loss: 1.3710 0.2935 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3520 Training loss: 1.3710 0.2936 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3520 Training loss: 1.3708 0.2936 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3520 Training loss: 1.3705 0.2955 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3520 Training loss: 1.3704 0.2933 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3520 Training loss: 1.3704 0.2955 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3520 Training loss: 1.3704 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3520 Training loss: 1.3702 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3520 Training loss: 1.3701 0.2941 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3520 Training loss: 1.3700 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3520 Training loss: 1.3701 0.2937 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3520 Training loss: 1.3700 0.2935 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3520 Training loss: 1.3698 0.2928 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3520 Training loss: 1.3696 0.2925 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3520 Training loss: 1.3693 0.2933 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3520 Training loss: 1.3692 0.2941 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3520 Training loss: 1.3691 0.2935 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3520 Training loss: 1.3689 0.2946 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3520 Training loss: 1.3689 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3520 Training loss: 1.3689 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3520 Training loss: 1.3690 0.2930 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3520 Training loss: 1.3687 0.2938 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3520 Training loss: 1.3687 0.2947 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3520 Training loss: 1.3687 0.2941 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3520 Training loss: 1.3684 0.2945 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3520 Training loss: 1.3680 0.2941 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3520 Training loss: 1.3678 0.2936 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3520 Training loss: 1.3676 0.2932 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3520 Training loss: 1.3675 0.2928 sec/batch\n",
      "Validation loss: 1.24635 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 1401/3520 Training loss: 1.3682 0.2974 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3520 Training loss: 1.3682 0.2971 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3520 Training loss: 1.3682 0.2953 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3520 Training loss: 1.3681 0.2943 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3520 Training loss: 1.3680 0.2953 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3520 Training loss: 1.3680 0.2947 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3520 Training loss: 1.3680 0.2940 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3520 Training loss: 1.3682 0.2960 sec/batch\n",
      "Epoch 9/20  Iteration 1409/3520 Training loss: 1.3936 0.2942 sec/batch\n",
      "Epoch 9/20  Iteration 1410/3520 Training loss: 1.3511 0.3045 sec/batch\n",
      "Epoch 9/20  Iteration 1411/3520 Training loss: 1.3515 0.2946 sec/batch\n",
      "Epoch 9/20  Iteration 1412/3520 Training loss: 1.3498 0.2951 sec/batch\n",
      "Epoch 9/20  Iteration 1413/3520 Training loss: 1.3541 0.2950 sec/batch\n",
      "Epoch 9/20  Iteration 1414/3520 Training loss: 1.3543 0.2942 sec/batch\n",
      "Epoch 9/20  Iteration 1415/3520 Training loss: 1.3522 0.2936 sec/batch\n",
      "Epoch 9/20  Iteration 1416/3520 Training loss: 1.3518 0.2934 sec/batch\n",
      "Epoch 9/20  Iteration 1417/3520 Training loss: 1.3490 0.2937 sec/batch\n",
      "Epoch 9/20  Iteration 1418/3520 Training loss: 1.3480 0.2940 sec/batch\n",
      "Epoch 9/20  Iteration 1419/3520 Training loss: 1.3496 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1420/3520 Training loss: 1.3496 0.2953 sec/batch\n",
      "Epoch 9/20  Iteration 1421/3520 Training loss: 1.3487 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1422/3520 Training loss: 1.3505 0.2937 sec/batch\n",
      "Epoch 9/20  Iteration 1423/3520 Training loss: 1.3500 0.2934 sec/batch\n",
      "Epoch 9/20  Iteration 1424/3520 Training loss: 1.3491 0.2943 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3520 Training loss: 1.3479 0.2928 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3520 Training loss: 1.3484 0.2934 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3520 Training loss: 1.3473 0.2935 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3520 Training loss: 1.3486 0.2934 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3520 Training loss: 1.3491 0.2959 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3520 Training loss: 1.3480 0.2936 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3520 Training loss: 1.3490 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3520 Training loss: 1.3497 0.2938 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3520 Training loss: 1.3501 0.2945 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3520 Training loss: 1.3496 0.2940 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3520 Training loss: 1.3489 0.2944 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3520 Training loss: 1.3489 0.2941 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3520 Training loss: 1.3482 0.2953 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3520 Training loss: 1.3488 0.2946 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3520 Training loss: 1.3484 0.2934 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3520 Training loss: 1.3483 0.2934 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3520 Training loss: 1.3490 0.2937 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3520 Training loss: 1.3489 0.2949 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3520 Training loss: 1.3489 0.2948 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3520 Training loss: 1.3491 0.2947 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3520 Training loss: 1.3483 0.2938 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3520 Training loss: 1.3492 0.2946 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3520 Training loss: 1.3500 0.2942 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3520 Training loss: 1.3501 0.2937 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3520 Training loss: 1.3497 0.2949 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3520 Training loss: 1.3489 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3520 Training loss: 1.3487 0.2954 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3520 Training loss: 1.3479 0.2947 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3520 Training loss: 1.3473 0.2937 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3520 Training loss: 1.3469 0.2944 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3520 Training loss: 1.3469 0.2949 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3520 Training loss: 1.3471 0.2941 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3520 Training loss: 1.3470 0.2928 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3520 Training loss: 1.3469 0.2933 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3520 Training loss: 1.3468 0.2936 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3520 Training loss: 1.3467 0.2966 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3520 Training loss: 1.3466 0.2931 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3520 Training loss: 1.3467 0.2960 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3520 Training loss: 1.3463 0.2949 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3520 Training loss: 1.3463 0.2957 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3520 Training loss: 1.3461 0.2937 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3520 Training loss: 1.3460 0.2949 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3520 Training loss: 1.3455 0.2952 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3520 Training loss: 1.3457 0.2952 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3520 Training loss: 1.3454 0.2954 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3520 Training loss: 1.3454 0.2952 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3520 Training loss: 1.3454 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3520 Training loss: 1.3452 0.2941 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3520 Training loss: 1.3448 0.2941 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3520 Training loss: 1.3449 0.2932 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3520 Training loss: 1.3445 0.2946 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3520 Training loss: 1.3441 0.2936 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3520 Training loss: 1.3437 0.2936 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3520 Training loss: 1.3437 0.2957 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3520 Training loss: 1.3435 0.2947 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3520 Training loss: 1.3431 0.2953 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3520 Training loss: 1.3428 0.2953 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3520 Training loss: 1.3423 0.2956 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3520 Training loss: 1.3421 0.2948 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3520 Training loss: 1.3421 0.2941 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3520 Training loss: 1.3424 0.2955 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3520 Training loss: 1.3427 0.2954 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3520 Training loss: 1.3427 0.2956 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3520 Training loss: 1.3426 0.2954 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3520 Training loss: 1.3422 0.2958 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3520 Training loss: 1.3426 0.2950 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3520 Training loss: 1.3424 0.2944 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3520 Training loss: 1.3425 0.2945 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3520 Training loss: 1.3424 0.2942 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3520 Training loss: 1.3419 0.2952 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3520 Training loss: 1.3422 0.2949 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3520 Training loss: 1.3420 0.2948 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3520 Training loss: 1.3421 0.2947 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3520 Training loss: 1.3419 0.3045 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3520 Training loss: 1.3418 0.2946 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3520 Training loss: 1.3420 0.2937 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3520 Training loss: 1.3416 0.2932 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3520 Training loss: 1.3414 0.2935 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3520 Training loss: 1.3412 0.2945 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3520 Training loss: 1.3408 0.2928 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3520 Training loss: 1.3411 0.2946 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3520 Training loss: 1.3410 0.2947 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3520 Training loss: 1.3411 0.2936 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3520 Training loss: 1.3409 0.2940 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3520 Training loss: 1.3407 0.2938 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3520 Training loss: 1.3406 0.2940 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3520 Training loss: 1.3403 0.2944 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3520 Training loss: 1.3397 0.2944 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3520 Training loss: 1.3393 0.2936 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3520 Training loss: 1.3395 0.2932 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3520 Training loss: 1.3394 0.2937 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3520 Training loss: 1.3390 0.2941 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3520 Training loss: 1.3391 0.2950 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3520 Training loss: 1.3390 0.2949 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3520 Training loss: 1.3390 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3520 Training loss: 1.3391 0.2954 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3520 Training loss: 1.3392 0.2952 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3520 Training loss: 1.3390 0.2952 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3520 Training loss: 1.3388 0.2949 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3520 Training loss: 1.3386 0.2948 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3520 Training loss: 1.3383 0.2942 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3520 Training loss: 1.3383 0.2935 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3520 Training loss: 1.3382 0.2925 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3520 Training loss: 1.3382 0.2934 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3520 Training loss: 1.3381 0.2934 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3520 Training loss: 1.3380 0.2944 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3520 Training loss: 1.3378 0.2996 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3520 Training loss: 1.3377 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3520 Training loss: 1.3376 0.2936 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3520 Training loss: 1.3372 0.2946 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3520 Training loss: 1.3370 0.2940 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3520 Training loss: 1.3370 0.2945 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3520 Training loss: 1.3370 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3520 Training loss: 1.3370 0.2938 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3520 Training loss: 1.3369 0.2943 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3520 Training loss: 1.3372 0.2937 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3520 Training loss: 1.3371 0.2935 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3520 Training loss: 1.3371 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3520 Training loss: 1.3372 0.2924 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3520 Training loss: 1.3371 0.2933 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3520 Training loss: 1.3371 0.2942 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3520 Training loss: 1.3368 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3520 Training loss: 1.3369 0.2947 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3520 Training loss: 1.3370 0.2942 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3520 Training loss: 1.3371 0.2929 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3520 Training loss: 1.3369 0.2967 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3520 Training loss: 1.3367 0.2947 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3520 Training loss: 1.3366 0.2942 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3520 Training loss: 1.3366 0.2936 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3520 Training loss: 1.3366 0.2933 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3520 Training loss: 1.3364 0.2938 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3520 Training loss: 1.3364 0.2951 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3520 Training loss: 1.3364 0.3016 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3520 Training loss: 1.3365 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3520 Training loss: 1.3364 0.2935 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3520 Training loss: 1.3362 0.2936 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3520 Training loss: 1.3361 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3520 Training loss: 1.3359 0.2988 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3520 Training loss: 1.3358 0.2943 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3520 Training loss: 1.3357 0.2937 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3520 Training loss: 1.3354 0.2942 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3520 Training loss: 1.3354 0.2938 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3520 Training loss: 1.3354 0.2931 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3520 Training loss: 1.3355 0.2943 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3520 Training loss: 1.3353 0.2975 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3520 Training loss: 1.3352 0.2944 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3520 Training loss: 1.3352 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3520 Training loss: 1.3350 0.2947 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3520 Training loss: 1.3346 0.2937 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3520 Training loss: 1.3344 0.2940 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3520 Training loss: 1.3342 0.2946 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3520 Training loss: 1.3341 0.2945 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3520 Training loss: 1.3342 0.2935 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3520 Training loss: 1.3341 0.2935 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3520 Training loss: 1.3341 0.2939 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3520 Training loss: 1.3341 0.2942 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3520 Training loss: 1.3340 0.2948 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3520 Training loss: 1.3341 0.2935 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3520 Training loss: 1.3340 0.2933 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3520 Training loss: 1.3341 0.2942 sec/batch\n",
      "Epoch 10/20  Iteration 1585/3520 Training loss: 1.3618 0.2945 sec/batch\n",
      "Epoch 10/20  Iteration 1586/3520 Training loss: 1.3212 0.2946 sec/batch\n",
      "Epoch 10/20  Iteration 1587/3520 Training loss: 1.3187 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1588/3520 Training loss: 1.3186 0.2949 sec/batch\n",
      "Epoch 10/20  Iteration 1589/3520 Training loss: 1.3227 0.2945 sec/batch\n",
      "Epoch 10/20  Iteration 1590/3520 Training loss: 1.3222 0.2939 sec/batch\n",
      "Epoch 10/20  Iteration 1591/3520 Training loss: 1.3187 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1592/3520 Training loss: 1.3187 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1593/3520 Training loss: 1.3160 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1594/3520 Training loss: 1.3154 0.2958 sec/batch\n",
      "Epoch 10/20  Iteration 1595/3520 Training loss: 1.3152 0.2938 sec/batch\n",
      "Epoch 10/20  Iteration 1596/3520 Training loss: 1.3154 0.2931 sec/batch\n",
      "Epoch 10/20  Iteration 1597/3520 Training loss: 1.3150 0.2938 sec/batch\n",
      "Epoch 10/20  Iteration 1598/3520 Training loss: 1.3165 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1599/3520 Training loss: 1.3159 0.2939 sec/batch\n",
      "Epoch 10/20  Iteration 1600/3520 Training loss: 1.3149 0.2941 sec/batch\n",
      "Validation loss: 1.21756 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 1601/3520 Training loss: 1.3211 0.2993 sec/batch\n",
      "Epoch 10/20  Iteration 1602/3520 Training loss: 1.3216 0.2973 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3520 Training loss: 1.3205 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3520 Training loss: 1.3219 0.2941 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3520 Training loss: 1.3225 0.2945 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3520 Training loss: 1.3220 0.2943 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3520 Training loss: 1.3228 0.2942 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3520 Training loss: 1.3236 0.2932 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3520 Training loss: 1.3242 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3520 Training loss: 1.3238 0.2933 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3520 Training loss: 1.3227 0.2938 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3520 Training loss: 1.3226 0.2944 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3520 Training loss: 1.3216 0.2940 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3520 Training loss: 1.3222 0.2947 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3520 Training loss: 1.3215 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3520 Training loss: 1.3213 0.2935 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3520 Training loss: 1.3219 0.2932 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3520 Training loss: 1.3213 0.2926 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3520 Training loss: 1.3212 0.2971 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3520 Training loss: 1.3216 0.2932 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3520 Training loss: 1.3206 0.2935 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3520 Training loss: 1.3211 0.2945 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3520 Training loss: 1.3221 0.2930 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3520 Training loss: 1.3221 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3520 Training loss: 1.3218 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3520 Training loss: 1.3209 0.2960 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3520 Training loss: 1.3207 0.2943 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3520 Training loss: 1.3200 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3520 Training loss: 1.3196 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3520 Training loss: 1.3193 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3520 Training loss: 1.3191 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3520 Training loss: 1.3191 0.2959 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3520 Training loss: 1.3190 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3520 Training loss: 1.3191 0.2949 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3520 Training loss: 1.3189 0.3048 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3520 Training loss: 1.3188 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3520 Training loss: 1.3187 0.2952 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3520 Training loss: 1.3185 0.2951 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3520 Training loss: 1.3184 0.2943 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3520 Training loss: 1.3183 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3520 Training loss: 1.3180 0.2932 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3520 Training loss: 1.3179 0.2944 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3520 Training loss: 1.3174 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3520 Training loss: 1.3177 0.2954 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3520 Training loss: 1.3173 0.2946 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3520 Training loss: 1.3172 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3520 Training loss: 1.3171 0.2947 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3520 Training loss: 1.3169 0.2938 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3520 Training loss: 1.3163 0.2948 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3520 Training loss: 1.3163 0.2946 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3520 Training loss: 1.3161 0.2952 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3520 Training loss: 1.3155 0.2931 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3520 Training loss: 1.3153 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3520 Training loss: 1.3153 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3520 Training loss: 1.3151 0.2938 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3520 Training loss: 1.3148 0.2944 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3520 Training loss: 1.3144 0.2938 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3520 Training loss: 1.3140 0.2943 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3520 Training loss: 1.3138 0.2945 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3520 Training loss: 1.3141 0.2940 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3520 Training loss: 1.3142 0.3071 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3520 Training loss: 1.3146 0.2943 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3520 Training loss: 1.3145 0.2946 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3520 Training loss: 1.3143 0.2949 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3520 Training loss: 1.3139 0.2939 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3520 Training loss: 1.3142 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3520 Training loss: 1.3140 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3520 Training loss: 1.3142 0.2949 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3520 Training loss: 1.3141 0.2935 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3520 Training loss: 1.3136 0.2940 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3520 Training loss: 1.3139 0.2944 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3520 Training loss: 1.3137 0.2952 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3520 Training loss: 1.3139 0.2951 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3520 Training loss: 1.3136 0.2942 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3520 Training loss: 1.3134 0.3069 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3520 Training loss: 1.3136 0.2968 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3520 Training loss: 1.3134 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3520 Training loss: 1.3133 0.2947 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3520 Training loss: 1.3131 0.2952 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3520 Training loss: 1.3127 0.2950 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3520 Training loss: 1.3129 0.2945 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3520 Training loss: 1.3128 0.2940 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3520 Training loss: 1.3128 0.3046 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3520 Training loss: 1.3127 0.2942 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3520 Training loss: 1.3124 0.2941 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3520 Training loss: 1.3124 0.2950 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3520 Training loss: 1.3122 0.2939 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3520 Training loss: 1.3117 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3520 Training loss: 1.3112 0.2933 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3520 Training loss: 1.3114 0.2929 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3520 Training loss: 1.3112 0.2939 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3520 Training loss: 1.3109 0.2948 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3520 Training loss: 1.3109 0.2944 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3520 Training loss: 1.3108 0.2944 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3520 Training loss: 1.3107 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3520 Training loss: 1.3109 0.2939 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3520 Training loss: 1.3109 0.2933 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3520 Training loss: 1.3107 0.2943 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3520 Training loss: 1.3105 0.3033 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3520 Training loss: 1.3103 0.2945 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3520 Training loss: 1.3101 0.2952 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3520 Training loss: 1.3103 0.2938 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3520 Training loss: 1.3102 0.2940 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3520 Training loss: 1.3101 0.2939 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3520 Training loss: 1.3100 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3520 Training loss: 1.3099 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3520 Training loss: 1.3099 0.2946 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3520 Training loss: 1.3098 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3520 Training loss: 1.3097 0.2938 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3520 Training loss: 1.3093 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3520 Training loss: 1.3091 0.2952 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3520 Training loss: 1.3091 0.2938 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3520 Training loss: 1.3090 0.2945 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3520 Training loss: 1.3091 0.2935 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3520 Training loss: 1.3090 0.2946 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3520 Training loss: 1.3093 0.2947 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3520 Training loss: 1.3092 0.2954 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3520 Training loss: 1.3093 0.2945 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3520 Training loss: 1.3093 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3520 Training loss: 1.3092 0.2942 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3520 Training loss: 1.3093 0.2947 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3520 Training loss: 1.3091 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3520 Training loss: 1.3092 0.2950 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3520 Training loss: 1.3093 0.2944 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3520 Training loss: 1.3093 0.2944 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3520 Training loss: 1.3091 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3520 Training loss: 1.3087 0.2938 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3520 Training loss: 1.3088 0.2935 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3520 Training loss: 1.3089 0.3049 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3520 Training loss: 1.3089 0.2946 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3520 Training loss: 1.3089 0.2943 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3520 Training loss: 1.3088 0.2939 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3520 Training loss: 1.3087 0.2944 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3520 Training loss: 1.3088 0.2949 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3520 Training loss: 1.3087 0.2951 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3520 Training loss: 1.3085 0.2960 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3520 Training loss: 1.3084 0.3053 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3520 Training loss: 1.3082 0.2947 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3520 Training loss: 1.3081 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3520 Training loss: 1.3080 0.2942 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3520 Training loss: 1.3078 0.2935 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3520 Training loss: 1.3078 0.2943 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3520 Training loss: 1.3078 0.2926 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3520 Training loss: 1.3078 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3520 Training loss: 1.3076 0.2936 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3520 Training loss: 1.3075 0.2934 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3520 Training loss: 1.3075 0.3052 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3520 Training loss: 1.3073 0.2945 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3520 Training loss: 1.3069 0.2957 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3520 Training loss: 1.3067 0.2960 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3520 Training loss: 1.3065 0.2938 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3520 Training loss: 1.3065 0.2941 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3520 Training loss: 1.3066 0.2949 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3520 Training loss: 1.3065 0.2940 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3520 Training loss: 1.3064 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3520 Training loss: 1.3064 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3520 Training loss: 1.3063 0.2950 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3520 Training loss: 1.3063 0.2943 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3520 Training loss: 1.3062 0.2937 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3520 Training loss: 1.3063 0.2933 sec/batch\n",
      "Epoch 11/20  Iteration 1761/3520 Training loss: 1.3309 0.2931 sec/batch\n",
      "Epoch 11/20  Iteration 1762/3520 Training loss: 1.2963 0.2929 sec/batch\n",
      "Epoch 11/20  Iteration 1763/3520 Training loss: 1.2940 0.2934 sec/batch\n",
      "Epoch 11/20  Iteration 1764/3520 Training loss: 1.2940 0.2950 sec/batch\n",
      "Epoch 11/20  Iteration 1765/3520 Training loss: 1.2967 0.2957 sec/batch\n",
      "Epoch 11/20  Iteration 1766/3520 Training loss: 1.2958 0.3032 sec/batch\n",
      "Epoch 11/20  Iteration 1767/3520 Training loss: 1.2913 0.2950 sec/batch\n",
      "Epoch 11/20  Iteration 1768/3520 Training loss: 1.2914 0.2941 sec/batch\n",
      "Epoch 11/20  Iteration 1769/3520 Training loss: 1.2884 0.2947 sec/batch\n",
      "Epoch 11/20  Iteration 1770/3520 Training loss: 1.2882 0.2934 sec/batch\n",
      "Epoch 11/20  Iteration 1771/3520 Training loss: 1.2885 0.2944 sec/batch\n",
      "Epoch 11/20  Iteration 1772/3520 Training loss: 1.2891 0.2949 sec/batch\n",
      "Epoch 11/20  Iteration 1773/3520 Training loss: 1.2889 0.2938 sec/batch\n",
      "Epoch 11/20  Iteration 1774/3520 Training loss: 1.2898 0.2943 sec/batch\n",
      "Epoch 11/20  Iteration 1775/3520 Training loss: 1.2897 0.2950 sec/batch\n",
      "Epoch 11/20  Iteration 1776/3520 Training loss: 1.2883 0.2942 sec/batch\n",
      "Epoch 11/20  Iteration 1777/3520 Training loss: 1.2877 0.2940 sec/batch\n",
      "Epoch 11/20  Iteration 1778/3520 Training loss: 1.2879 0.2950 sec/batch\n",
      "Epoch 11/20  Iteration 1779/3520 Training loss: 1.2868 0.2935 sec/batch\n",
      "Epoch 11/20  Iteration 1780/3520 Training loss: 1.2880 0.2945 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3520 Training loss: 1.2884 0.2956 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3520 Training loss: 1.2882 0.2941 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3520 Training loss: 1.2889 0.2939 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3520 Training loss: 1.2894 0.2949 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3520 Training loss: 1.2904 0.2947 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3520 Training loss: 1.2896 0.2952 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3520 Training loss: 1.2887 0.2935 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3520 Training loss: 1.2887 0.2941 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3520 Training loss: 1.2877 0.2944 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3520 Training loss: 1.2881 0.2938 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3520 Training loss: 1.2875 0.2960 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3520 Training loss: 1.2872 0.2939 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3520 Training loss: 1.2881 0.2939 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3520 Training loss: 1.2877 0.2952 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3520 Training loss: 1.2875 0.2950 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3520 Training loss: 1.2879 0.2949 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3520 Training loss: 1.2869 0.2959 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3520 Training loss: 1.2878 0.2944 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3520 Training loss: 1.2886 0.2951 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3520 Training loss: 1.2888 0.2943 sec/batch\n",
      "Validation loss: 1.18914 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 1801/3520 Training loss: 1.2920 0.2958 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3520 Training loss: 1.2915 0.2970 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3520 Training loss: 1.2912 0.2944 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3520 Training loss: 1.2904 0.2937 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3520 Training loss: 1.2899 0.2946 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3520 Training loss: 1.2896 0.2952 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3520 Training loss: 1.2893 0.2954 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3520 Training loss: 1.2894 0.2946 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3520 Training loss: 1.2894 0.2958 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3520 Training loss: 1.2891 0.2931 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3520 Training loss: 1.2889 0.2922 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3520 Training loss: 1.2888 0.2930 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3520 Training loss: 1.2888 0.2933 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3520 Training loss: 1.2886 0.2937 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3520 Training loss: 1.2881 0.2959 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3520 Training loss: 1.2881 0.2936 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3520 Training loss: 1.2879 0.3068 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3520 Training loss: 1.2878 0.2959 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3520 Training loss: 1.2871 0.2939 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3520 Training loss: 1.2874 0.2964 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3520 Training loss: 1.2872 0.2937 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3520 Training loss: 1.2872 0.2938 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3520 Training loss: 1.2869 0.2962 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3520 Training loss: 1.2867 0.2937 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3520 Training loss: 1.2862 0.2944 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3520 Training loss: 1.2860 0.2945 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3520 Training loss: 1.2856 0.2945 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3520 Training loss: 1.2850 0.2943 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3520 Training loss: 1.2846 0.2941 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3520 Training loss: 1.2845 0.2935 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3520 Training loss: 1.2843 0.2933 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3520 Training loss: 1.2841 0.2940 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3520 Training loss: 1.2838 0.2936 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3520 Training loss: 1.2833 0.2937 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3520 Training loss: 1.2831 0.2934 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3520 Training loss: 1.2832 0.2962 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3520 Training loss: 1.2834 0.2941 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3520 Training loss: 1.2837 0.2943 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3520 Training loss: 1.2837 0.2949 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3520 Training loss: 1.2834 0.2949 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3520 Training loss: 1.2829 0.2942 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3520 Training loss: 1.2831 0.2950 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3520 Training loss: 1.2828 0.2943 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3520 Training loss: 1.2828 0.2944 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3520 Training loss: 1.2826 0.2939 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3520 Training loss: 1.2820 0.2931 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3520 Training loss: 1.2823 0.2943 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3520 Training loss: 1.2821 0.2977 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3520 Training loss: 1.2821 0.2931 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3520 Training loss: 1.2818 0.2933 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3520 Training loss: 1.2816 0.2938 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3520 Training loss: 1.2817 0.2942 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3520 Training loss: 1.2814 0.2937 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3520 Training loss: 1.2812 0.2939 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3520 Training loss: 1.2810 0.2937 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3520 Training loss: 1.2805 0.2938 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3520 Training loss: 1.2806 0.2934 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3520 Training loss: 1.2805 0.2939 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3520 Training loss: 1.2803 0.2936 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3520 Training loss: 1.2800 0.2935 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3520 Training loss: 1.2797 0.2944 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3520 Training loss: 1.2796 0.2948 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3520 Training loss: 1.2792 0.2930 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3520 Training loss: 1.2786 0.2949 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3520 Training loss: 1.2781 0.2952 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3520 Training loss: 1.2781 0.2950 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3520 Training loss: 1.2780 0.2945 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3520 Training loss: 1.2777 0.2941 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3520 Training loss: 1.2777 0.2938 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3520 Training loss: 1.2775 0.2934 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3520 Training loss: 1.2774 0.2945 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3520 Training loss: 1.2774 0.2952 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3520 Training loss: 1.2773 0.2940 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3520 Training loss: 1.2770 0.2950 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3520 Training loss: 1.2768 0.2952 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3520 Training loss: 1.2766 0.2943 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3520 Training loss: 1.2763 0.2933 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3520 Training loss: 1.2764 0.2946 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3520 Training loss: 1.2763 0.2940 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3520 Training loss: 1.2761 0.2942 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3520 Training loss: 1.2759 0.2948 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3520 Training loss: 1.2759 0.2937 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3520 Training loss: 1.2757 0.2939 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3520 Training loss: 1.2755 0.2938 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3520 Training loss: 1.2753 0.2948 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3520 Training loss: 1.2750 0.2946 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3520 Training loss: 1.2747 0.2939 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3520 Training loss: 1.2747 0.2947 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3520 Training loss: 1.2745 0.2948 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3520 Training loss: 1.2745 0.2938 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3520 Training loss: 1.2744 0.2955 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3520 Training loss: 1.2746 0.2952 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3520 Training loss: 1.2745 0.2965 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3520 Training loss: 1.2745 0.2932 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3520 Training loss: 1.2744 0.2936 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3520 Training loss: 1.2744 0.2943 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3520 Training loss: 1.2744 0.2934 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3520 Training loss: 1.2742 0.2955 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3520 Training loss: 1.2742 0.2945 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3520 Training loss: 1.2743 0.2936 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3520 Training loss: 1.2742 0.2955 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3520 Training loss: 1.2740 0.2948 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3520 Training loss: 1.2737 0.2938 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3520 Training loss: 1.2736 0.2947 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3520 Training loss: 1.2736 0.2951 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3520 Training loss: 1.2736 0.2946 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3520 Training loss: 1.2734 0.2945 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3520 Training loss: 1.2733 0.2962 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3520 Training loss: 1.2732 0.2936 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3520 Training loss: 1.2732 0.2946 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3520 Training loss: 1.2731 0.2943 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3520 Training loss: 1.2729 0.2931 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3520 Training loss: 1.2728 0.2943 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3520 Training loss: 1.2725 0.2955 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3520 Training loss: 1.2724 0.2936 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3520 Training loss: 1.2723 0.2937 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3520 Training loss: 1.2721 0.2949 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3520 Training loss: 1.2721 0.2938 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3520 Training loss: 1.2720 0.2946 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3520 Training loss: 1.2721 0.2932 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3520 Training loss: 1.2718 0.2936 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3520 Training loss: 1.2718 0.2951 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3520 Training loss: 1.2718 0.2934 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3520 Training loss: 1.2715 0.2947 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3520 Training loss: 1.2711 0.2948 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3520 Training loss: 1.2709 0.2939 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3520 Training loss: 1.2706 0.2932 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3520 Training loss: 1.2705 0.2971 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3520 Training loss: 1.2706 0.2943 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3520 Training loss: 1.2704 0.2945 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3520 Training loss: 1.2704 0.2946 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3520 Training loss: 1.2704 0.2945 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3520 Training loss: 1.2703 0.2934 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3520 Training loss: 1.2703 0.2932 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3520 Training loss: 1.2703 0.2946 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3520 Training loss: 1.2705 0.2946 sec/batch\n",
      "Epoch 12/20  Iteration 1937/3520 Training loss: 1.3509 0.2941 sec/batch\n",
      "Epoch 12/20  Iteration 1938/3520 Training loss: 1.2919 0.2938 sec/batch\n",
      "Epoch 12/20  Iteration 1939/3520 Training loss: 1.2794 0.2956 sec/batch\n",
      "Epoch 12/20  Iteration 1940/3520 Training loss: 1.2736 0.2969 sec/batch\n",
      "Epoch 12/20  Iteration 1941/3520 Training loss: 1.2725 0.2964 sec/batch\n",
      "Epoch 12/20  Iteration 1942/3520 Training loss: 1.2712 0.2933 sec/batch\n",
      "Epoch 12/20  Iteration 1943/3520 Training loss: 1.2669 0.2931 sec/batch\n",
      "Epoch 12/20  Iteration 1944/3520 Training loss: 1.2653 0.2940 sec/batch\n",
      "Epoch 12/20  Iteration 1945/3520 Training loss: 1.2620 0.2932 sec/batch\n",
      "Epoch 12/20  Iteration 1946/3520 Training loss: 1.2614 0.2937 sec/batch\n",
      "Epoch 12/20  Iteration 1947/3520 Training loss: 1.2613 0.2931 sec/batch\n",
      "Epoch 12/20  Iteration 1948/3520 Training loss: 1.2609 0.2934 sec/batch\n",
      "Epoch 12/20  Iteration 1949/3520 Training loss: 1.2596 0.2934 sec/batch\n",
      "Epoch 12/20  Iteration 1950/3520 Training loss: 1.2603 0.2946 sec/batch\n",
      "Epoch 12/20  Iteration 1951/3520 Training loss: 1.2603 0.2943 sec/batch\n",
      "Epoch 12/20  Iteration 1952/3520 Training loss: 1.2593 0.2935 sec/batch\n",
      "Epoch 12/20  Iteration 1953/3520 Training loss: 1.2580 0.2934 sec/batch\n",
      "Epoch 12/20  Iteration 1954/3520 Training loss: 1.2581 0.2941 sec/batch\n",
      "Epoch 12/20  Iteration 1955/3520 Training loss: 1.2569 0.2943 sec/batch\n",
      "Epoch 12/20  Iteration 1956/3520 Training loss: 1.2578 0.2940 sec/batch\n",
      "Epoch 12/20  Iteration 1957/3520 Training loss: 1.2583 0.2940 sec/batch\n",
      "Epoch 12/20  Iteration 1958/3520 Training loss: 1.2575 0.2936 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3520 Training loss: 1.2582 0.2933 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3520 Training loss: 1.2591 0.2945 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3520 Training loss: 1.2593 0.2933 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3520 Training loss: 1.2586 0.2931 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3520 Training loss: 1.2574 0.2932 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3520 Training loss: 1.2577 0.2932 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3520 Training loss: 1.2567 0.2961 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3520 Training loss: 1.2568 0.2935 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3520 Training loss: 1.2561 0.2949 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3520 Training loss: 1.2557 0.2940 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3520 Training loss: 1.2563 0.2954 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3520 Training loss: 1.2558 0.2964 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3520 Training loss: 1.2555 0.2938 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3520 Training loss: 1.2557 0.2949 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3520 Training loss: 1.2547 0.2938 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3520 Training loss: 1.2553 0.2944 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3520 Training loss: 1.2559 0.2946 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3520 Training loss: 1.2559 0.2940 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3520 Training loss: 1.2556 0.2948 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3520 Training loss: 1.2548 0.2938 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3520 Training loss: 1.2546 0.2938 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3520 Training loss: 1.2538 0.3047 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3520 Training loss: 1.2533 0.2934 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3520 Training loss: 1.2530 0.2934 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3520 Training loss: 1.2528 0.2950 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3520 Training loss: 1.2527 0.2952 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3520 Training loss: 1.2523 0.2947 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3520 Training loss: 1.2523 0.2954 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3520 Training loss: 1.2521 0.2946 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3520 Training loss: 1.2522 0.2942 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3520 Training loss: 1.2522 0.3045 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3520 Training loss: 1.2522 0.2948 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3520 Training loss: 1.2520 0.2946 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3520 Training loss: 1.2521 0.2951 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3520 Training loss: 1.2518 0.2953 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3520 Training loss: 1.2517 0.2946 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3520 Training loss: 1.2512 0.2962 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3520 Training loss: 1.2513 0.2950 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3520 Training loss: 1.2509 0.2949 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3520 Training loss: 1.2509 0.2950 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3520 Training loss: 1.2509 0.2951 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3520 Training loss: 1.2508 0.2955 sec/batch\n",
      "Validation loss: 1.15504 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 2001/3520 Training loss: 1.2529 0.2958 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3520 Training loss: 1.2530 0.2968 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3520 Training loss: 1.2529 0.2934 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3520 Training loss: 1.2524 0.2939 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3520 Training loss: 1.2521 0.2930 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3520 Training loss: 1.2521 0.2935 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3520 Training loss: 1.2519 0.2942 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3520 Training loss: 1.2516 0.2938 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3520 Training loss: 1.2512 0.2931 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3520 Training loss: 1.2505 0.2956 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3520 Training loss: 1.2502 0.2948 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3520 Training loss: 1.2504 0.2946 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3520 Training loss: 1.2506 0.2941 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3520 Training loss: 1.2509 0.2944 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3520 Training loss: 1.2507 0.2931 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3520 Training loss: 1.2504 0.2939 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3520 Training loss: 1.2501 0.2945 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3520 Training loss: 1.2504 0.2938 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3520 Training loss: 1.2501 0.2947 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3520 Training loss: 1.2502 0.2933 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3520 Training loss: 1.2500 0.2936 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3520 Training loss: 1.2494 0.2943 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3520 Training loss: 1.2496 0.3076 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3520 Training loss: 1.2493 0.2941 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3520 Training loss: 1.2495 0.2942 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3520 Training loss: 1.2492 0.2935 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3520 Training loss: 1.2491 0.2926 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3520 Training loss: 1.2493 0.2931 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3520 Training loss: 1.2489 0.2947 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3520 Training loss: 1.2487 0.2956 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3520 Training loss: 1.2485 0.2950 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3520 Training loss: 1.2481 0.2961 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3520 Training loss: 1.2483 0.2945 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3520 Training loss: 1.2484 0.2943 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3520 Training loss: 1.2484 0.2937 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3520 Training loss: 1.2482 0.2942 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3520 Training loss: 1.2481 0.3042 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3520 Training loss: 1.2480 0.2940 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3520 Training loss: 1.2479 0.2932 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3520 Training loss: 1.2473 0.2947 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3520 Training loss: 1.2470 0.2946 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3520 Training loss: 1.2471 0.2943 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3520 Training loss: 1.2471 0.2935 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3520 Training loss: 1.2468 0.2940 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3520 Training loss: 1.2468 0.2922 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3520 Training loss: 1.2467 0.2936 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3520 Training loss: 1.2467 0.2950 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3520 Training loss: 1.2468 0.2946 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3520 Training loss: 1.2469 0.2952 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3520 Training loss: 1.2467 0.2954 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3520 Training loss: 1.2465 0.2946 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3520 Training loss: 1.2464 0.2942 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3520 Training loss: 1.2462 0.2955 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3520 Training loss: 1.2463 0.2951 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3520 Training loss: 1.2463 0.2956 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3520 Training loss: 1.2462 0.2975 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3520 Training loss: 1.2461 0.2935 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3520 Training loss: 1.2461 0.2937 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3520 Training loss: 1.2460 0.2948 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3520 Training loss: 1.2459 0.2948 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3520 Training loss: 1.2457 0.2945 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3520 Training loss: 1.2454 0.2925 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3520 Training loss: 1.2452 0.2936 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3520 Training loss: 1.2453 0.2932 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3520 Training loss: 1.2452 0.2935 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3520 Training loss: 1.2453 0.2936 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3520 Training loss: 1.2452 0.2935 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3520 Training loss: 1.2455 0.2945 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3520 Training loss: 1.2455 0.2935 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3520 Training loss: 1.2455 0.2948 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3520 Training loss: 1.2456 0.2935 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3520 Training loss: 1.2455 0.2936 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3520 Training loss: 1.2455 0.2933 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3520 Training loss: 1.2453 0.2939 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3520 Training loss: 1.2453 0.2946 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3520 Training loss: 1.2454 0.2947 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3520 Training loss: 1.2455 0.2930 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3520 Training loss: 1.2453 0.2943 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3520 Training loss: 1.2450 0.2936 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3520 Training loss: 1.2449 0.2932 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3520 Training loss: 1.2449 0.2934 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3520 Training loss: 1.2450 0.2934 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3520 Training loss: 1.2448 0.2936 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3520 Training loss: 1.2447 0.2934 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3520 Training loss: 1.2447 0.2948 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3520 Training loss: 1.2448 0.2939 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3520 Training loss: 1.2447 0.2940 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3520 Training loss: 1.2445 0.2940 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3520 Training loss: 1.2444 0.2939 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3520 Training loss: 1.2442 0.2937 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3520 Training loss: 1.2441 0.2930 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3520 Training loss: 1.2441 0.2928 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3520 Training loss: 1.2438 0.2949 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3520 Training loss: 1.2439 0.2932 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3520 Training loss: 1.2439 0.2938 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3520 Training loss: 1.2439 0.2944 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3520 Training loss: 1.2437 0.2940 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3520 Training loss: 1.2437 0.2973 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3520 Training loss: 1.2437 0.2961 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3520 Training loss: 1.2435 0.3010 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3520 Training loss: 1.2432 0.2940 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3520 Training loss: 1.2430 0.2944 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3520 Training loss: 1.2429 0.2939 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3520 Training loss: 1.2429 0.2949 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3520 Training loss: 1.2430 0.2967 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3520 Training loss: 1.2429 0.2939 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3520 Training loss: 1.2429 0.2950 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3520 Training loss: 1.2429 0.2945 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3520 Training loss: 1.2428 0.2961 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3520 Training loss: 1.2428 0.2947 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3520 Training loss: 1.2429 0.2933 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3520 Training loss: 1.2431 0.2952 sec/batch\n",
      "Epoch 13/20  Iteration 2113/3520 Training loss: 1.3338 0.2936 sec/batch\n",
      "Epoch 13/20  Iteration 2114/3520 Training loss: 1.2692 0.2941 sec/batch\n",
      "Epoch 13/20  Iteration 2115/3520 Training loss: 1.2572 0.2954 sec/batch\n",
      "Epoch 13/20  Iteration 2116/3520 Training loss: 1.2526 0.2952 sec/batch\n",
      "Epoch 13/20  Iteration 2117/3520 Training loss: 1.2512 0.2943 sec/batch\n",
      "Epoch 13/20  Iteration 2118/3520 Training loss: 1.2482 0.3048 sec/batch\n",
      "Epoch 13/20  Iteration 2119/3520 Training loss: 1.2435 0.2938 sec/batch\n",
      "Epoch 13/20  Iteration 2120/3520 Training loss: 1.2429 0.2946 sec/batch\n",
      "Epoch 13/20  Iteration 2121/3520 Training loss: 1.2392 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2122/3520 Training loss: 1.2375 0.2954 sec/batch\n",
      "Epoch 13/20  Iteration 2123/3520 Training loss: 1.2377 0.2939 sec/batch\n",
      "Epoch 13/20  Iteration 2124/3520 Training loss: 1.2373 0.2945 sec/batch\n",
      "Epoch 13/20  Iteration 2125/3520 Training loss: 1.2361 0.2948 sec/batch\n",
      "Epoch 13/20  Iteration 2126/3520 Training loss: 1.2371 0.2938 sec/batch\n",
      "Epoch 13/20  Iteration 2127/3520 Training loss: 1.2360 0.2941 sec/batch\n",
      "Epoch 13/20  Iteration 2128/3520 Training loss: 1.2340 0.2935 sec/batch\n",
      "Epoch 13/20  Iteration 2129/3520 Training loss: 1.2326 0.2941 sec/batch\n",
      "Epoch 13/20  Iteration 2130/3520 Training loss: 1.2330 0.2958 sec/batch\n",
      "Epoch 13/20  Iteration 2131/3520 Training loss: 1.2320 0.2934 sec/batch\n",
      "Epoch 13/20  Iteration 2132/3520 Training loss: 1.2330 0.2941 sec/batch\n",
      "Epoch 13/20  Iteration 2133/3520 Training loss: 1.2334 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2134/3520 Training loss: 1.2327 0.2942 sec/batch\n",
      "Epoch 13/20  Iteration 2135/3520 Training loss: 1.2334 0.2982 sec/batch\n",
      "Epoch 13/20  Iteration 2136/3520 Training loss: 1.2345 0.2954 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3520 Training loss: 1.2351 0.2939 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3520 Training loss: 1.2343 0.2942 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3520 Training loss: 1.2335 0.2943 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3520 Training loss: 1.2336 0.2947 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3520 Training loss: 1.2326 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3520 Training loss: 1.2330 0.2938 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3520 Training loss: 1.2326 0.2947 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3520 Training loss: 1.2321 0.2929 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3520 Training loss: 1.2331 0.2938 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3520 Training loss: 1.2325 0.2936 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3520 Training loss: 1.2323 0.2945 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3520 Training loss: 1.2329 0.2949 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3520 Training loss: 1.2319 0.2946 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3520 Training loss: 1.2328 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3520 Training loss: 1.2336 0.2938 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3520 Training loss: 1.2334 0.2930 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3520 Training loss: 1.2333 0.2929 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3520 Training loss: 1.2327 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3520 Training loss: 1.2325 0.3059 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3520 Training loss: 1.2318 0.2945 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3520 Training loss: 1.2313 0.2941 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3520 Training loss: 1.2312 0.2946 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3520 Training loss: 1.2312 0.2939 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3520 Training loss: 1.2314 0.2936 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3520 Training loss: 1.2313 0.2935 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3520 Training loss: 1.2313 0.2926 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3520 Training loss: 1.2312 0.2933 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3520 Training loss: 1.2312 0.2948 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3520 Training loss: 1.2311 0.2933 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3520 Training loss: 1.2310 0.2929 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3520 Training loss: 1.2307 0.2951 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3520 Training loss: 1.2307 0.2932 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3520 Training loss: 1.2304 0.2944 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3520 Training loss: 1.2304 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3520 Training loss: 1.2298 0.2942 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3520 Training loss: 1.2300 0.2942 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3520 Training loss: 1.2298 0.2939 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3520 Training loss: 1.2297 0.2938 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3520 Training loss: 1.2296 0.2941 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3520 Training loss: 1.2295 0.2933 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3520 Training loss: 1.2291 0.2941 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3520 Training loss: 1.2293 0.2940 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3520 Training loss: 1.2290 0.2950 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3520 Training loss: 1.2284 0.2951 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3520 Training loss: 1.2281 0.2942 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3520 Training loss: 1.2280 0.2940 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3520 Training loss: 1.2278 0.2940 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3520 Training loss: 1.2277 0.2955 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3520 Training loss: 1.2274 0.2947 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3520 Training loss: 1.2269 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3520 Training loss: 1.2269 0.2939 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3520 Training loss: 1.2270 0.2951 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3520 Training loss: 1.2272 0.2944 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3520 Training loss: 1.2275 0.2945 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3520 Training loss: 1.2276 0.2944 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3520 Training loss: 1.2274 0.2944 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3520 Training loss: 1.2271 0.2938 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3520 Training loss: 1.2274 0.2942 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3520 Training loss: 1.2272 0.2946 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3520 Training loss: 1.2274 0.2943 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3520 Training loss: 1.2273 0.2950 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3520 Training loss: 1.2268 0.2948 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3520 Training loss: 1.2272 0.2944 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3520 Training loss: 1.2270 0.2946 sec/batch\n",
      "Validation loss: 1.14255 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 2201/3520 Training loss: 1.2291 0.2971 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3520 Training loss: 1.2290 0.2969 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3520 Training loss: 1.2288 0.2935 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3520 Training loss: 1.2290 0.2938 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3520 Training loss: 1.2287 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3520 Training loss: 1.2285 0.2944 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3520 Training loss: 1.2285 0.2940 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3520 Training loss: 1.2281 0.2962 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3520 Training loss: 1.2283 0.2954 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3520 Training loss: 1.2282 0.2936 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3520 Training loss: 1.2282 0.2934 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3520 Training loss: 1.2281 0.2936 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3520 Training loss: 1.2279 0.2935 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3520 Training loss: 1.2280 0.2939 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3520 Training loss: 1.2276 0.2934 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3520 Training loss: 1.2271 0.2948 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3520 Training loss: 1.2267 0.2947 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3520 Training loss: 1.2268 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3520 Training loss: 1.2268 0.2961 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3520 Training loss: 1.2265 0.2938 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3520 Training loss: 1.2265 0.2949 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3520 Training loss: 1.2263 0.2944 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3520 Training loss: 1.2264 0.2926 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3520 Training loss: 1.2265 0.2946 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3520 Training loss: 1.2265 0.2933 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3520 Training loss: 1.2263 0.2935 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3520 Training loss: 1.2262 0.2942 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3520 Training loss: 1.2262 0.2942 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3520 Training loss: 1.2260 0.2934 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3520 Training loss: 1.2261 0.2940 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3520 Training loss: 1.2261 0.2944 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3520 Training loss: 1.2260 0.2949 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3520 Training loss: 1.2259 0.2947 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3520 Training loss: 1.2259 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3520 Training loss: 1.2259 0.2947 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3520 Training loss: 1.2258 0.2941 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3520 Training loss: 1.2257 0.2932 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3520 Training loss: 1.2254 0.2945 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3520 Training loss: 1.2252 0.2927 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3520 Training loss: 1.2253 0.2964 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3520 Training loss: 1.2252 0.2953 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3520 Training loss: 1.2253 0.2949 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3520 Training loss: 1.2252 0.2949 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3520 Training loss: 1.2255 0.2940 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3520 Training loss: 1.2255 0.2947 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3520 Training loss: 1.2256 0.2946 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3520 Training loss: 1.2258 0.2987 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3520 Training loss: 1.2257 0.2940 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3520 Training loss: 1.2257 0.2952 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3520 Training loss: 1.2256 0.2957 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3520 Training loss: 1.2257 0.2946 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3520 Training loss: 1.2258 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3520 Training loss: 1.2258 0.2938 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3520 Training loss: 1.2256 0.3025 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3520 Training loss: 1.2253 0.2941 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3520 Training loss: 1.2252 0.2936 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3520 Training loss: 1.2254 0.2957 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3520 Training loss: 1.2253 0.2929 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3520 Training loss: 1.2253 0.2938 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3520 Training loss: 1.2252 0.2945 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3520 Training loss: 1.2252 0.2947 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3520 Training loss: 1.2253 0.2948 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3520 Training loss: 1.2252 0.2939 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3520 Training loss: 1.2250 0.2935 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3520 Training loss: 1.2250 0.2930 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3520 Training loss: 1.2248 0.2936 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3520 Training loss: 1.2246 0.2940 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3520 Training loss: 1.2246 0.2951 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3520 Training loss: 1.2244 0.2941 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3520 Training loss: 1.2244 0.2936 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3520 Training loss: 1.2244 0.2942 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3520 Training loss: 1.2245 0.2940 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3520 Training loss: 1.2243 0.2986 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3520 Training loss: 1.2244 0.2949 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3520 Training loss: 1.2245 0.2946 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3520 Training loss: 1.2243 0.2947 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3520 Training loss: 1.2240 0.2952 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3520 Training loss: 1.2238 0.2936 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3520 Training loss: 1.2236 0.2939 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3520 Training loss: 1.2236 0.2940 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3520 Training loss: 1.2237 0.2944 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3520 Training loss: 1.2236 0.2952 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3520 Training loss: 1.2236 0.2951 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3520 Training loss: 1.2236 0.2951 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3520 Training loss: 1.2235 0.2937 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3520 Training loss: 1.2235 0.2952 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3520 Training loss: 1.2235 0.2957 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3520 Training loss: 1.2237 0.2946 sec/batch\n",
      "Epoch 14/20  Iteration 2289/3520 Training loss: 1.3196 0.2943 sec/batch\n",
      "Epoch 14/20  Iteration 2290/3520 Training loss: 1.2549 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2291/3520 Training loss: 1.2434 0.2947 sec/batch\n",
      "Epoch 14/20  Iteration 2292/3520 Training loss: 1.2390 0.2939 sec/batch\n",
      "Epoch 14/20  Iteration 2293/3520 Training loss: 1.2390 0.2960 sec/batch\n",
      "Epoch 14/20  Iteration 2294/3520 Training loss: 1.2362 0.2933 sec/batch\n",
      "Epoch 14/20  Iteration 2295/3520 Training loss: 1.2307 0.2948 sec/batch\n",
      "Epoch 14/20  Iteration 2296/3520 Training loss: 1.2312 0.2953 sec/batch\n",
      "Epoch 14/20  Iteration 2297/3520 Training loss: 1.2275 0.2966 sec/batch\n",
      "Epoch 14/20  Iteration 2298/3520 Training loss: 1.2259 0.2942 sec/batch\n",
      "Epoch 14/20  Iteration 2299/3520 Training loss: 1.2259 0.2943 sec/batch\n",
      "Epoch 14/20  Iteration 2300/3520 Training loss: 1.2252 0.2947 sec/batch\n",
      "Epoch 14/20  Iteration 2301/3520 Training loss: 1.2238 0.2950 sec/batch\n",
      "Epoch 14/20  Iteration 2302/3520 Training loss: 1.2246 0.2946 sec/batch\n",
      "Epoch 14/20  Iteration 2303/3520 Training loss: 1.2241 0.2947 sec/batch\n",
      "Epoch 14/20  Iteration 2304/3520 Training loss: 1.2222 0.2955 sec/batch\n",
      "Epoch 14/20  Iteration 2305/3520 Training loss: 1.2213 0.2964 sec/batch\n",
      "Epoch 14/20  Iteration 2306/3520 Training loss: 1.2218 0.2948 sec/batch\n",
      "Epoch 14/20  Iteration 2307/3520 Training loss: 1.2208 0.2946 sec/batch\n",
      "Epoch 14/20  Iteration 2308/3520 Training loss: 1.2212 0.2940 sec/batch\n",
      "Epoch 14/20  Iteration 2309/3520 Training loss: 1.2214 0.2939 sec/batch\n",
      "Epoch 14/20  Iteration 2310/3520 Training loss: 1.2203 0.2929 sec/batch\n",
      "Epoch 14/20  Iteration 2311/3520 Training loss: 1.2204 0.2957 sec/batch\n",
      "Epoch 14/20  Iteration 2312/3520 Training loss: 1.2210 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2313/3520 Training loss: 1.2213 0.2937 sec/batch\n",
      "Epoch 14/20  Iteration 2314/3520 Training loss: 1.2208 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3520 Training loss: 1.2198 0.2952 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3520 Training loss: 1.2196 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3520 Training loss: 1.2186 0.2939 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3520 Training loss: 1.2186 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3520 Training loss: 1.2180 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3520 Training loss: 1.2175 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3520 Training loss: 1.2182 0.2944 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3520 Training loss: 1.2176 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3520 Training loss: 1.2174 0.3058 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3520 Training loss: 1.2177 0.2975 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3520 Training loss: 1.2167 0.2957 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3520 Training loss: 1.2174 0.2935 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3520 Training loss: 1.2179 0.2943 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3520 Training loss: 1.2180 0.2927 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3520 Training loss: 1.2177 0.2929 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3520 Training loss: 1.2168 0.2943 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3520 Training loss: 1.2167 0.2942 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3520 Training loss: 1.2158 0.2942 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3520 Training loss: 1.2155 0.2931 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3520 Training loss: 1.2150 0.2939 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3520 Training loss: 1.2151 0.2941 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3520 Training loss: 1.2151 0.2941 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3520 Training loss: 1.2149 0.2949 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3520 Training loss: 1.2148 0.2966 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3520 Training loss: 1.2146 0.2944 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3520 Training loss: 1.2145 0.2937 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3520 Training loss: 1.2145 0.2949 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3520 Training loss: 1.2143 0.2939 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3520 Training loss: 1.2141 0.2935 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3520 Training loss: 1.2141 0.2961 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3520 Training loss: 1.2138 0.2932 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3520 Training loss: 1.2138 0.2942 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3520 Training loss: 1.2131 0.2935 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3520 Training loss: 1.2133 0.2934 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3520 Training loss: 1.2132 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3520 Training loss: 1.2131 0.2945 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3520 Training loss: 1.2133 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3520 Training loss: 1.2131 0.3049 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3520 Training loss: 1.2126 0.2932 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3520 Training loss: 1.2127 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3520 Training loss: 1.2125 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3520 Training loss: 1.2121 0.2939 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3520 Training loss: 1.2119 0.2934 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3520 Training loss: 1.2119 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3520 Training loss: 1.2118 0.3034 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3520 Training loss: 1.2115 0.2937 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3520 Training loss: 1.2112 0.2941 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3520 Training loss: 1.2107 0.2925 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3520 Training loss: 1.2106 0.2934 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3520 Training loss: 1.2106 0.2947 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3520 Training loss: 1.2108 0.2939 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3520 Training loss: 1.2112 0.2935 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3520 Training loss: 1.2111 0.2940 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3520 Training loss: 1.2108 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3520 Training loss: 1.2105 0.2933 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3520 Training loss: 1.2108 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3520 Training loss: 1.2106 0.2934 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3520 Training loss: 1.2107 0.2935 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3520 Training loss: 1.2106 0.2943 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3520 Training loss: 1.2102 0.2947 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3520 Training loss: 1.2105 0.2949 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3520 Training loss: 1.2103 0.2940 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3520 Training loss: 1.2105 0.2941 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3520 Training loss: 1.2102 0.2945 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3520 Training loss: 1.2100 0.2950 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3520 Training loss: 1.2102 0.2937 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3520 Training loss: 1.2100 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3520 Training loss: 1.2098 0.2942 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3520 Training loss: 1.2097 0.2940 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3520 Training loss: 1.2093 0.2934 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3520 Training loss: 1.2095 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3520 Training loss: 1.2095 0.2943 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3520 Training loss: 1.2095 0.2955 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3520 Training loss: 1.2093 0.2946 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3520 Training loss: 1.2092 0.2952 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3520 Training loss: 1.2092 0.2948 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3520 Training loss: 1.2090 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3520 Training loss: 1.2085 0.2935 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3520 Training loss: 1.2081 0.2940 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3520 Training loss: 1.2081 0.2942 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3520 Training loss: 1.2081 0.2947 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3520 Training loss: 1.2079 0.2961 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3520 Training loss: 1.2079 0.2944 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3520 Training loss: 1.2078 0.2929 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3520 Training loss: 1.2078 0.2932 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3520 Training loss: 1.2080 0.2934 sec/batch\n",
      "Validation loss: 1.12903 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 2401/3520 Training loss: 1.2097 0.2968 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3520 Training loss: 1.2095 0.2966 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3520 Training loss: 1.2094 0.2929 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3520 Training loss: 1.2093 0.2932 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3520 Training loss: 1.2092 0.2937 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3520 Training loss: 1.2093 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3520 Training loss: 1.2094 0.2930 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3520 Training loss: 1.2092 0.2979 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3520 Training loss: 1.2092 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3520 Training loss: 1.2092 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3520 Training loss: 1.2092 0.2941 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3520 Training loss: 1.2092 0.2935 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3520 Training loss: 1.2090 0.2939 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3520 Training loss: 1.2088 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3520 Training loss: 1.2086 0.2951 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3520 Training loss: 1.2086 0.2951 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3520 Training loss: 1.2085 0.2937 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3520 Training loss: 1.2086 0.2935 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3520 Training loss: 1.2085 0.2946 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3520 Training loss: 1.2088 0.3038 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3520 Training loss: 1.2087 0.2948 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3520 Training loss: 1.2088 0.2939 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3520 Training loss: 1.2088 0.2952 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3520 Training loss: 1.2088 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3520 Training loss: 1.2087 0.2942 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3520 Training loss: 1.2085 0.3059 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3520 Training loss: 1.2086 0.2949 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3520 Training loss: 1.2087 0.2947 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3520 Training loss: 1.2088 0.2961 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3520 Training loss: 1.2086 0.2939 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3520 Training loss: 1.2084 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3520 Training loss: 1.2083 0.2943 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3520 Training loss: 1.2085 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3520 Training loss: 1.2085 0.2940 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3520 Training loss: 1.2084 0.2950 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3520 Training loss: 1.2084 0.2943 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3520 Training loss: 1.2083 0.2940 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3520 Training loss: 1.2084 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3520 Training loss: 1.2082 0.2938 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3520 Training loss: 1.2080 0.2939 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3520 Training loss: 1.2080 0.2934 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3520 Training loss: 1.2078 0.2943 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3520 Training loss: 1.2077 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3520 Training loss: 1.2076 0.2933 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3520 Training loss: 1.2074 0.2933 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3520 Training loss: 1.2075 0.2946 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3520 Training loss: 1.2075 0.2942 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3520 Training loss: 1.2075 0.2945 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3520 Training loss: 1.2073 0.2945 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3520 Training loss: 1.2073 0.2948 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3520 Training loss: 1.2073 0.2984 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3520 Training loss: 1.2072 0.2951 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3520 Training loss: 1.2068 0.2942 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3520 Training loss: 1.2066 0.2932 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3520 Training loss: 1.2064 0.2943 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3520 Training loss: 1.2064 0.2931 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3520 Training loss: 1.2064 0.2931 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3520 Training loss: 1.2063 0.2933 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3520 Training loss: 1.2063 0.2936 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3520 Training loss: 1.2063 0.2937 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3520 Training loss: 1.2063 0.2934 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3520 Training loss: 1.2063 0.2943 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3520 Training loss: 1.2063 0.2944 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3520 Training loss: 1.2065 0.3035 sec/batch\n",
      "Epoch 15/20  Iteration 2465/3520 Training loss: 1.2994 0.2946 sec/batch\n",
      "Epoch 15/20  Iteration 2466/3520 Training loss: 1.2375 0.2937 sec/batch\n",
      "Epoch 15/20  Iteration 2467/3520 Training loss: 1.2200 0.2942 sec/batch\n",
      "Epoch 15/20  Iteration 2468/3520 Training loss: 1.2150 0.2950 sec/batch\n",
      "Epoch 15/20  Iteration 2469/3520 Training loss: 1.2130 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2470/3520 Training loss: 1.2114 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2471/3520 Training loss: 1.2076 0.2942 sec/batch\n",
      "Epoch 15/20  Iteration 2472/3520 Training loss: 1.2055 0.2948 sec/batch\n",
      "Epoch 15/20  Iteration 2473/3520 Training loss: 1.2024 0.2932 sec/batch\n",
      "Epoch 15/20  Iteration 2474/3520 Training loss: 1.2013 0.2934 sec/batch\n",
      "Epoch 15/20  Iteration 2475/3520 Training loss: 1.2019 0.2934 sec/batch\n",
      "Epoch 15/20  Iteration 2476/3520 Training loss: 1.2011 0.2937 sec/batch\n",
      "Epoch 15/20  Iteration 2477/3520 Training loss: 1.2000 0.2937 sec/batch\n",
      "Epoch 15/20  Iteration 2478/3520 Training loss: 1.2021 0.2930 sec/batch\n",
      "Epoch 15/20  Iteration 2479/3520 Training loss: 1.2012 0.2934 sec/batch\n",
      "Epoch 15/20  Iteration 2480/3520 Training loss: 1.2000 0.2941 sec/batch\n",
      "Epoch 15/20  Iteration 2481/3520 Training loss: 1.1990 0.2935 sec/batch\n",
      "Epoch 15/20  Iteration 2482/3520 Training loss: 1.1991 0.2936 sec/batch\n",
      "Epoch 15/20  Iteration 2483/3520 Training loss: 1.1986 0.2945 sec/batch\n",
      "Epoch 15/20  Iteration 2484/3520 Training loss: 1.1994 0.2945 sec/batch\n",
      "Epoch 15/20  Iteration 2485/3520 Training loss: 1.1999 0.2944 sec/batch\n",
      "Epoch 15/20  Iteration 2486/3520 Training loss: 1.1996 0.2935 sec/batch\n",
      "Epoch 15/20  Iteration 2487/3520 Training loss: 1.2003 0.2931 sec/batch\n",
      "Epoch 15/20  Iteration 2488/3520 Training loss: 1.2012 0.2948 sec/batch\n",
      "Epoch 15/20  Iteration 2489/3520 Training loss: 1.2018 0.2940 sec/batch\n",
      "Epoch 15/20  Iteration 2490/3520 Training loss: 1.2013 0.2938 sec/batch\n",
      "Epoch 15/20  Iteration 2491/3520 Training loss: 1.2004 0.2939 sec/batch\n",
      "Epoch 15/20  Iteration 2492/3520 Training loss: 1.2004 0.2939 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3520 Training loss: 1.1994 0.2934 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3520 Training loss: 1.1998 0.2937 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3520 Training loss: 1.1991 0.2944 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3520 Training loss: 1.1991 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3520 Training loss: 1.1999 0.2941 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3520 Training loss: 1.1995 0.2943 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3520 Training loss: 1.1992 0.2935 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3520 Training loss: 1.1995 0.2936 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3520 Training loss: 1.1988 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3520 Training loss: 1.1993 0.2941 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3520 Training loss: 1.1999 0.2945 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3520 Training loss: 1.2000 0.2945 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3520 Training loss: 1.1998 0.2935 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3520 Training loss: 1.1989 0.2948 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3520 Training loss: 1.1987 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3520 Training loss: 1.1981 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3520 Training loss: 1.1978 0.2978 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3520 Training loss: 1.1976 0.2942 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3520 Training loss: 1.1976 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3520 Training loss: 1.1978 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3520 Training loss: 1.1977 0.2966 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3520 Training loss: 1.1978 0.2954 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3520 Training loss: 1.1976 0.3038 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3520 Training loss: 1.1976 0.2944 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3520 Training loss: 1.1976 0.2944 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3520 Training loss: 1.1973 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3520 Training loss: 1.1971 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3520 Training loss: 1.1972 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3520 Training loss: 1.1972 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3520 Training loss: 1.1971 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3520 Training loss: 1.1965 0.2976 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3520 Training loss: 1.1967 0.2950 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3520 Training loss: 1.1967 0.2948 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3520 Training loss: 1.1966 0.2944 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3520 Training loss: 1.1966 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3520 Training loss: 1.1965 0.2949 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3520 Training loss: 1.1960 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3520 Training loss: 1.1961 0.2939 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3520 Training loss: 1.1960 0.2946 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3520 Training loss: 1.1956 0.2937 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3520 Training loss: 1.1953 0.2934 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3520 Training loss: 1.1952 0.2943 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3520 Training loss: 1.1951 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3520 Training loss: 1.1949 0.2945 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3520 Training loss: 1.1946 0.2944 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3520 Training loss: 1.1940 0.2949 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3520 Training loss: 1.1938 0.3053 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3520 Training loss: 1.1939 0.2949 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3520 Training loss: 1.1941 0.2942 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3520 Training loss: 1.1945 0.2954 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3520 Training loss: 1.1945 0.2972 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3520 Training loss: 1.1944 0.2955 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3520 Training loss: 1.1940 0.2938 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3520 Training loss: 1.1943 0.2942 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3520 Training loss: 1.1941 0.2937 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3520 Training loss: 1.1943 0.2946 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3520 Training loss: 1.1942 0.2936 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3520 Training loss: 1.1938 0.2941 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3520 Training loss: 1.1940 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3520 Training loss: 1.1938 0.2943 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3520 Training loss: 1.1939 0.2950 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3520 Training loss: 1.1936 0.2945 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3520 Training loss: 1.1934 0.2954 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3520 Training loss: 1.1936 0.2950 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3520 Training loss: 1.1934 0.2954 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3520 Training loss: 1.1933 0.3080 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3520 Training loss: 1.1931 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3520 Training loss: 1.1928 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3520 Training loss: 1.1930 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3520 Training loss: 1.1930 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3520 Training loss: 1.1931 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3520 Training loss: 1.1929 0.2949 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3520 Training loss: 1.1928 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3520 Training loss: 1.1928 0.2938 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3520 Training loss: 1.1925 0.2953 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3520 Training loss: 1.1921 0.2949 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3520 Training loss: 1.1917 0.2948 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3520 Training loss: 1.1917 0.2968 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3520 Training loss: 1.1918 0.2949 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3520 Training loss: 1.1916 0.2946 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3520 Training loss: 1.1916 0.2944 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3520 Training loss: 1.1915 0.2949 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3520 Training loss: 1.1917 0.2945 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3520 Training loss: 1.1918 0.2960 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3520 Training loss: 1.1919 0.2965 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3520 Training loss: 1.1918 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3520 Training loss: 1.1917 0.2953 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3520 Training loss: 1.1916 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3520 Training loss: 1.1913 0.2954 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3520 Training loss: 1.1915 0.2934 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3520 Training loss: 1.1915 0.2949 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3520 Training loss: 1.1916 0.2946 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3520 Training loss: 1.1915 0.2949 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3520 Training loss: 1.1915 0.2953 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3520 Training loss: 1.1914 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3520 Training loss: 1.1914 0.2936 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3520 Training loss: 1.1914 0.2950 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3520 Training loss: 1.1911 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3520 Training loss: 1.1910 0.2938 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3520 Training loss: 1.1911 0.2940 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3520 Training loss: 1.1910 0.2944 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3520 Training loss: 1.1911 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3520 Training loss: 1.1910 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3520 Training loss: 1.1914 0.2956 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3520 Training loss: 1.1913 0.2955 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3520 Training loss: 1.1914 0.2960 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3520 Training loss: 1.1914 0.2941 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3520 Training loss: 1.1914 0.2946 sec/batch\n",
      "Validation loss: 1.11716 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 2601/3520 Training loss: 1.1929 0.2968 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3520 Training loss: 1.1927 0.2965 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3520 Training loss: 1.1929 0.2943 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3520 Training loss: 1.1929 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3520 Training loss: 1.1929 0.2948 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3520 Training loss: 1.1928 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3520 Training loss: 1.1926 0.3035 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3520 Training loss: 1.1925 0.2937 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3520 Training loss: 1.1927 0.2950 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3520 Training loss: 1.1927 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3520 Training loss: 1.1926 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3520 Training loss: 1.1926 0.2946 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3520 Training loss: 1.1925 0.2945 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3520 Training loss: 1.1927 0.2955 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3520 Training loss: 1.1926 0.2956 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3520 Training loss: 1.1925 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3520 Training loss: 1.1924 0.2943 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3520 Training loss: 1.1923 0.2955 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3520 Training loss: 1.1922 0.2943 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3520 Training loss: 1.1921 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3520 Training loss: 1.1919 0.2955 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3520 Training loss: 1.1919 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3520 Training loss: 1.1920 0.2950 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3520 Training loss: 1.1921 0.2952 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3520 Training loss: 1.1919 0.2947 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3520 Training loss: 1.1918 0.2939 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3520 Training loss: 1.1918 0.2946 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3520 Training loss: 1.1916 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3520 Training loss: 1.1912 0.2946 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3520 Training loss: 1.1911 0.3047 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3520 Training loss: 1.1909 0.2953 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3520 Training loss: 1.1909 0.2951 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3520 Training loss: 1.1910 0.2941 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3520 Training loss: 1.1909 0.2938 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3520 Training loss: 1.1909 0.2949 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3520 Training loss: 1.1909 0.2938 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3520 Training loss: 1.1909 0.2968 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3520 Training loss: 1.1909 0.2929 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3520 Training loss: 1.1909 0.2945 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3520 Training loss: 1.1911 0.2940 sec/batch\n",
      "Epoch 16/20  Iteration 2641/3520 Training loss: 1.2762 0.2945 sec/batch\n",
      "Epoch 16/20  Iteration 2642/3520 Training loss: 1.2191 0.2947 sec/batch\n",
      "Epoch 16/20  Iteration 2643/3520 Training loss: 1.2076 0.2949 sec/batch\n",
      "Epoch 16/20  Iteration 2644/3520 Training loss: 1.2025 0.2936 sec/batch\n",
      "Epoch 16/20  Iteration 2645/3520 Training loss: 1.2015 0.2933 sec/batch\n",
      "Epoch 16/20  Iteration 2646/3520 Training loss: 1.1991 0.2937 sec/batch\n",
      "Epoch 16/20  Iteration 2647/3520 Training loss: 1.1938 0.2950 sec/batch\n",
      "Epoch 16/20  Iteration 2648/3520 Training loss: 1.1929 0.2948 sec/batch\n",
      "Epoch 16/20  Iteration 2649/3520 Training loss: 1.1897 0.2944 sec/batch\n",
      "Epoch 16/20  Iteration 2650/3520 Training loss: 1.1886 0.2961 sec/batch\n",
      "Epoch 16/20  Iteration 2651/3520 Training loss: 1.1890 0.2947 sec/batch\n",
      "Epoch 16/20  Iteration 2652/3520 Training loss: 1.1885 0.3043 sec/batch\n",
      "Epoch 16/20  Iteration 2653/3520 Training loss: 1.1874 0.2952 sec/batch\n",
      "Epoch 16/20  Iteration 2654/3520 Training loss: 1.1893 0.2941 sec/batch\n",
      "Epoch 16/20  Iteration 2655/3520 Training loss: 1.1888 0.2948 sec/batch\n",
      "Epoch 16/20  Iteration 2656/3520 Training loss: 1.1867 0.2951 sec/batch\n",
      "Epoch 16/20  Iteration 2657/3520 Training loss: 1.1858 0.2947 sec/batch\n",
      "Epoch 16/20  Iteration 2658/3520 Training loss: 1.1867 0.2953 sec/batch\n",
      "Epoch 16/20  Iteration 2659/3520 Training loss: 1.1858 0.2937 sec/batch\n",
      "Epoch 16/20  Iteration 2660/3520 Training loss: 1.1865 0.2951 sec/batch\n",
      "Epoch 16/20  Iteration 2661/3520 Training loss: 1.1868 0.2941 sec/batch\n",
      "Epoch 16/20  Iteration 2662/3520 Training loss: 1.1856 0.2939 sec/batch\n",
      "Epoch 16/20  Iteration 2663/3520 Training loss: 1.1866 0.2946 sec/batch\n",
      "Epoch 16/20  Iteration 2664/3520 Training loss: 1.1874 0.2949 sec/batch\n",
      "Epoch 16/20  Iteration 2665/3520 Training loss: 1.1880 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2666/3520 Training loss: 1.1876 0.2959 sec/batch\n",
      "Epoch 16/20  Iteration 2667/3520 Training loss: 1.1867 0.2945 sec/batch\n",
      "Epoch 16/20  Iteration 2668/3520 Training loss: 1.1865 0.2954 sec/batch\n",
      "Epoch 16/20  Iteration 2669/3520 Training loss: 1.1856 0.2936 sec/batch\n",
      "Epoch 16/20  Iteration 2670/3520 Training loss: 1.1860 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3520 Training loss: 1.1856 0.2943 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3520 Training loss: 1.1855 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3520 Training loss: 1.1861 0.2947 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3520 Training loss: 1.1858 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3520 Training loss: 1.1856 0.2945 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3520 Training loss: 1.1859 0.2937 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3520 Training loss: 1.1851 0.2940 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3520 Training loss: 1.1856 0.2928 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3520 Training loss: 1.1860 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3520 Training loss: 1.1864 0.2929 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3520 Training loss: 1.1859 0.2937 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3520 Training loss: 1.1853 0.2940 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3520 Training loss: 1.1852 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3520 Training loss: 1.1846 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3520 Training loss: 1.1843 0.2937 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3520 Training loss: 1.1840 0.2953 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3520 Training loss: 1.1840 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3520 Training loss: 1.1841 0.2950 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3520 Training loss: 1.1839 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3520 Training loss: 1.1838 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3520 Training loss: 1.1836 0.2945 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3520 Training loss: 1.1837 0.2932 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3520 Training loss: 1.1837 0.2950 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3520 Training loss: 1.1835 0.2939 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3520 Training loss: 1.1832 0.2944 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3520 Training loss: 1.1832 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3520 Training loss: 1.1831 0.2939 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3520 Training loss: 1.1831 0.2946 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3520 Training loss: 1.1826 0.2940 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3520 Training loss: 1.1826 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3520 Training loss: 1.1825 0.2948 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3520 Training loss: 1.1825 0.2956 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3520 Training loss: 1.1827 0.2937 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3520 Training loss: 1.1827 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3520 Training loss: 1.1823 0.2943 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3520 Training loss: 1.1824 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3520 Training loss: 1.1823 0.2945 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3520 Training loss: 1.1819 0.2937 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3520 Training loss: 1.1817 0.2943 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3520 Training loss: 1.1817 0.2933 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3520 Training loss: 1.1816 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3520 Training loss: 1.1815 0.2933 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3520 Training loss: 1.1813 0.2946 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3520 Training loss: 1.1808 0.2951 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3520 Training loss: 1.1806 0.2944 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3520 Training loss: 1.1808 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3520 Training loss: 1.1810 0.2943 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3520 Training loss: 1.1815 0.2948 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3520 Training loss: 1.1815 0.3055 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3520 Training loss: 1.1812 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3520 Training loss: 1.1810 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3520 Training loss: 1.1813 0.2927 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3520 Training loss: 1.1811 0.2944 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3520 Training loss: 1.1814 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3520 Training loss: 1.1813 0.2977 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3520 Training loss: 1.1809 0.2936 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3520 Training loss: 1.1811 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3520 Training loss: 1.1809 0.2941 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3520 Training loss: 1.1812 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3520 Training loss: 1.1809 0.2939 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3520 Training loss: 1.1808 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3520 Training loss: 1.1810 0.2945 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3520 Training loss: 1.1807 0.2952 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3520 Training loss: 1.1806 0.2948 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3520 Training loss: 1.1805 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3520 Training loss: 1.1802 0.2930 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3520 Training loss: 1.1804 0.2944 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3520 Training loss: 1.1803 0.2927 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3520 Training loss: 1.1803 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3520 Training loss: 1.1802 0.2931 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3520 Training loss: 1.1801 0.2943 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3520 Training loss: 1.1802 0.2942 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3520 Training loss: 1.1800 0.2949 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3520 Training loss: 1.1795 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3520 Training loss: 1.1791 0.2944 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3520 Training loss: 1.1792 0.3052 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3520 Training loss: 1.1792 0.2942 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3520 Training loss: 1.1789 0.2955 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3520 Training loss: 1.1789 0.2950 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3520 Training loss: 1.1790 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3520 Training loss: 1.1791 0.2941 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3520 Training loss: 1.1792 0.2943 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3520 Training loss: 1.1792 0.2945 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3520 Training loss: 1.1791 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3520 Training loss: 1.1790 0.2926 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3520 Training loss: 1.1789 0.2942 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3520 Training loss: 1.1789 0.2928 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3520 Training loss: 1.1790 0.2952 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3520 Training loss: 1.1791 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3520 Training loss: 1.1790 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3520 Training loss: 1.1788 0.2935 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3520 Training loss: 1.1788 0.2936 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3520 Training loss: 1.1786 0.2940 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3520 Training loss: 1.1786 0.2936 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3520 Training loss: 1.1786 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3520 Training loss: 1.1783 0.2933 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3520 Training loss: 1.1782 0.2943 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3520 Training loss: 1.1783 0.2936 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3520 Training loss: 1.1782 0.2943 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3520 Training loss: 1.1784 0.2926 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3520 Training loss: 1.1784 0.2942 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3520 Training loss: 1.1786 0.2941 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3520 Training loss: 1.1785 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3520 Training loss: 1.1786 0.2940 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3520 Training loss: 1.1787 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3520 Training loss: 1.1787 0.2949 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3520 Training loss: 1.1788 0.2955 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3520 Training loss: 1.1786 0.2947 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3520 Training loss: 1.1787 0.2951 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3520 Training loss: 1.1788 0.2942 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3520 Training loss: 1.1788 0.2950 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3520 Training loss: 1.1786 0.2950 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3520 Training loss: 1.1783 0.2936 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3520 Training loss: 1.1783 0.2936 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3520 Training loss: 1.1783 0.2943 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3520 Training loss: 1.1783 0.2927 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3520 Training loss: 1.1782 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3520 Training loss: 1.1782 0.2932 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3520 Training loss: 1.1781 0.2946 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3520 Training loss: 1.1783 0.2945 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3520 Training loss: 1.1781 0.2936 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3520 Training loss: 1.1780 0.2946 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3520 Training loss: 1.1779 0.2949 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3520 Training loss: 1.1778 0.2947 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3520 Training loss: 1.1777 0.2940 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3520 Training loss: 1.1777 0.2933 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3520 Training loss: 1.1775 0.2947 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3520 Training loss: 1.1775 0.2944 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3520 Training loss: 1.1776 0.2951 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3520 Training loss: 1.1777 0.2934 sec/batch\n",
      "Validation loss: 1.10925 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 2801/3520 Training loss: 1.1787 0.2966 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3520 Training loss: 1.1788 0.2967 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3520 Training loss: 1.1788 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3520 Training loss: 1.1787 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3520 Training loss: 1.1783 0.2948 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3520 Training loss: 1.1783 0.2947 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3520 Training loss: 1.1781 0.2951 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3520 Training loss: 1.1781 0.2939 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3520 Training loss: 1.1782 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3520 Training loss: 1.1782 0.2934 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3520 Training loss: 1.1782 0.2946 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3520 Training loss: 1.1783 0.2938 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3520 Training loss: 1.1782 0.2947 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3520 Training loss: 1.1783 0.2940 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3520 Training loss: 1.1784 0.2942 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3520 Training loss: 1.1786 0.2942 sec/batch\n",
      "Epoch 17/20  Iteration 2817/3520 Training loss: 1.2725 0.2930 sec/batch\n",
      "Epoch 17/20  Iteration 2818/3520 Training loss: 1.2096 0.2933 sec/batch\n",
      "Epoch 17/20  Iteration 2819/3520 Training loss: 1.1971 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2820/3520 Training loss: 1.1931 0.2949 sec/batch\n",
      "Epoch 17/20  Iteration 2821/3520 Training loss: 1.1908 0.2934 sec/batch\n",
      "Epoch 17/20  Iteration 2822/3520 Training loss: 1.1876 0.2939 sec/batch\n",
      "Epoch 17/20  Iteration 2823/3520 Training loss: 1.1835 0.2936 sec/batch\n",
      "Epoch 17/20  Iteration 2824/3520 Training loss: 1.1812 0.2931 sec/batch\n",
      "Epoch 17/20  Iteration 2825/3520 Training loss: 1.1780 0.2948 sec/batch\n",
      "Epoch 17/20  Iteration 2826/3520 Training loss: 1.1773 0.2936 sec/batch\n",
      "Epoch 17/20  Iteration 2827/3520 Training loss: 1.1770 0.2948 sec/batch\n",
      "Epoch 17/20  Iteration 2828/3520 Training loss: 1.1776 0.2938 sec/batch\n",
      "Epoch 17/20  Iteration 2829/3520 Training loss: 1.1758 0.2949 sec/batch\n",
      "Epoch 17/20  Iteration 2830/3520 Training loss: 1.1765 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2831/3520 Training loss: 1.1764 0.2949 sec/batch\n",
      "Epoch 17/20  Iteration 2832/3520 Training loss: 1.1749 0.2950 sec/batch\n",
      "Epoch 17/20  Iteration 2833/3520 Training loss: 1.1739 0.2944 sec/batch\n",
      "Epoch 17/20  Iteration 2834/3520 Training loss: 1.1743 0.2941 sec/batch\n",
      "Epoch 17/20  Iteration 2835/3520 Training loss: 1.1733 0.2946 sec/batch\n",
      "Epoch 17/20  Iteration 2836/3520 Training loss: 1.1740 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2837/3520 Training loss: 1.1743 0.2947 sec/batch\n",
      "Epoch 17/20  Iteration 2838/3520 Training loss: 1.1736 0.2977 sec/batch\n",
      "Epoch 17/20  Iteration 2839/3520 Training loss: 1.1743 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2840/3520 Training loss: 1.1752 0.2949 sec/batch\n",
      "Epoch 17/20  Iteration 2841/3520 Training loss: 1.1755 0.2944 sec/batch\n",
      "Epoch 17/20  Iteration 2842/3520 Training loss: 1.1749 0.2940 sec/batch\n",
      "Epoch 17/20  Iteration 2843/3520 Training loss: 1.1738 0.2938 sec/batch\n",
      "Epoch 17/20  Iteration 2844/3520 Training loss: 1.1735 0.2951 sec/batch\n",
      "Epoch 17/20  Iteration 2845/3520 Training loss: 1.1725 0.2945 sec/batch\n",
      "Epoch 17/20  Iteration 2846/3520 Training loss: 1.1730 0.2954 sec/batch\n",
      "Epoch 17/20  Iteration 2847/3520 Training loss: 1.1728 0.2956 sec/batch\n",
      "Epoch 17/20  Iteration 2848/3520 Training loss: 1.1723 0.2942 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3520 Training loss: 1.1728 0.2957 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3520 Training loss: 1.1725 0.2950 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3520 Training loss: 1.1725 0.2945 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3520 Training loss: 1.1731 0.2943 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3520 Training loss: 1.1724 0.2930 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3520 Training loss: 1.1730 0.2943 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3520 Training loss: 1.1734 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3520 Training loss: 1.1737 0.2946 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3520 Training loss: 1.1733 0.2946 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3520 Training loss: 1.1728 0.2960 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3520 Training loss: 1.1727 0.2942 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3520 Training loss: 1.1720 0.2932 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3520 Training loss: 1.1718 0.2934 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3520 Training loss: 1.1717 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3520 Training loss: 1.1718 0.2945 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3520 Training loss: 1.1718 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3520 Training loss: 1.1717 0.2934 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3520 Training loss: 1.1718 0.2947 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3520 Training loss: 1.1718 0.2945 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3520 Training loss: 1.1719 0.2936 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3520 Training loss: 1.1720 0.2931 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3520 Training loss: 1.1718 0.3048 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3520 Training loss: 1.1717 0.2933 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3520 Training loss: 1.1718 0.2947 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3520 Training loss: 1.1715 0.2929 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3520 Training loss: 1.1714 0.2940 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3520 Training loss: 1.1708 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3520 Training loss: 1.1711 0.2943 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3520 Training loss: 1.1711 0.2938 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3520 Training loss: 1.1711 0.2933 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3520 Training loss: 1.1712 0.2936 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3520 Training loss: 1.1709 0.2943 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3520 Training loss: 1.1706 0.2936 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3520 Training loss: 1.1707 0.2940 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3520 Training loss: 1.1706 0.2947 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3520 Training loss: 1.1703 0.2940 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3520 Training loss: 1.1700 0.2944 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3520 Training loss: 1.1700 0.2927 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3520 Training loss: 1.1698 0.2934 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3520 Training loss: 1.1697 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3520 Training loss: 1.1694 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3520 Training loss: 1.1691 0.2932 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3520 Training loss: 1.1690 0.2945 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3520 Training loss: 1.1692 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3520 Training loss: 1.1695 0.2939 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3520 Training loss: 1.1699 0.2949 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3520 Training loss: 1.1700 0.2939 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3520 Training loss: 1.1698 0.2939 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3520 Training loss: 1.1696 0.2972 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3520 Training loss: 1.1699 0.2949 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3520 Training loss: 1.1698 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3520 Training loss: 1.1701 0.2939 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3520 Training loss: 1.1700 0.2953 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3520 Training loss: 1.1696 0.2952 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3520 Training loss: 1.1698 0.2928 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3520 Training loss: 1.1696 0.2951 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3520 Training loss: 1.1698 0.2949 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3520 Training loss: 1.1694 0.2946 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3520 Training loss: 1.1693 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3520 Training loss: 1.1694 0.2946 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3520 Training loss: 1.1692 0.2948 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3520 Training loss: 1.1690 0.2950 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3520 Training loss: 1.1689 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3520 Training loss: 1.1687 0.2940 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3520 Training loss: 1.1688 0.2947 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3520 Training loss: 1.1691 0.2947 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3520 Training loss: 1.1694 0.2940 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3520 Training loss: 1.1693 0.2936 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3520 Training loss: 1.1692 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3520 Training loss: 1.1693 0.2927 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3520 Training loss: 1.1691 0.3030 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3520 Training loss: 1.1687 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3520 Training loss: 1.1684 0.2940 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3520 Training loss: 1.1686 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3520 Training loss: 1.1686 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3520 Training loss: 1.1684 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3520 Training loss: 1.1685 0.2938 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3520 Training loss: 1.1686 0.2947 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3520 Training loss: 1.1688 0.2947 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3520 Training loss: 1.1690 0.3039 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3520 Training loss: 1.1691 0.2948 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3520 Training loss: 1.1690 0.2961 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3520 Training loss: 1.1689 0.2944 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3520 Training loss: 1.1689 0.2934 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3520 Training loss: 1.1688 0.2938 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3520 Training loss: 1.1690 0.2939 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3520 Training loss: 1.1690 0.2933 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3520 Training loss: 1.1690 0.2982 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3520 Training loss: 1.1690 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3520 Training loss: 1.1690 0.2940 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3520 Training loss: 1.1689 0.3058 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3520 Training loss: 1.1690 0.2929 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3520 Training loss: 1.1689 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3520 Training loss: 1.1688 0.2938 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3520 Training loss: 1.1687 0.2952 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3520 Training loss: 1.1689 0.2959 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3520 Training loss: 1.1688 0.2952 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3520 Training loss: 1.1690 0.2946 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3520 Training loss: 1.1690 0.2947 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3520 Training loss: 1.1692 0.2936 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3520 Training loss: 1.1691 0.2952 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3520 Training loss: 1.1691 0.2940 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3520 Training loss: 1.1692 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3520 Training loss: 1.1692 0.2977 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3520 Training loss: 1.1691 0.2942 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3520 Training loss: 1.1690 0.3052 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3520 Training loss: 1.1691 0.2947 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3520 Training loss: 1.1693 0.2931 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3520 Training loss: 1.1693 0.2930 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3520 Training loss: 1.1692 0.2938 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3520 Training loss: 1.1690 0.2934 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3520 Training loss: 1.1689 0.2943 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3520 Training loss: 1.1690 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3520 Training loss: 1.1691 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3520 Training loss: 1.1690 0.2930 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3520 Training loss: 1.1690 0.2941 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3520 Training loss: 1.1690 0.2950 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3520 Training loss: 1.1691 0.2938 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3520 Training loss: 1.1690 0.2996 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3520 Training loss: 1.1689 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3520 Training loss: 1.1688 0.2936 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3520 Training loss: 1.1687 0.2934 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3520 Training loss: 1.1685 0.2932 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3520 Training loss: 1.1684 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3520 Training loss: 1.1682 0.2930 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3520 Training loss: 1.1683 0.2944 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3520 Training loss: 1.1683 0.2944 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3520 Training loss: 1.1683 0.2949 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3520 Training loss: 1.1681 0.2936 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3520 Training loss: 1.1682 0.2933 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3520 Training loss: 1.1682 0.2939 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3520 Training loss: 1.1681 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3520 Training loss: 1.1678 0.2933 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3520 Training loss: 1.1676 0.2937 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3520 Training loss: 1.1674 0.2946 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3520 Training loss: 1.1674 0.2939 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3520 Training loss: 1.1675 0.2936 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3520 Training loss: 1.1675 0.2951 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3520 Training loss: 1.1675 0.2935 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3520 Training loss: 1.1676 0.2934 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3520 Training loss: 1.1675 0.2932 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3520 Training loss: 1.1676 0.2938 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3520 Training loss: 1.1677 0.2940 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3520 Training loss: 1.1679 0.2942 sec/batch\n",
      "Epoch 18/20  Iteration 2993/3520 Training loss: 1.2644 0.2942 sec/batch\n",
      "Epoch 18/20  Iteration 2994/3520 Training loss: 1.2028 0.2943 sec/batch\n",
      "Epoch 18/20  Iteration 2995/3520 Training loss: 1.1880 0.2934 sec/batch\n",
      "Epoch 18/20  Iteration 2996/3520 Training loss: 1.1820 0.2934 sec/batch\n",
      "Epoch 18/20  Iteration 2997/3520 Training loss: 1.1814 0.2946 sec/batch\n",
      "Epoch 18/20  Iteration 2998/3520 Training loss: 1.1793 0.2946 sec/batch\n",
      "Epoch 18/20  Iteration 2999/3520 Training loss: 1.1738 0.2944 sec/batch\n",
      "Epoch 18/20  Iteration 3000/3520 Training loss: 1.1725 0.2951 sec/batch\n",
      "Validation loss: 1.10347 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 3001/3520 Training loss: 1.1908 0.2963 sec/batch\n",
      "Epoch 18/20  Iteration 3002/3520 Training loss: 1.1890 0.2992 sec/batch\n",
      "Epoch 18/20  Iteration 3003/3520 Training loss: 1.1875 0.2931 sec/batch\n",
      "Epoch 18/20  Iteration 3004/3520 Training loss: 1.1851 0.2937 sec/batch\n",
      "Epoch 18/20  Iteration 3005/3520 Training loss: 1.1820 0.3046 sec/batch\n",
      "Epoch 18/20  Iteration 3006/3520 Training loss: 1.1830 0.2930 sec/batch\n",
      "Epoch 18/20  Iteration 3007/3520 Training loss: 1.1813 0.2932 sec/batch\n",
      "Epoch 18/20  Iteration 3008/3520 Training loss: 1.1789 0.2944 sec/batch\n",
      "Epoch 18/20  Iteration 3009/3520 Training loss: 1.1778 0.2946 sec/batch\n",
      "Epoch 18/20  Iteration 3010/3520 Training loss: 1.1777 0.2934 sec/batch\n",
      "Epoch 18/20  Iteration 3011/3520 Training loss: 1.1761 0.2937 sec/batch\n",
      "Epoch 18/20  Iteration 3012/3520 Training loss: 1.1761 0.2939 sec/batch\n",
      "Epoch 18/20  Iteration 3013/3520 Training loss: 1.1762 0.2939 sec/batch\n",
      "Epoch 18/20  Iteration 3014/3520 Training loss: 1.1755 0.2940 sec/batch\n",
      "Epoch 18/20  Iteration 3015/3520 Training loss: 1.1756 0.2939 sec/batch\n",
      "Epoch 18/20  Iteration 3016/3520 Training loss: 1.1760 0.2940 sec/batch\n",
      "Epoch 18/20  Iteration 3017/3520 Training loss: 1.1760 0.2936 sec/batch\n",
      "Epoch 18/20  Iteration 3018/3520 Training loss: 1.1752 0.2931 sec/batch\n",
      "Epoch 18/20  Iteration 3019/3520 Training loss: 1.1741 0.2937 sec/batch\n",
      "Epoch 18/20  Iteration 3020/3520 Training loss: 1.1739 0.2948 sec/batch\n",
      "Epoch 18/20  Iteration 3021/3520 Training loss: 1.1726 0.2941 sec/batch\n",
      "Epoch 18/20  Iteration 3022/3520 Training loss: 1.1725 0.3061 sec/batch\n",
      "Epoch 18/20  Iteration 3023/3520 Training loss: 1.1716 0.2940 sec/batch\n",
      "Epoch 18/20  Iteration 3024/3520 Training loss: 1.1711 0.2944 sec/batch\n",
      "Epoch 18/20  Iteration 3025/3520 Training loss: 1.1715 0.2939 sec/batch\n",
      "Epoch 18/20  Iteration 3026/3520 Training loss: 1.1710 0.2937 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3520 Training loss: 1.1708 0.2960 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3520 Training loss: 1.1708 0.2952 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3520 Training loss: 1.1695 0.2939 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3520 Training loss: 1.1700 0.2935 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3520 Training loss: 1.1703 0.2948 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3520 Training loss: 1.1702 0.2947 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3520 Training loss: 1.1700 0.2941 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3520 Training loss: 1.1693 0.2948 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3520 Training loss: 1.1689 0.2942 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3520 Training loss: 1.1681 0.2932 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3520 Training loss: 1.1678 0.2935 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3520 Training loss: 1.1677 0.2932 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3520 Training loss: 1.1676 0.2934 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3520 Training loss: 1.1676 0.2944 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3520 Training loss: 1.1675 0.2945 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3520 Training loss: 1.1673 0.2946 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3520 Training loss: 1.1673 0.2938 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3520 Training loss: 1.1674 0.2952 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3520 Training loss: 1.1676 0.2934 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3520 Training loss: 1.1675 0.2941 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3520 Training loss: 1.1674 0.2937 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3520 Training loss: 1.1674 0.2946 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3520 Training loss: 1.1671 0.2947 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3520 Training loss: 1.1670 0.2948 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3520 Training loss: 1.1665 0.2931 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3520 Training loss: 1.1666 0.2935 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3520 Training loss: 1.1665 0.2933 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3520 Training loss: 1.1665 0.3039 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3520 Training loss: 1.1664 0.2937 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3520 Training loss: 1.1662 0.2948 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3520 Training loss: 1.1658 0.2939 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3520 Training loss: 1.1658 0.2939 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3520 Training loss: 1.1657 0.3057 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3520 Training loss: 1.1651 0.2936 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3520 Training loss: 1.1647 0.2953 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3520 Training loss: 1.1646 0.2948 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3520 Training loss: 1.1642 0.2942 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3520 Training loss: 1.1639 0.2944 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3520 Training loss: 1.1636 0.2946 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3520 Training loss: 1.1631 0.2939 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3520 Training loss: 1.1627 0.2933 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3520 Training loss: 1.1628 0.2941 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3520 Training loss: 1.1629 0.2935 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3520 Training loss: 1.1632 0.2951 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3520 Training loss: 1.1633 0.2930 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3520 Training loss: 1.1631 0.2936 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3520 Training loss: 1.1628 0.2946 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3520 Training loss: 1.1631 0.2935 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3520 Training loss: 1.1629 0.2934 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3520 Training loss: 1.1631 0.2933 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3520 Training loss: 1.1630 0.2932 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3520 Training loss: 1.1626 0.2940 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3520 Training loss: 1.1629 0.2946 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3520 Training loss: 1.1626 0.3074 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3520 Training loss: 1.1627 0.2951 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3520 Training loss: 1.1624 0.2952 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3520 Training loss: 1.1622 0.2945 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3520 Training loss: 1.1623 0.2941 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3520 Training loss: 1.1621 0.2939 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3520 Training loss: 1.1620 0.2942 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3520 Training loss: 1.1619 0.2934 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3520 Training loss: 1.1616 0.3045 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3520 Training loss: 1.1618 0.2933 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3520 Training loss: 1.1618 0.2933 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3520 Training loss: 1.1618 0.2931 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3520 Training loss: 1.1617 0.2935 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3520 Training loss: 1.1615 0.2950 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3520 Training loss: 1.1615 0.2940 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3520 Training loss: 1.1613 0.2936 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3520 Training loss: 1.1607 0.2944 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3520 Training loss: 1.1604 0.2959 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3520 Training loss: 1.1605 0.2938 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3520 Training loss: 1.1604 0.2948 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3520 Training loss: 1.1602 0.2929 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3520 Training loss: 1.1602 0.2940 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3520 Training loss: 1.1602 0.2950 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3520 Training loss: 1.1603 0.2943 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3520 Training loss: 1.1603 0.2950 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3520 Training loss: 1.1604 0.2947 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3520 Training loss: 1.1602 0.2947 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3520 Training loss: 1.1600 0.2947 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3520 Training loss: 1.1599 0.2942 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3520 Training loss: 1.1598 0.2948 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3520 Training loss: 1.1599 0.2938 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3520 Training loss: 1.1599 0.2940 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3520 Training loss: 1.1598 0.2946 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3520 Training loss: 1.1599 0.2933 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3520 Training loss: 1.1598 0.2949 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3520 Training loss: 1.1598 0.2941 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3520 Training loss: 1.1597 0.2949 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3520 Training loss: 1.1596 0.2937 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3520 Training loss: 1.1594 0.2942 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3520 Training loss: 1.1593 0.2945 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3520 Training loss: 1.1593 0.2932 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3520 Training loss: 1.1591 0.2929 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3520 Training loss: 1.1593 0.2944 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3520 Training loss: 1.1591 0.2933 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3520 Training loss: 1.1594 0.2956 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3520 Training loss: 1.1594 0.2941 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3520 Training loss: 1.1595 0.2949 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3520 Training loss: 1.1595 0.2934 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3520 Training loss: 1.1594 0.2951 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3520 Training loss: 1.1595 0.2956 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3520 Training loss: 1.1594 0.2949 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3520 Training loss: 1.1595 0.2946 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3520 Training loss: 1.1596 0.2949 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3520 Training loss: 1.1596 0.2973 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3520 Training loss: 1.1595 0.2958 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3520 Training loss: 1.1592 0.2950 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3520 Training loss: 1.1591 0.2952 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3520 Training loss: 1.1592 0.2938 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3520 Training loss: 1.1592 0.2935 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3520 Training loss: 1.1591 0.2945 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3520 Training loss: 1.1590 0.2947 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3520 Training loss: 1.1589 0.2951 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3520 Training loss: 1.1591 0.2937 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3520 Training loss: 1.1589 0.2944 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3520 Training loss: 1.1587 0.2937 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3520 Training loss: 1.1587 0.2970 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3520 Training loss: 1.1586 0.2938 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3520 Training loss: 1.1585 0.3022 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3520 Training loss: 1.1584 0.2942 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3520 Training loss: 1.1583 0.2949 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3520 Training loss: 1.1583 0.2933 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3520 Training loss: 1.1583 0.2948 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3520 Training loss: 1.1583 0.2957 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3520 Training loss: 1.1582 0.2940 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3520 Training loss: 1.1583 0.2953 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3520 Training loss: 1.1582 0.2965 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3520 Training loss: 1.1580 0.2975 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3520 Training loss: 1.1578 0.2981 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3520 Training loss: 1.1576 0.2951 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3520 Training loss: 1.1574 0.2931 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3520 Training loss: 1.1573 0.2938 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3520 Training loss: 1.1574 0.2950 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3520 Training loss: 1.1573 0.2964 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3520 Training loss: 1.1574 0.2949 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3520 Training loss: 1.1575 0.2944 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3520 Training loss: 1.1574 0.2952 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3520 Training loss: 1.1574 0.2938 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3520 Training loss: 1.1574 0.2951 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3520 Training loss: 1.1577 0.2941 sec/batch\n",
      "Epoch 19/20  Iteration 3169/3520 Training loss: 1.2459 0.2945 sec/batch\n",
      "Epoch 19/20  Iteration 3170/3520 Training loss: 1.1856 0.2945 sec/batch\n",
      "Epoch 19/20  Iteration 3171/3520 Training loss: 1.1740 0.3031 sec/batch\n",
      "Epoch 19/20  Iteration 3172/3520 Training loss: 1.1696 0.2954 sec/batch\n",
      "Epoch 19/20  Iteration 3173/3520 Training loss: 1.1672 0.2956 sec/batch\n",
      "Epoch 19/20  Iteration 3174/3520 Training loss: 1.1645 0.2948 sec/batch\n",
      "Epoch 19/20  Iteration 3175/3520 Training loss: 1.1598 0.2941 sec/batch\n",
      "Epoch 19/20  Iteration 3176/3520 Training loss: 1.1594 0.2971 sec/batch\n",
      "Epoch 19/20  Iteration 3177/3520 Training loss: 1.1568 0.2946 sec/batch\n",
      "Epoch 19/20  Iteration 3178/3520 Training loss: 1.1559 0.2952 sec/batch\n",
      "Epoch 19/20  Iteration 3179/3520 Training loss: 1.1553 0.2930 sec/batch\n",
      "Epoch 19/20  Iteration 3180/3520 Training loss: 1.1551 0.2949 sec/batch\n",
      "Epoch 19/20  Iteration 3181/3520 Training loss: 1.1536 0.2961 sec/batch\n",
      "Epoch 19/20  Iteration 3182/3520 Training loss: 1.1547 0.2936 sec/batch\n",
      "Epoch 19/20  Iteration 3183/3520 Training loss: 1.1542 0.2951 sec/batch\n",
      "Epoch 19/20  Iteration 3184/3520 Training loss: 1.1526 0.2948 sec/batch\n",
      "Epoch 19/20  Iteration 3185/3520 Training loss: 1.1518 0.2948 sec/batch\n",
      "Epoch 19/20  Iteration 3186/3520 Training loss: 1.1524 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3187/3520 Training loss: 1.1519 0.2949 sec/batch\n",
      "Epoch 19/20  Iteration 3188/3520 Training loss: 1.1526 0.2941 sec/batch\n",
      "Epoch 19/20  Iteration 3189/3520 Training loss: 1.1531 0.2949 sec/batch\n",
      "Epoch 19/20  Iteration 3190/3520 Training loss: 1.1524 0.2936 sec/batch\n",
      "Epoch 19/20  Iteration 3191/3520 Training loss: 1.1530 0.2953 sec/batch\n",
      "Epoch 19/20  Iteration 3192/3520 Training loss: 1.1540 0.2951 sec/batch\n",
      "Epoch 19/20  Iteration 3193/3520 Training loss: 1.1543 0.2942 sec/batch\n",
      "Epoch 19/20  Iteration 3194/3520 Training loss: 1.1541 0.2944 sec/batch\n",
      "Epoch 19/20  Iteration 3195/3520 Training loss: 1.1532 0.2950 sec/batch\n",
      "Epoch 19/20  Iteration 3196/3520 Training loss: 1.1531 0.2942 sec/batch\n",
      "Epoch 19/20  Iteration 3197/3520 Training loss: 1.1523 0.2939 sec/batch\n",
      "Epoch 19/20  Iteration 3198/3520 Training loss: 1.1526 0.2954 sec/batch\n",
      "Epoch 19/20  Iteration 3199/3520 Training loss: 1.1523 0.2944 sec/batch\n",
      "Epoch 19/20  Iteration 3200/3520 Training loss: 1.1523 0.2951 sec/batch\n",
      "Validation loss: 1.0951 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 3201/3520 Training loss: 1.1591 0.2973 sec/batch\n",
      "Epoch 19/20  Iteration 3202/3520 Training loss: 1.1590 0.2970 sec/batch\n",
      "Epoch 19/20  Iteration 3203/3520 Training loss: 1.1586 0.2939 sec/batch\n",
      "Epoch 19/20  Iteration 3204/3520 Training loss: 1.1589 0.2939 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3520 Training loss: 1.1578 0.2952 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3520 Training loss: 1.1581 0.2942 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3520 Training loss: 1.1585 0.2944 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3520 Training loss: 1.1584 0.2936 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3520 Training loss: 1.1581 0.2935 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3520 Training loss: 1.1575 0.2940 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3520 Training loss: 1.1575 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3520 Training loss: 1.1568 0.2949 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3520 Training loss: 1.1563 0.2950 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3520 Training loss: 1.1563 0.2947 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3520 Training loss: 1.1564 0.2951 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3520 Training loss: 1.1564 0.2950 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3520 Training loss: 1.1562 0.2948 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3520 Training loss: 1.1561 0.2941 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3520 Training loss: 1.1558 0.2938 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3520 Training loss: 1.1558 0.2944 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3520 Training loss: 1.1558 0.2948 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3520 Training loss: 1.1556 0.2942 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3520 Training loss: 1.1554 0.2938 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3520 Training loss: 1.1555 0.2944 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3520 Training loss: 1.1552 0.2948 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3520 Training loss: 1.1551 0.2947 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3520 Training loss: 1.1546 0.2925 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3520 Training loss: 1.1547 0.2944 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3520 Training loss: 1.1547 0.2954 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3520 Training loss: 1.1546 0.2947 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3520 Training loss: 1.1546 0.2939 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3520 Training loss: 1.1544 0.2936 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3520 Training loss: 1.1540 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3520 Training loss: 1.1539 0.2954 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3520 Training loss: 1.1537 0.2964 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3520 Training loss: 1.1534 0.2948 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3520 Training loss: 1.1531 0.2942 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3520 Training loss: 1.1530 0.2934 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3520 Training loss: 1.1528 0.2951 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3520 Training loss: 1.1527 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3520 Training loss: 1.1524 0.2934 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3520 Training loss: 1.1520 0.2939 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3520 Training loss: 1.1518 0.2949 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3520 Training loss: 1.1519 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3520 Training loss: 1.1520 0.3056 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3520 Training loss: 1.1523 0.2936 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3520 Training loss: 1.1524 0.2938 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3520 Training loss: 1.1521 0.3000 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3520 Training loss: 1.1518 0.2947 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3520 Training loss: 1.1521 0.2944 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3520 Training loss: 1.1520 0.2948 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3520 Training loss: 1.1522 0.2932 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3520 Training loss: 1.1521 0.2938 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3520 Training loss: 1.1518 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3520 Training loss: 1.1521 0.2938 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3520 Training loss: 1.1518 0.2939 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3520 Training loss: 1.1520 0.2949 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3520 Training loss: 1.1518 0.2946 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3520 Training loss: 1.1517 0.2947 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3520 Training loss: 1.1519 0.2935 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3520 Training loss: 1.1516 0.2969 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3520 Training loss: 1.1514 0.2934 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3520 Training loss: 1.1514 0.2945 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3520 Training loss: 1.1511 0.2936 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3520 Training loss: 1.1512 0.2951 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3520 Training loss: 1.1513 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3520 Training loss: 1.1514 0.2933 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3520 Training loss: 1.1513 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3520 Training loss: 1.1512 0.2939 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3520 Training loss: 1.1511 0.2929 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3520 Training loss: 1.1509 0.2932 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3520 Training loss: 1.1504 0.2952 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3520 Training loss: 1.1501 0.2942 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3520 Training loss: 1.1502 0.2960 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3520 Training loss: 1.1502 0.2939 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3520 Training loss: 1.1499 0.2946 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3520 Training loss: 1.1501 0.2934 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3520 Training loss: 1.1500 0.2943 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3520 Training loss: 1.1501 0.2938 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3520 Training loss: 1.1501 0.2949 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3520 Training loss: 1.1502 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3520 Training loss: 1.1501 0.2939 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3520 Training loss: 1.1500 0.2944 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3520 Training loss: 1.1498 0.2944 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3520 Training loss: 1.1496 0.2974 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3520 Training loss: 1.1497 0.2951 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3520 Training loss: 1.1497 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3520 Training loss: 1.1497 0.2936 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3520 Training loss: 1.1496 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3520 Training loss: 1.1496 0.2941 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3520 Training loss: 1.1496 0.2950 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3520 Training loss: 1.1495 0.2938 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3520 Training loss: 1.1495 0.2941 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3520 Training loss: 1.1493 0.2925 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3520 Training loss: 1.1492 0.2947 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3520 Training loss: 1.1492 0.2935 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3520 Training loss: 1.1491 0.2942 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3520 Training loss: 1.1492 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3520 Training loss: 1.1491 0.2938 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3520 Training loss: 1.1493 0.2932 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3520 Training loss: 1.1493 0.2943 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3520 Training loss: 1.1492 0.2943 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3520 Training loss: 1.1493 0.2936 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3520 Training loss: 1.1492 0.2956 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3520 Training loss: 1.1493 0.2958 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3520 Training loss: 1.1491 0.2941 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3520 Training loss: 1.1492 0.2947 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3520 Training loss: 1.1493 0.2934 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3520 Training loss: 1.1492 0.2942 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3520 Training loss: 1.1490 0.2946 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3520 Training loss: 1.1487 0.2930 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3520 Training loss: 1.1487 0.2942 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3520 Training loss: 1.1488 0.2933 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3520 Training loss: 1.1489 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3520 Training loss: 1.1489 0.2934 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3520 Training loss: 1.1488 0.2934 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3520 Training loss: 1.1488 0.2950 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3520 Training loss: 1.1489 0.2951 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3520 Training loss: 1.1488 0.2948 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3520 Training loss: 1.1487 0.2941 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3520 Training loss: 1.1487 0.2938 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3520 Training loss: 1.1485 0.2947 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3520 Training loss: 1.1485 0.2944 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3520 Training loss: 1.1484 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3520 Training loss: 1.1482 0.2950 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3520 Training loss: 1.1482 0.2932 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3520 Training loss: 1.1482 0.2955 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3520 Training loss: 1.1483 0.2948 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3520 Training loss: 1.1481 0.2958 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3520 Training loss: 1.1481 0.2966 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3520 Training loss: 1.1481 0.2950 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3520 Training loss: 1.1479 0.2942 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3520 Training loss: 1.1476 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3520 Training loss: 1.1475 0.2948 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3520 Training loss: 1.1473 0.2933 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3520 Training loss: 1.1473 0.2945 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3520 Training loss: 1.1474 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3520 Training loss: 1.1474 0.2939 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3520 Training loss: 1.1474 0.2944 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3520 Training loss: 1.1475 0.2950 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3520 Training loss: 1.1474 0.2937 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3520 Training loss: 1.1474 0.2933 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3520 Training loss: 1.1475 0.2956 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3520 Training loss: 1.1477 0.2939 sec/batch\n",
      "Epoch 20/20  Iteration 3345/3520 Training loss: 1.2407 0.2930 sec/batch\n",
      "Epoch 20/20  Iteration 3346/3520 Training loss: 1.1841 0.2943 sec/batch\n",
      "Epoch 20/20  Iteration 3347/3520 Training loss: 1.1725 0.3044 sec/batch\n",
      "Epoch 20/20  Iteration 3348/3520 Training loss: 1.1645 0.2940 sec/batch\n",
      "Epoch 20/20  Iteration 3349/3520 Training loss: 1.1618 0.2942 sec/batch\n",
      "Epoch 20/20  Iteration 3350/3520 Training loss: 1.1601 0.2932 sec/batch\n",
      "Epoch 20/20  Iteration 3351/3520 Training loss: 1.1555 0.2932 sec/batch\n",
      "Epoch 20/20  Iteration 3352/3520 Training loss: 1.1533 0.2948 sec/batch\n",
      "Epoch 20/20  Iteration 3353/3520 Training loss: 1.1507 0.2944 sec/batch\n",
      "Epoch 20/20  Iteration 3354/3520 Training loss: 1.1487 0.2938 sec/batch\n",
      "Epoch 20/20  Iteration 3355/3520 Training loss: 1.1495 0.2953 sec/batch\n",
      "Epoch 20/20  Iteration 3356/3520 Training loss: 1.1494 0.2946 sec/batch\n",
      "Epoch 20/20  Iteration 3357/3520 Training loss: 1.1478 0.2938 sec/batch\n",
      "Epoch 20/20  Iteration 3358/3520 Training loss: 1.1486 0.2951 sec/batch\n",
      "Epoch 20/20  Iteration 3359/3520 Training loss: 1.1477 0.2945 sec/batch\n",
      "Epoch 20/20  Iteration 3360/3520 Training loss: 1.1463 0.2940 sec/batch\n",
      "Epoch 20/20  Iteration 3361/3520 Training loss: 1.1455 0.2968 sec/batch\n",
      "Epoch 20/20  Iteration 3362/3520 Training loss: 1.1462 0.2935 sec/batch\n",
      "Epoch 20/20  Iteration 3363/3520 Training loss: 1.1449 0.2940 sec/batch\n",
      "Epoch 20/20  Iteration 3364/3520 Training loss: 1.1455 0.2952 sec/batch\n",
      "Epoch 20/20  Iteration 3365/3520 Training loss: 1.1457 0.2936 sec/batch\n",
      "Epoch 20/20  Iteration 3366/3520 Training loss: 1.1452 0.2949 sec/batch\n",
      "Epoch 20/20  Iteration 3367/3520 Training loss: 1.1455 0.2935 sec/batch\n",
      "Epoch 20/20  Iteration 3368/3520 Training loss: 1.1464 0.2955 sec/batch\n",
      "Epoch 20/20  Iteration 3369/3520 Training loss: 1.1471 0.2949 sec/batch\n",
      "Epoch 20/20  Iteration 3370/3520 Training loss: 1.1466 0.2945 sec/batch\n",
      "Epoch 20/20  Iteration 3371/3520 Training loss: 1.1456 0.2937 sec/batch\n",
      "Epoch 20/20  Iteration 3372/3520 Training loss: 1.1453 0.2944 sec/batch\n",
      "Epoch 20/20  Iteration 3373/3520 Training loss: 1.1442 0.2968 sec/batch\n",
      "Epoch 20/20  Iteration 3374/3520 Training loss: 1.1444 0.2945 sec/batch\n",
      "Epoch 20/20  Iteration 3375/3520 Training loss: 1.1437 0.2934 sec/batch\n",
      "Epoch 20/20  Iteration 3376/3520 Training loss: 1.1437 0.2936 sec/batch\n",
      "Epoch 20/20  Iteration 3377/3520 Training loss: 1.1444 0.2933 sec/batch\n",
      "Epoch 20/20  Iteration 3378/3520 Training loss: 1.1441 0.2930 sec/batch\n",
      "Epoch 20/20  Iteration 3379/3520 Training loss: 1.1439 0.2949 sec/batch\n",
      "Epoch 20/20  Iteration 3380/3520 Training loss: 1.1443 0.2937 sec/batch\n",
      "Epoch 20/20  Iteration 3381/3520 Training loss: 1.1437 0.3041 sec/batch\n",
      "Epoch 20/20  Iteration 3382/3520 Training loss: 1.1442 0.2952 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3520 Training loss: 1.1446 0.2940 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3520 Training loss: 1.1446 0.2951 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3520 Training loss: 1.1444 0.2948 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3520 Training loss: 1.1438 0.2954 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3520 Training loss: 1.1438 0.2954 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3520 Training loss: 1.1433 0.2948 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3520 Training loss: 1.1431 0.2937 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3520 Training loss: 1.1428 0.2940 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3520 Training loss: 1.1428 0.2950 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3520 Training loss: 1.1431 0.2963 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3520 Training loss: 1.1430 0.2939 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3520 Training loss: 1.1433 0.2937 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3520 Training loss: 1.1432 0.2940 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3520 Training loss: 1.1433 0.2930 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3520 Training loss: 1.1434 0.2942 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3520 Training loss: 1.1435 0.2936 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3520 Training loss: 1.1433 0.2935 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3520 Training loss: 1.1434 0.2939 sec/batch\n",
      "Validation loss: 1.08932 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 3401/3520 Training loss: 1.1468 0.3004 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3520 Training loss: 1.1470 0.2989 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3520 Training loss: 1.1466 0.2952 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3520 Training loss: 1.1469 0.2950 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3520 Training loss: 1.1467 0.2967 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3520 Training loss: 1.1467 0.2960 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3520 Training loss: 1.1468 0.2944 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3520 Training loss: 1.1467 0.2940 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3520 Training loss: 1.1464 0.2951 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3520 Training loss: 1.1464 0.2935 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3520 Training loss: 1.1462 0.2953 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3520 Training loss: 1.1456 0.2939 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3520 Training loss: 1.1454 0.2950 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3520 Training loss: 1.1453 0.2965 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3520 Training loss: 1.1450 0.2945 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3520 Training loss: 1.1448 0.2942 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3520 Training loss: 1.1445 0.2943 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3520 Training loss: 1.1440 0.2952 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3520 Training loss: 1.1438 0.2938 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3520 Training loss: 1.1440 0.2941 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3520 Training loss: 1.1441 0.2948 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3520 Training loss: 1.1445 0.3044 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3520 Training loss: 1.1445 0.2947 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3520 Training loss: 1.1443 0.2931 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3520 Training loss: 1.1440 0.2947 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3520 Training loss: 1.1443 0.2950 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3520 Training loss: 1.1442 0.2947 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3520 Training loss: 1.1443 0.2956 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3520 Training loss: 1.1441 0.3041 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3520 Training loss: 1.1438 0.2945 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3520 Training loss: 1.1439 0.2950 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3520 Training loss: 1.1438 0.2936 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3520 Training loss: 1.1438 0.2950 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3520 Training loss: 1.1436 0.3039 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3520 Training loss: 1.1435 0.2945 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3520 Training loss: 1.1436 0.2945 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3520 Training loss: 1.1435 0.2952 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3520 Training loss: 1.1434 0.2934 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3520 Training loss: 1.1433 0.2937 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3520 Training loss: 1.1429 0.2959 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3520 Training loss: 1.1430 0.2934 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3520 Training loss: 1.1430 0.2944 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3520 Training loss: 1.1430 0.2936 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3520 Training loss: 1.1428 0.2941 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3520 Training loss: 1.1426 0.2943 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3520 Training loss: 1.1427 0.2941 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3520 Training loss: 1.1423 0.2946 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3520 Training loss: 1.1418 0.3037 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3520 Training loss: 1.1414 0.3034 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3520 Training loss: 1.1414 0.2939 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3520 Training loss: 1.1414 0.2940 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3520 Training loss: 1.1411 0.2936 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3520 Training loss: 1.1412 0.2947 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3520 Training loss: 1.1412 0.2954 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3520 Training loss: 1.1412 0.2950 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3520 Training loss: 1.1413 0.2942 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3520 Training loss: 1.1414 0.2939 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3520 Training loss: 1.1413 0.2936 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3520 Training loss: 1.1412 0.2972 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3520 Training loss: 1.1411 0.2953 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3520 Training loss: 1.1410 0.2942 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3520 Training loss: 1.1410 0.2936 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3520 Training loss: 1.1410 0.2942 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3520 Training loss: 1.1410 0.2930 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3520 Training loss: 1.1409 0.2935 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3520 Training loss: 1.1409 0.2953 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3520 Training loss: 1.1408 0.2943 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3520 Training loss: 1.1408 0.2942 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3520 Training loss: 1.1408 0.2955 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3520 Training loss: 1.1405 0.2944 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3520 Training loss: 1.1405 0.2938 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3520 Training loss: 1.1406 0.2940 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3520 Training loss: 1.1405 0.2955 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3520 Training loss: 1.1407 0.2941 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3520 Training loss: 1.1406 0.2938 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3520 Training loss: 1.1409 0.2942 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3520 Training loss: 1.1408 0.2946 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3520 Training loss: 1.1409 0.2948 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3520 Training loss: 1.1409 0.2941 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3520 Training loss: 1.1409 0.2935 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3520 Training loss: 1.1409 0.2933 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3520 Training loss: 1.1408 0.2944 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3520 Training loss: 1.1409 0.2950 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3520 Training loss: 1.1410 0.2956 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3520 Training loss: 1.1410 0.2978 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3520 Training loss: 1.1408 0.2956 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3520 Training loss: 1.1406 0.2940 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3520 Training loss: 1.1406 0.2947 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3520 Training loss: 1.1406 0.2955 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3520 Training loss: 1.1406 0.2950 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3520 Training loss: 1.1406 0.2946 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3520 Training loss: 1.1405 0.2944 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3520 Training loss: 1.1405 0.2942 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3520 Training loss: 1.1407 0.2954 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3520 Training loss: 1.1405 0.2952 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3520 Training loss: 1.1403 0.2944 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3520 Training loss: 1.1403 0.2951 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3520 Training loss: 1.1401 0.2941 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3520 Training loss: 1.1400 0.2951 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3520 Training loss: 1.1400 0.2942 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3520 Training loss: 1.1398 0.2951 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3520 Training loss: 1.1398 0.2957 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3520 Training loss: 1.1398 0.2947 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3520 Training loss: 1.1398 0.2945 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3520 Training loss: 1.1397 0.2954 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3520 Training loss: 1.1398 0.2947 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3520 Training loss: 1.1398 0.2948 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3520 Training loss: 1.1397 0.2947 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3520 Training loss: 1.1394 0.2937 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3520 Training loss: 1.1393 0.2953 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3520 Training loss: 1.1392 0.2943 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3520 Training loss: 1.1392 0.2948 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3520 Training loss: 1.1392 0.2940 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3520 Training loss: 1.1392 0.2952 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3520 Training loss: 1.1392 0.2953 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3520 Training loss: 1.1393 0.2947 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3520 Training loss: 1.1392 0.2951 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3520 Training loss: 1.1393 0.2949 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3520 Training loss: 1.1393 0.2945 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3520 Training loss: 1.1394 0.2941 sec/batch\n",
      "Validation loss: 1.08981 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 100\n",
    "num_steps = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "for lstm_size in [256,512]:\n",
    "    for num_layers in [1, 2]:\n",
    "        for learning_rate in [0.002]:\n",
    "            \n",
    "            log_string_train = 'logs/anna/train/lr={},rl={},ru={}'.format(learning_rate, num_layers, lstm_size)\n",
    "            train_writer = tf.summary.FileWriter(log_string_train)\n",
    "            \n",
    "            log_string_test = 'logs/anna/test/lr={},rl={},ru={}'.format(learning_rate, num_layers, lstm_size)\n",
    "            test_writer = tf.summary.FileWriter(log_string_test)\n",
    "            \n",
    "            model = build_rnn(len(vocab), \n",
    "                    batch_size=batch_size,\n",
    "                    num_steps=num_steps,\n",
    "                    learning_rate=learning_rate,\n",
    "                    lstm_size=lstm_size,\n",
    "                    num_layers=num_layers)\n",
    "            \n",
    "            train(model, epochs, train_writer, test_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i3520_l512_1.090.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i200_l512_2.295.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i400_l512_1.791.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i600_l512_1.547.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i800_l512_1.416.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1000_l512_1.336.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1200_l512_1.284.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1400_l512_1.246.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1600_l512_1.218.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1800_l512_1.189.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2000_l512_1.155.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2200_l512_1.143.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2400_l512_1.129.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2600_l512_1.117.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i2800_l512_1.109.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3000_l512_1.103.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3200_l512_1.095.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3400_l512_1.089.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i3520_l512_1.090.ckpt\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            #x[0,0] = ord(c)# vocab_to_int[c]\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab), 1)\n",
    "        #samples.append(chr(c))#(int_to_vocab[c])\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            #samples.append(chr(c))# (int_to_vocab[c])\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivanovitch., and\n",
      "this with their sounds with him, but that he had said, and his wife\n",
      "was already book, but she felt so the province and the telling him of\n",
      "anything but a pearant and to compose. He would have turned a long white\n",
      "below with his comprehension. He was not the stear on the tall, they\n",
      "all the steps of his cape of his hand to the position to this signs of\n",
      "this fields about this. And all this had been, and she came into this\n",
      "throat of the strange telring the poor of the sort, and his cross on\n",
      "a service to set the feeling, and he could not the society, and that\n",
      "he was all at once, and she will be a spirit at her. The mistress of\n",
      "his own whole both open the childrons, all the place which was near\n",
      "the step.\n",
      "\n",
      "The chief walked a low words on the part of his stray, a long while with\n",
      "a finger of the sumstle, this feeling of the massionary and work\n",
      "and to blind the carriage at the prince, while a solition absorbed the\n",
      "state of her sought to a little tatter. She was there at once of\n",
      "which he had said a good heat. To another thought he did not ask\n",
      "the soul to say that when the point of society, were seeing the\n",
      "princess.\n",
      "\n",
      "\"Oh, we have been a splanding on, the silence, and she seems and some\n",
      "one of the millions of the matter?\"\n",
      "\n",
      "\"Yes to meet you to took the corner of their hostility to the same the\n",
      "brother's, but the clur help and a man, to be instinctively to my percept\n",
      "of the mother's answer; and I should have a station of my table, while you\n",
      "say what I was anyone when you want to see you. They don't see\n",
      "it, and this minute have been saying through it.\"\n",
      "\n",
      "\"And there's no one of you to my son,\" she said. \"I shouldn't be at the\n",
      "whole old people,\" she went on, trying to see her, and thought his\n",
      "wife. \"The meadow, I don't suppose.... What are you saying, but this is\n",
      "straight the position. And that that I don't know how that it was\n",
      "to say with her a last meaning on a long while...\"\n",
      "\n",
      "\"No, I'm glad!\"\n",
      "\n",
      "\"No, there's one of this woman, and she is too,\" she said, still with his\n",
      "spring\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3520_l512_1.090.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Ivan\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
