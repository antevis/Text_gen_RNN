{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Text-gen RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load text into a one huge string (millions of chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every unhappy family is unhappy in its own\n",
      "way.\n",
      "\n",
      "Everythin\n",
      "1966145\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create vocabulary : a set of all chars of which 'text' consists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1', 'p', 'k', 'v', 'j', 'I', 'u', 'g', '9', 'M', 'i', 'q', '3', ')', '`', '(', 'b', '_', 'X', '!', 'V', 'y', '\\n', 'G', '8', '0', 'Y', 'S', \"'\", 'E', 'T', 'f', 'w', '?', 'm', '.', 'B', '2', 'h', 'A', '7', 'Q', 'l', '6', ';', ':', 'e', '\"', 'Z', 'z', 'x', 'a', 'W', 'R', 'd', 'H', 'F', ',', 'U', 'J', 'O', 'o', '5', 'C', 't', 'K', 'r', 'c', 'n', '-', 'P', 'N', 'D', 'L', '4', 's', ' '}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create vocab_to_int and int_to_vocab. These are dictionaries. You won't need it. Hopefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, 'v': 3, 'D': 72, 'R': 53, 'j': 4, 'I': 5, 'u': 6, 'g': 7, 'p': 1, 'M': 9, 'i': 10, 'q': 11, '3': 12, ')': 13, '`': 14, '(': 15, 'E': 29, '-': 69, 'b': 16, 'X': 18, '!': 19, 'V': 20, 'y': 21, '\\n': 22, '0': 25, '4': 74, 'Y': 26, 'w': 32, 'U': 58, 'S': 27, \"'\": 28, 'G': 23, 'r': 66, 'T': 30, '8': 24, 'F': 56, 'f': 31, '?': 33, '2': 37, 'm': 34, '.': 35, 'B': 36, 'h': 38, '7': 40, 'Q': 41, 'l': 42, '6': 43, ';': 44, 'c': 67, ':': 45, 'e': 46, '9': 8, 'z': 49, 'x': 50, 'a': 51, 'W': 52, 'd': 54, 'H': 55, '\"': 47, ',': 57, 'O': 60, 'o': 61, 'A': 39, 'J': 59, '5': 62, 'L': 73, 'C': 63, 't': 64, 's': 75, 'K': 65, 'k': 2, 'P': 70, 'n': 68, 'N': 71, '_': 17, 'Z': 48, ' ': 76}\n",
      "\n",
      "{0: '1', 1: 'p', 2: 'k', 3: 'v', 4: 'j', 5: 'I', 6: 'u', 7: 'g', 8: '9', 9: 'M', 10: 'i', 11: 'q', 12: '3', 13: ')', 14: '`', 15: '(', 16: 'b', 17: '_', 18: 'X', 19: '!', 20: 'V', 21: 'y', 22: '\\n', 23: 'G', 24: '8', 25: '0', 26: 'Y', 27: 'S', 28: \"'\", 29: 'E', 30: 'T', 31: 'f', 32: 'w', 33: '?', 34: 'm', 35: '.', 36: 'B', 37: '2', 38: 'h', 39: 'A', 40: '7', 41: 'Q', 42: 'l', 43: '6', 44: ';', 45: ':', 46: 'e', 47: '\"', 48: 'Z', 49: 'z', 50: 'x', 51: 'a', 52: 'W', 53: 'R', 54: 'd', 55: 'H', 56: 'F', 57: ',', 58: 'U', 59: 'J', 60: 'O', 61: 'o', 62: '5', 63: 'C', 64: 't', 65: 'K', 66: 'r', 67: 'c', 68: 'n', 69: '-', 70: 'P', 71: 'N', 72: 'D', 73: 'L', 74: '4', 75: 's', 76: ' '}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_to_int)\n",
    "print()\n",
    "print(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create an iteger representation of 'text' (millions of chars as ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63 38 51  1 64 46 66 76  0 22 22 22 55 51  1  1 21 76 31 51 34 10 42 10 46\n",
      " 75 76 51 66 46 76 51 42 42 76 51 42 10  2 46 44 76 46  3 46 66 21 76  6 68\n",
      " 38 51  1  1 21 76 31 51 34 10 42 21 76 10 75 76  6 68 38 51  1  1 21 76 10\n",
      " 68 76 10 64 75 76 61 32 68 22 32 51 21 35 22 22 29  3 46 66 21 64 38 10 68]\n"
     ]
    }
   ],
   "source": [
    "#chars = np.array([ord(c) for c in text], dtype=np.int32)\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "print(chars[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 176800)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)\n",
    "\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Creating training and validation sets using function defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "        outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.002\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, log_string):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_writer = tf.summary.FileWriter('./logs/anna/train/{}'.format(log_string),\n",
    "                                             sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/anna/test/{}'.format(log_string))\n",
    "\n",
    "        # Use the line below to load a checkpoint and resume training\n",
    "        #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "\n",
    "        n_batches = int(train_x.shape[1]/num_steps)\n",
    "        iterations = n_batches * epochs\n",
    "        for e in range(epochs):\n",
    "\n",
    "            # Train network\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            loss = 0\n",
    "            for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "                iteration = e*n_batches + b\n",
    "                start = time.time()\n",
    "                feed = {model.inputs: x,\n",
    "                        model.targets: y,\n",
    "                        model.keep_prob: keep_prob,\n",
    "                        model.initial_state: new_state}\n",
    "                summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                              model.final_state, model.optimizer], \n",
    "                                                              feed_dict=feed)\n",
    "                \n",
    "                loss += batch_loss\n",
    "                end = time.time()\n",
    "                print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                      'Iteration {}/{}'.format(iteration, iterations),\n",
    "                      'Training loss: {:.4f}'.format(loss/b),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "                train_writer.add_summary(summary, iteration)\n",
    "                \n",
    "                if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                    # Check performance, notice dropout has been set to 1\n",
    "                    val_loss = []\n",
    "                    new_state = sess.run(model.initial_state)\n",
    "                    for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                        feed = {model.inputs: x,\n",
    "                                model.targets: y,\n",
    "                                model.keep_prob: 1.,\n",
    "                                model.initial_state: new_state}\n",
    "                        summary, batch_loss, new_state = sess.run([model.merged,\n",
    "                                                                   model.cost, \n",
    "                                                                   model.final_state], \n",
    "                                                                  feed_dict=feed)\n",
    "                        val_loss.append(batch_loss)\n",
    "\n",
    "                    test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                    print('Validation loss:', np.mean(val_loss),\n",
    "                          'Saving checkpoint!')\n",
    "                    # Below command is commented out in Mat's version\n",
    "                    saver.save(sess, \n",
    "                               \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, \n",
    "                                                                             lstm_size, \n",
    "                                                                             np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training and validation data...\n",
      "Data generation complete.\n",
      "Building model...\n",
      "Model built\n",
      "Epoch 1/1  Iteration 1/176 Training loss: 4.3479 6.4462 sec/batch\n",
      "Epoch 1/1  Iteration 2/176 Training loss: 4.2779 5.6878 sec/batch\n",
      "Epoch 1/1  Iteration 3/176 Training loss: 4.8322 6.1623 sec/batch\n",
      "Epoch 1/1  Iteration 4/176 Training loss: 4.6433 6.3655 sec/batch\n",
      "Epoch 1/1  Iteration 5/176 Training loss: 4.4711 6.5034 sec/batch\n",
      "Epoch 1/1  Iteration 6/176 Training loss: 4.3322 5.7222 sec/batch\n",
      "Epoch 1/1  Iteration 7/176 Training loss: 4.2070 5.6751 sec/batch\n",
      "Epoch 1/1  Iteration 8/176 Training loss: 4.1048 5.9416 sec/batch\n",
      "Epoch 1/1  Iteration 9/176 Training loss: 4.0189 5.8041 sec/batch\n",
      "Epoch 1/1  Iteration 10/176 Training loss: 3.9459 5.7832 sec/batch\n",
      "Epoch 1/1  Iteration 11/176 Training loss: 3.8844 5.6367 sec/batch\n",
      "Epoch 1/1  Iteration 12/176 Training loss: 3.8335 5.6381 sec/batch\n",
      "Epoch 1/1  Iteration 13/176 Training loss: 3.7908 5.6543 sec/batch\n",
      "Epoch 1/1  Iteration 14/176 Training loss: 3.7502 5.5168 sec/batch\n",
      "Epoch 1/1  Iteration 15/176 Training loss: 3.7141 5.5147 sec/batch\n",
      "Epoch 1/1  Iteration 16/176 Training loss: 3.6822 5.6704 sec/batch\n",
      "Epoch 1/1  Iteration 17/176 Training loss: 3.6539 5.5358 sec/batch\n",
      "Epoch 1/1  Iteration 18/176 Training loss: 3.6297 5.7270 sec/batch\n",
      "Epoch 1/1  Iteration 19/176 Training loss: 3.6061 5.6214 sec/batch\n",
      "Epoch 1/1  Iteration 20/176 Training loss: 3.5855 5.5850 sec/batch\n",
      "Epoch 1/1  Iteration 21/176 Training loss: 3.5660 5.5401 sec/batch\n",
      "Epoch 1/1  Iteration 22/176 Training loss: 3.5487 6.2823 sec/batch\n",
      "Epoch 1/1  Iteration 23/176 Training loss: 3.5323 6.5415 sec/batch\n",
      "Epoch 1/1  Iteration 24/176 Training loss: 3.5173 5.6510 sec/batch\n",
      "Epoch 1/1  Iteration 25/176 Training loss: 3.5035 7.9684 sec/batch\n",
      "Epoch 1/1  Iteration 26/176 Training loss: 3.4903 7.4330 sec/batch\n",
      "Epoch 1/1  Iteration 27/176 Training loss: 3.4783 6.8826 sec/batch\n",
      "Epoch 1/1  Iteration 28/176 Training loss: 3.4662 7.3509 sec/batch\n",
      "Epoch 1/1  Iteration 29/176 Training loss: 3.4555 6.6534 sec/batch\n",
      "Epoch 1/1  Iteration 30/176 Training loss: 3.4450 5.6399 sec/batch\n",
      "Epoch 1/1  Iteration 31/176 Training loss: 3.4347 5.8926 sec/batch\n",
      "Epoch 1/1  Iteration 32/176 Training loss: 3.4251 5.5146 sec/batch\n",
      "Epoch 1/1  Iteration 33/176 Training loss: 3.4162 5.5477 sec/batch\n",
      "Epoch 1/1  Iteration 34/176 Training loss: 3.4084 5.6215 sec/batch\n",
      "Epoch 1/1  Iteration 35/176 Training loss: 3.4002 5.4818 sec/batch\n",
      "Epoch 1/1  Iteration 36/176 Training loss: 3.3927 5.5128 sec/batch\n",
      "Epoch 1/1  Iteration 37/176 Training loss: 3.3856 6.8210 sec/batch\n",
      "Epoch 1/1  Iteration 38/176 Training loss: 3.3788 5.9658 sec/batch\n",
      "Epoch 1/1  Iteration 39/176 Training loss: 3.3725 5.5988 sec/batch\n",
      "Epoch 1/1  Iteration 40/176 Training loss: 3.3662 5.4715 sec/batch\n",
      "Epoch 1/1  Iteration 41/176 Training loss: 3.3604 5.5248 sec/batch\n",
      "Epoch 1/1  Iteration 42/176 Training loss: 3.3546 5.4642 sec/batch\n",
      "Epoch 1/1  Iteration 43/176 Training loss: 3.3491 5.4597 sec/batch\n",
      "Epoch 1/1  Iteration 44/176 Training loss: 3.3432 5.4901 sec/batch\n",
      "Epoch 1/1  Iteration 45/176 Training loss: 3.3377 5.4821 sec/batch\n",
      "Epoch 1/1  Iteration 46/176 Training loss: 3.3324 5.4745 sec/batch\n",
      "Epoch 1/1  Iteration 47/176 Training loss: 3.3274 5.5074 sec/batch\n",
      "Epoch 1/1  Iteration 48/176 Training loss: 3.3226 5.4676 sec/batch\n",
      "Epoch 1/1  Iteration 49/176 Training loss: 3.3182 5.5068 sec/batch\n",
      "Epoch 1/1  Iteration 50/176 Training loss: 3.3143 5.4855 sec/batch\n",
      "Validation loss: 3.06197 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 51/176 Training loss: 3.3104 5.4714 sec/batch\n",
      "Epoch 1/1  Iteration 52/176 Training loss: 3.3064 5.3820 sec/batch\n",
      "Epoch 1/1  Iteration 53/176 Training loss: 3.3023 5.3799 sec/batch\n",
      "Epoch 1/1  Iteration 54/176 Training loss: 3.2987 5.3486 sec/batch\n",
      "Epoch 1/1  Iteration 55/176 Training loss: 3.2948 5.3655 sec/batch\n",
      "Epoch 1/1  Iteration 56/176 Training loss: 3.2910 5.3096 sec/batch\n",
      "Epoch 1/1  Iteration 57/176 Training loss: 3.2871 5.3654 sec/batch\n",
      "Epoch 1/1  Iteration 58/176 Training loss: 3.2832 5.3431 sec/batch\n",
      "Epoch 1/1  Iteration 59/176 Training loss: 3.2795 5.3470 sec/batch\n",
      "Epoch 1/1  Iteration 60/176 Training loss: 3.2759 5.3554 sec/batch\n",
      "Epoch 1/1  Iteration 61/176 Training loss: 3.2721 5.3595 sec/batch\n",
      "Epoch 1/1  Iteration 62/176 Training loss: 3.2687 5.3561 sec/batch\n",
      "Epoch 1/1  Iteration 63/176 Training loss: 3.2650 5.3579 sec/batch\n",
      "Epoch 1/1  Iteration 64/176 Training loss: 3.2611 5.3564 sec/batch\n",
      "Epoch 1/1  Iteration 65/176 Training loss: 3.2576 5.3539 sec/batch\n",
      "Epoch 1/1  Iteration 66/176 Training loss: 3.2536 5.3348 sec/batch\n",
      "Epoch 1/1  Iteration 67/176 Training loss: 3.2500 5.3471 sec/batch\n",
      "Epoch 1/1  Iteration 68/176 Training loss: 3.2498 5.3778 sec/batch\n",
      "Epoch 1/1  Iteration 69/176 Training loss: 3.2713 5.3791 sec/batch\n",
      "Epoch 1/1  Iteration 70/176 Training loss: 3.2823 5.3622 sec/batch\n",
      "Epoch 1/1  Iteration 71/176 Training loss: 3.2794 5.4535 sec/batch\n",
      "Epoch 1/1  Iteration 72/176 Training loss: 3.2760 5.4375 sec/batch\n",
      "Epoch 1/1  Iteration 73/176 Training loss: 3.2727 5.5547 sec/batch\n",
      "Epoch 1/1  Iteration 74/176 Training loss: 3.2692 5.3940 sec/batch\n",
      "Epoch 1/1  Iteration 75/176 Training loss: 3.2654 5.3724 sec/batch\n",
      "Epoch 1/1  Iteration 76/176 Training loss: 3.2619 5.3849 sec/batch\n",
      "Epoch 1/1  Iteration 77/176 Training loss: 3.2582 6.1472 sec/batch\n",
      "Epoch 1/1  Iteration 78/176 Training loss: 3.2545 5.3408 sec/batch\n",
      "Epoch 1/1  Iteration 79/176 Training loss: 3.2508 5.3976 sec/batch\n",
      "Epoch 1/1  Iteration 80/176 Training loss: 3.2470 5.4084 sec/batch\n",
      "Epoch 1/1  Iteration 81/176 Training loss: 3.2432 5.3918 sec/batch\n",
      "Epoch 1/1  Iteration 82/176 Training loss: 3.2392 5.4098 sec/batch\n",
      "Epoch 1/1  Iteration 83/176 Training loss: 3.2353 5.3620 sec/batch\n",
      "Epoch 1/1  Iteration 84/176 Training loss: 3.2317 5.4733 sec/batch\n",
      "Epoch 1/1  Iteration 85/176 Training loss: 3.2278 5.3837 sec/batch\n",
      "Epoch 1/1  Iteration 86/176 Training loss: 3.2239 5.3829 sec/batch\n",
      "Epoch 1/1  Iteration 87/176 Training loss: 3.2199 5.3788 sec/batch\n",
      "Epoch 1/1  Iteration 88/176 Training loss: 3.2159 5.3944 sec/batch\n",
      "Epoch 1/1  Iteration 89/176 Training loss: 3.2119 5.3948 sec/batch\n",
      "Epoch 1/1  Iteration 90/176 Training loss: 3.2080 5.4003 sec/batch\n",
      "Epoch 1/1  Iteration 91/176 Training loss: 3.2039 5.3854 sec/batch\n",
      "Epoch 1/1  Iteration 92/176 Training loss: 3.2000 5.3808 sec/batch\n",
      "Epoch 1/1  Iteration 93/176 Training loss: 3.1958 5.3859 sec/batch\n",
      "Epoch 1/1  Iteration 94/176 Training loss: 3.1915 5.3466 sec/batch\n",
      "Epoch 1/1  Iteration 95/176 Training loss: 3.1872 5.5285 sec/batch\n",
      "Epoch 1/1  Iteration 96/176 Training loss: 3.1838 5.4485 sec/batch\n",
      "Epoch 1/1  Iteration 97/176 Training loss: 3.1804 5.3385 sec/batch\n",
      "Epoch 1/1  Iteration 98/176 Training loss: 3.1769 5.3120 sec/batch\n",
      "Epoch 1/1  Iteration 99/176 Training loss: 3.1731 5.3655 sec/batch\n",
      "Epoch 1/1  Iteration 100/176 Training loss: 3.1688 5.4318 sec/batch\n",
      "Validation loss: 2.71936 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 101/176 Training loss: 3.1649 5.2999 sec/batch\n",
      "Epoch 1/1  Iteration 102/176 Training loss: 3.1611 5.3155 sec/batch\n",
      "Epoch 1/1  Iteration 103/176 Training loss: 3.1571 5.3657 sec/batch\n",
      "Epoch 1/1  Iteration 104/176 Training loss: 3.1528 5.3258 sec/batch\n",
      "Epoch 1/1  Iteration 105/176 Training loss: 3.1487 5.3558 sec/batch\n",
      "Epoch 1/1  Iteration 106/176 Training loss: 3.1446 5.3167 sec/batch\n",
      "Epoch 1/1  Iteration 107/176 Training loss: 3.1402 5.4303 sec/batch\n",
      "Epoch 1/1  Iteration 108/176 Training loss: 3.1357 5.3133 sec/batch\n",
      "Epoch 1/1  Iteration 109/176 Training loss: 3.1313 5.4486 sec/batch\n",
      "Epoch 1/1  Iteration 110/176 Training loss: 3.1267 5.3340 sec/batch\n",
      "Epoch 1/1  Iteration 111/176 Training loss: 3.1220 5.3165 sec/batch\n",
      "Epoch 1/1  Iteration 112/176 Training loss: 3.1174 5.3145 sec/batch\n",
      "Epoch 1/1  Iteration 113/176 Training loss: 3.1130 5.2852 sec/batch\n",
      "Epoch 1/1  Iteration 114/176 Training loss: 3.1083 5.3645 sec/batch\n",
      "Epoch 1/1  Iteration 115/176 Training loss: 3.1036 5.3445 sec/batch\n",
      "Epoch 1/1  Iteration 116/176 Training loss: 3.0990 5.3457 sec/batch\n",
      "Epoch 1/1  Iteration 117/176 Training loss: 3.0945 5.3317 sec/batch\n",
      "Epoch 1/1  Iteration 118/176 Training loss: 3.0899 5.3083 sec/batch\n",
      "Epoch 1/1  Iteration 119/176 Training loss: 3.0854 5.2965 sec/batch\n",
      "Epoch 1/1  Iteration 120/176 Training loss: 3.0808 5.3420 sec/batch\n",
      "Epoch 1/1  Iteration 121/176 Training loss: 3.0765 5.3187 sec/batch\n",
      "Epoch 1/1  Iteration 122/176 Training loss: 3.0721 5.3426 sec/batch\n",
      "Epoch 1/1  Iteration 123/176 Training loss: 3.0674 5.3226 sec/batch\n",
      "Epoch 1/1  Iteration 124/176 Training loss: 3.0631 5.3163 sec/batch\n",
      "Epoch 1/1  Iteration 125/176 Training loss: 3.0588 5.3308 sec/batch\n",
      "Epoch 1/1  Iteration 126/176 Training loss: 3.0545 5.3302 sec/batch\n",
      "Epoch 1/1  Iteration 127/176 Training loss: 3.0500 5.3837 sec/batch\n",
      "Epoch 1/1  Iteration 128/176 Training loss: 3.0457 5.3947 sec/batch\n",
      "Epoch 1/1  Iteration 129/176 Training loss: 3.0412 5.4211 sec/batch\n",
      "Epoch 1/1  Iteration 130/176 Training loss: 3.0369 5.3764 sec/batch\n",
      "Epoch 1/1  Iteration 131/176 Training loss: 3.0328 5.4393 sec/batch\n",
      "Epoch 1/1  Iteration 132/176 Training loss: 3.0286 5.4115 sec/batch\n",
      "Epoch 1/1  Iteration 133/176 Training loss: 3.0247 5.3369 sec/batch\n",
      "Epoch 1/1  Iteration 134/176 Training loss: 3.0205 5.3198 sec/batch\n",
      "Epoch 1/1  Iteration 135/176 Training loss: 3.0165 5.3412 sec/batch\n",
      "Epoch 1/1  Iteration 136/176 Training loss: 3.0126 5.3655 sec/batch\n",
      "Epoch 1/1  Iteration 137/176 Training loss: 3.0088 5.3540 sec/batch\n",
      "Epoch 1/1  Iteration 138/176 Training loss: 3.0047 5.3495 sec/batch\n",
      "Epoch 1/1  Iteration 139/176 Training loss: 3.0007 5.3452 sec/batch\n",
      "Epoch 1/1  Iteration 140/176 Training loss: 2.9969 5.3620 sec/batch\n",
      "Epoch 1/1  Iteration 141/176 Training loss: 2.9930 5.3495 sec/batch\n",
      "Epoch 1/1  Iteration 142/176 Training loss: 2.9893 5.3142 sec/batch\n",
      "Epoch 1/1  Iteration 143/176 Training loss: 2.9854 5.3782 sec/batch\n",
      "Epoch 1/1  Iteration 144/176 Training loss: 2.9816 5.3834 sec/batch\n",
      "Epoch 1/1  Iteration 145/176 Training loss: 2.9779 5.3113 sec/batch\n",
      "Epoch 1/1  Iteration 146/176 Training loss: 2.9740 5.3397 sec/batch\n",
      "Epoch 1/1  Iteration 147/176 Training loss: 2.9702 5.3414 sec/batch\n",
      "Epoch 1/1  Iteration 148/176 Training loss: 2.9667 5.3835 sec/batch\n",
      "Epoch 1/1  Iteration 149/176 Training loss: 2.9631 5.3693 sec/batch\n",
      "Epoch 1/1  Iteration 150/176 Training loss: 2.9595 5.3855 sec/batch\n",
      "Validation loss: 2.32167 Saving checkpoint!\n",
      "Epoch 1/1  Iteration 151/176 Training loss: 2.9559 5.3123 sec/batch\n",
      "Epoch 1/1  Iteration 152/176 Training loss: 2.9522 5.2837 sec/batch\n",
      "Epoch 1/1  Iteration 153/176 Training loss: 2.9487 5.3706 sec/batch\n",
      "Epoch 1/1  Iteration 154/176 Training loss: 2.9452 5.3037 sec/batch\n",
      "Epoch 1/1  Iteration 155/176 Training loss: 2.9416 5.3049 sec/batch\n",
      "Epoch 1/1  Iteration 156/176 Training loss: 2.9378 5.3673 sec/batch\n",
      "Epoch 1/1  Iteration 157/176 Training loss: 2.9342 5.3442 sec/batch\n",
      "Epoch 1/1  Iteration 158/176 Training loss: 2.9306 5.2975 sec/batch\n",
      "Epoch 1/1  Iteration 159/176 Training loss: 2.9271 5.2992 sec/batch\n",
      "Epoch 1/1  Iteration 160/176 Training loss: 2.9235 5.3267 sec/batch\n",
      "Epoch 1/1  Iteration 161/176 Training loss: 2.9201 5.2775 sec/batch\n",
      "Epoch 1/1  Iteration 162/176 Training loss: 2.9165 5.3912 sec/batch\n",
      "Epoch 1/1  Iteration 163/176 Training loss: 2.9130 5.3179 sec/batch\n",
      "Epoch 1/1  Iteration 164/176 Training loss: 2.9093 5.3192 sec/batch\n",
      "Epoch 1/1  Iteration 165/176 Training loss: 2.9059 5.3352 sec/batch\n",
      "Epoch 1/1  Iteration 166/176 Training loss: 2.9024 5.3323 sec/batch\n",
      "Epoch 1/1  Iteration 167/176 Training loss: 2.8989 5.4166 sec/batch\n",
      "Epoch 1/1  Iteration 168/176 Training loss: 2.8956 5.3797 sec/batch\n",
      "Epoch 1/1  Iteration 169/176 Training loss: 2.8922 5.3198 sec/batch\n",
      "Epoch 1/1  Iteration 170/176 Training loss: 2.8888 5.2859 sec/batch\n",
      "Epoch 1/1  Iteration 171/176 Training loss: 2.8855 5.3448 sec/batch\n",
      "Epoch 1/1  Iteration 172/176 Training loss: 2.8823 5.3263 sec/batch\n",
      "Epoch 1/1  Iteration 173/176 Training loss: 2.8792 5.3295 sec/batch\n",
      "Epoch 1/1  Iteration 174/176 Training loss: 2.8761 5.3300 sec/batch\n",
      "Epoch 1/1  Iteration 175/176 Training loss: 2.8727 5.4264 sec/batch\n",
      "Epoch 1/1  Iteration 176/176 Training loss: 2.8697 5.3880 sec/batch\n",
      "Validation loss: 2.22686 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 100\n",
    "num_steps = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "for lstm_size in [256,512]:\n",
    "    for num_layers in [1, 2]:\n",
    "        for learning_rate in [0.002]:\n",
    "            log_string = 'lr={},rl={},ru={}'.format(learning_rate, num_layers, lstm_size)\n",
    "            model = build_rnn(len(vocab), \n",
    "                    batch_size=batch_size,\n",
    "                    num_steps=num_steps,\n",
    "                    learning_rate=learning_rate,\n",
    "                    lstm_size=lstm_size,\n",
    "                    num_layers=num_layers)\n",
    "            \n",
    "            train(model, epochs, log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i176_l512_2.227.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i50_l512_3.062.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i100_l512_2.719.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i150_l512_2.322.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i176_l512_2.227.ckpt\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            #x[0,0] = ord(c)# vocab_to_int[c]\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab), 1)\n",
    "        #samples.append(chr(c))#(int_to_vocab[c])\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            #samples.append(chr(c))# (int_to_vocab[c])\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frant ho hat wing the ande at tile there shen so ad onte thet and hom want on the her and ale the hing than the\n",
      "ceraled tin ates tise toet the her thes tor the shars hom he adere soud, and her wald her wos timesen it hin hes sith thit and\n",
      "saling hat han\n",
      "tasile of ther arend. The chrate so se the wering the ather the some thet anteste at her hit he thise ans the thes has this whad ale he an on therens wes one ho wend here wout ot he sees and ane sarering the the sonl asin at than wer shat se wall of, here salening womhe thaterene and him to silad ant thicilg tha thar he was the went on aled ta has sore se wand ther andines. \n",
      "he aldit on outes he shad his aned that se sas it the had\n",
      "ther wers on thar sat ard hese her so ware tir an on anting to she seen has wils and he the\n",
      "hesrad thas sis onter taed ther had se thas the\n",
      "chrins his sisthing the somint tha has hisesinge the hat whe she hade sad,, whis hes tind as te the sarer ons andithitg the sant his anden an tous, an he sasting had and on wand hes wor that heas tine soun her wom sor ta site har tin his ale wesh he sariln,\n",
      "at of whe hin thor was thes he the\n",
      "he hone thin tise on thas houd.\n",
      "\n",
      "The has has hased of hus hend wash hou tor aled here he are his the sand ansanding the\n",
      "tine he wore hos hin ade at the\n",
      "thingererend he sher wat th the his thate and taricante sating ot and toricig hant oud an aled an to hiche that and the torerater thot to weshe aned afde tor in her as oud seringering asderasting an his sering at tint il sas illey. I\n",
      "ting to worere hine and to masit of an his ane hise sored he sore and hit as ar in the had has tine the tal all ane sare sanederithe ard antere at he tase satithes wont his\n",
      "hist the wins sherersand and hisesed to hat thot war at of he tor has anderensit ther\n",
      "the an an thes sot he he sare and wesh an sout the are thou ton he hone her hishe thithe terensine walt,\n",
      "an hat he thithene woun thing she tar and, so me ate to marle an wethe had hered, het ad he and\n",
      "other thered, and war her to hat wa\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i176_l512_2.227.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Fra\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
